{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ja: JSONL Algebra","text":"<p>Relational algebra meets JSON streaming. Transform your data with the power of mathematical principles and the simplicity of Unix pipes.</p>"},{"location":"#what-is-ja","title":"What is ja?","text":"<p><code>ja</code> (JSONL Algebra) is a command-line tool that brings the elegance of relational algebra to JSON data processing. It treats JSONL files as relations (tables) and provides operations that can be composed into powerful data pipelines.</p> <pre><code># A taste of ja\ncat orders.jsonl \\\n  | ja select 'status == \"shipped\"' \\\n  | ja join customers.jsonl --on customer_id=id \\\n  | ja groupby region \\\n  | ja agg revenue=sum(amount),orders=count\n</code></pre>"},{"location":"#why-ja","title":"Why ja?","text":"<ul> <li>\ud83e\uddee Algebraic Foundation: Based on mathematical principles that guarantee composability</li> <li>\ud83d\udd17 Unix Philosophy: Small, focused tools that do one thing well</li> <li>\ud83d\udcca Streaming Architecture: Process gigabytes without loading into memory</li> <li>\ud83c\udfaf Nested Data Support: First-class support for real-world JSON structures</li> <li>\u26a1 Zero Dependencies: Pure Python implementation (with optional enhancements)</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quickstart \u2192 Get running in 5 minutes</li> <li>Concepts \u2192 Understand the theory</li> <li>Operations \u2192 Learn each operation</li> <li>Cookbook \u2192 Real-world examples</li> </ul>"},{"location":"#at-a-glance","title":"At a Glance","text":""},{"location":"#the-operations","title":"The Operations","text":"Operation Symbol Purpose Example select \u03c3 Filter rows <code>ja select 'age &gt; 30'</code> project \u03c0 Select columns <code>ja project name,email</code> join \u22c8 Combine relations <code>ja join users.jsonl orders.jsonl --on id=user_id</code> groupby \u03b3 Group rows <code>ja groupby department</code> union \u222a Combine all rows <code>ja union file1.jsonl file2.jsonl</code> distinct \u03b4 Remove duplicates <code>ja distinct</code>"},{"location":"#the-philosophy","title":"The Philosophy","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data   \u2502 --&gt; \u2502 Filter  \u2502 --&gt; \u2502  Join   \u2502 --&gt; \u2502 Result \u2502\n\u2502 (JSONL) \u2502     \u2502(select) \u2502     \u2502 (join)  \u2502     \u2502(JSONL) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2193               \u2193               \u2193               \u2193\n  Relation  --&gt;  Relation  --&gt;  Relation  --&gt;  Relation\n</code></pre> <p>Every operation takes relations and produces relations. This closure property enables infinite composability.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install jsonl-algebra\n</code></pre> <p>That's it! You now have the <code>ja</code> command available.</p>"},{"location":"#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's analyze some order data. Create <code>orders.jsonl</code>:</p> <pre><code>{\"order_id\": 1, \"customer\": \"Alice\", \"amount\": 99.99, \"status\": \"shipped\"}\n{\"order_id\": 2, \"customer\": \"Bob\", \"amount\": 149.99, \"status\": \"pending\"}\n{\"order_id\": 3, \"customer\": \"Alice\", \"amount\": 79.99, \"status\": \"shipped\"}\n{\"order_id\": 4, \"customer\": \"Charlie\", \"amount\": 199.99, \"status\": \"shipped\"}\n{\"order_id\": 5, \"customer\": \"Bob\", \"amount\": 59.99, \"status\": \"cancelled\"}\n</code></pre>"},{"location":"#1-filter-orders","title":"1. Filter Orders","text":"<p>Get only shipped orders:</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl\n</code></pre>"},{"location":"#2-calculate-totals","title":"2. Calculate Totals","text":"<p>Total revenue from shipped orders:</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl | ja agg total=sum(amount)\n</code></pre> <p>Output:</p> <pre><code>{\"total\": 379.97}\n</code></pre>"},{"location":"#3-group-by-customer","title":"3. Group by Customer","text":"<p>Revenue per customer (shipped only):</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl \\\n  | ja groupby customer \\\n  | ja agg revenue=sum(amount),orders=count\n</code></pre> <p>Output:</p> <pre><code>{\"customer\": \"Alice\", \"revenue\": 179.98, \"orders\": 2}\n{\"customer\": \"Charlie\", \"revenue\": 199.99, \"orders\": 1}\n</code></pre>"},{"location":"#4-multi-level-grouping","title":"4. Multi-Level Grouping","text":"<p>Our innovative chained groupby enables complex analytics:</p> <pre><code>cat sales.jsonl \\\n  | ja groupby region \\      # First level grouping\n  | ja groupby product \\     # Second level grouping  \n  | ja agg total=sum(amount) # Final aggregation\n</code></pre> <p>This produces results like:</p> <pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"total\": 1250}\n{\"region\": \"North\", \"product\": \"Gadget\", \"total\": 850}\n{\"region\": \"South\", \"product\": \"Widget\", \"total\": 900}\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Relational Operations: select, project, join, union, intersection, difference, distinct, and more</li> <li>Chained Grouping: Multi-level grouping that preserves composability</li> <li>Nested Data Support: Access and manipulate nested fields using intuitive dot notation</li> <li>Streaming Architecture: Process large datasets without loading into memory</li> <li>Expression Language: Safe and expressive filtering with ExprEval</li> <li>Interactive REPL: Build data pipelines step-by-step interactively</li> <li>Format Conversion: Import/export CSV, JSON arrays, and directory structures</li> <li>Unix Philosophy: Designed for pipes and command composition</li> </ul>"},{"location":"#working-with-nested-data","title":"Working with Nested Data","text":"<p><code>ja</code> makes working with nested JSON objects effortless:</p> <pre><code># Project nested fields\nja project user.name,user.email,order.total data.jsonl\n\n# Group by nested values\nja groupby user.region orders.jsonl | ja agg revenue=sum(amount)\n\n# Filter on nested conditions\nja select 'user.age &gt; 30 and order.status == \"shipped\"' data.jsonl\n</code></pre>"},{"location":"#interactive-mode","title":"Interactive Mode","text":"<p>Want to explore? Try the REPL:</p> <pre><code>ja repl\n\nja&gt; from orders.jsonl\nja&gt; select amount &gt; 100\nja&gt; groupby customer\nja&gt; agg total=sum(amount)\nja&gt; execute\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Read the Quickstart - Get hands-on in 5 minutes</li> <li>Explore the Concepts - Understand the theory</li> <li>Browse the Cookbook - See real examples</li> <li>Join the Community - Contribute and get help</li> </ol>"},{"location":"#dependencies-and-setup","title":"Dependencies and Setup","text":"<p><code>ja</code> includes optional dependencies for enhanced functionality:</p> <ul> <li>jmespath: For safe and expressive filtering (replaces eval)</li> <li>jsonschema: For schema validation features</li> <li>All other features work without external dependencies</li> </ul>"},{"location":"#for-users-from-pypi","title":"For users (from PyPI)","text":"<pre><code>pip install jsonl-algebra\n</code></pre> <p>This automatically installs the required dependencies.</p>"},{"location":"#for-developers-from-local-repository","title":"For developers (from local repository)","text":"<pre><code># Standard installation\npip install .\n\n# Editable mode for development\npip install -e .\n</code></pre> <p>Ready to transform your JSON data? Start with the quickstart guide or dive into the concepts to understand the theory behind the tool.</p>"},{"location":"chained-groups/","title":"Chained GroupBy Operations","text":"<p><code>ja</code> supports a powerful pattern of chaining multiple <code>groupby</code> operations, allowing you to create multi-level aggregations while maintaining the JSONL format throughout the pipeline.</p>"},{"location":"chained-groups/#how-it-works","title":"How It Works","text":"<p>When you use <code>groupby</code> without the <code>--agg</code> flag, <code>ja</code> adds special metadata fields to each row:</p> <ul> <li><code>_group</code>: The value of the grouping key for this row</li> <li><code>_group_field</code>: The field name used for grouping</li> <li><code>_group_size</code>: Total number of rows in this group</li> <li><code>_group_index</code>: This row's index within its group</li> </ul> <p>These fields allow subsequent operations to understand the grouping structure while keeping the data in JSONL format.</p>"},{"location":"chained-groups/#basic-example","title":"Basic Example","text":"<pre><code># Group sales by region, then by product within each region\ncat sales.jsonl | ja groupby region | ja groupby product | ja agg \"total=sum(amount),count\"\n</code></pre>"},{"location":"chained-groups/#representation","title":"Representation","text":""},{"location":"chained-groups/#original-data","title":"Original Data","text":"<pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 100}\n{\"region\": \"North\", \"product\": \"Gadget\", \"amount\": 150}\n{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 200}\n{\"region\": \"South\", \"product\": \"Widget\", \"amount\": 250}\n</code></pre>"},{"location":"chained-groups/#after-first-groupby","title":"After First GroupBy","text":"<pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 100, \"_group\": \"North\", \"_group_field\": \"region\", \"_group_size\": 3, \"_group_index\": 0}\n{\"region\": \"North\", \"product\": \"Gadget\", \"amount\": 150, \"_group\": \"North\", \"_group_field\": \"region\", \"_group_size\": 3, \"_group_index\": 1}\n{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 200, \"_group\": \"North\", \"_group_field\": \"region\", \"_group_size\": 3, \"_group_index\": 2}\n{\"region\": \"South\", \"product\": \"Widget\", \"amount\": 250, \"_group\": \"South\", \"_group_field\": \"region\", \"_group_size\": 1, \"_group_index\": 0}\n</code></pre>"},{"location":"chained-groups/#after-second-groupby","title":"After Second GroupBy","text":"<pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 100, \"_group\": \"North.Widget\", \"_group_field\": \"product\", \"_group_size\": 2, \"_group_index\": 0, \"_parent_group\": \"North\", \"_group_trail\": [\"region\", \"product\"]}\n{\"region\": \"North\", \"product\": \"Widget\", \"amount\": 200, \"_group\": \"North.Widget\", \"_group_field\": \"product\", \"_group_size\": 2, \"_group_index\": 1, \"_parent_group\": \"North\", \"_group_trail\": [\"region\", \"product\"]}\n{\"region\": \"North\", \"product\": \"Gadget\", \"amount\": 150, \"_group\": \"North.Gadget\", \"_group_field\": \"product\", \"_group_size\": 1, \"_group_index\": 0, \"_parent_group\": \"North\", \"_group_trail\": [\"region\", \"product\"]}\n{\"region\": \"South\", \"product\": \"Widget\", \"amount\": 250, \"_group\": \"South.Widget\", \"_group_field\": \"product\", \"_group_size\": 1, \"_group_index\": 0, \"_parent_group\": \"South\", \"_group_trail\": [\"region\", \"product\"]}\n</code></pre>"},{"location":"chained-groups/#after-aggregation","title":"After Aggregation","text":"<pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"total\": 300, \"count\": 2}\n{\"region\": \"North\", \"product\": \"Gadget\", \"total\": 150, \"count\": 1}\n{\"region\": \"South\", \"product\": \"Widget\", \"total\": 250, \"count\": 1}\n</code></pre>"},{"location":"chained-groups/#advanced-examples","title":"Advanced Examples","text":""},{"location":"chained-groups/#three-level-grouping","title":"Three-Level Grouping","text":"<pre><code># Group by year, then month, then category\ncat transactions.jsonl \\\n  | ja groupby year \\\n  | ja groupby month \\\n  | ja groupby category \\\n  | ja agg revenue=sum(amount),transactions=count,avg_transaction=avg(amount)\n</code></pre>"},{"location":"chained-groups/#filtering-between-groups","title":"Filtering Between Groups","text":"<pre><code># Group by user, filter to active users, then group by product\ncat purchases.jsonl \\\n  | ja groupby user_id \\\n  | ja select '_group_size &gt; 5' \\\n  | ja groupby product_id \\\n  | ja agg total=sum(price)\n</code></pre>"},{"location":"chained-groups/#inspecting-intermediate-results","title":"Inspecting Intermediate Results","text":"<pre><code># Use jq to examine the grouping structure\ncat data.jsonl | ja groupby category | jq '._group_field, ._group_size' | sort | uniq -c\n</code></pre>"},{"location":"chained-groups/#comparison-with-direct-aggregation","title":"Comparison with Direct Aggregation","text":"<p>You can still use the <code>--agg</code> flag for more efficient single-level aggregations:</p> <pre><code># Direct aggregation (more efficient for simple cases)\nja groupby region --agg total=sum(amount),count sales.jsonl\n\n# Equivalent chained operation\ncat sales.jsonl | ja groupby region | ja agg total=sum(amount),count\n</code></pre> <p>The chained approach is more flexible but may be slightly less efficient for simple aggregations. Use it when you need: - Multi-level grouping - Intermediate filtering or transformation - Exploratory analysis with inspection of groups - Integration with other tools in a pipeline</p>"},{"location":"future-plans/","title":"Future Plans for ja","text":""},{"location":"future-plans/#lazy-evaluation-and-json-query-language","title":"Lazy Evaluation and JSON Query Language","text":""},{"location":"future-plans/#the-core-idea","title":"The Core Idea","text":"<p>What if we could represent JSONL algebra pipelines as JSON? Instead of executing each step immediately, we'd build a query plan that could be inspected, modified, and optimized before execution.</p>"},{"location":"future-plans/#simple-example","title":"Simple Example","text":"<pre><code># Current eager mode (executes immediately)\ncat orders.jsonl | ja select 'amount &gt; 100' | ja groupby customer | ja agg total=sum(amount)\n\n# Future lazy mode (builds query plan)\ncat orders.jsonl | ja query --build | ja select 'amount &gt; 100' | ja groupby customer | ja agg total=sum(amount)\n</code></pre> <p>This would output a JSON query plan:</p> <pre><code>{\n  \"source\": \"stdin\",\n  \"operations\": [\n    {\"op\": \"select\", \"expr\": \"amount &gt; 100\"},\n    {\"op\": \"groupby\", \"key\": \"customer\"},\n    {\"op\": \"agg\", \"spec\": {\"total\": \"sum(amount)\"}}\n  ]\n}\n</code></pre>"},{"location":"future-plans/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Inspection: See what will happen before it happens</li> <li>Optimization: Rearrange operations for better performance</li> <li>Reusability: Save and share query definitions</li> <li>Tooling: Other tools could generate or consume these plans</li> </ol>"},{"location":"future-plans/#multi-source-example","title":"Multi-Source Example","text":"<pre><code># A join pipeline\ncat transactions.jsonl | ja query --build | ja join users.jsonl --on user_id | ja select 'user.active == true'\n</code></pre> <p>Query plan:</p> <pre><code>{\n  \"source\": \"stdin\",\n  \"operations\": [\n    {\n      \"op\": \"join\",\n      \"with\": \"users.jsonl\",\n      \"on\": [\"user_id\", \"id\"]\n    },\n    {\n      \"op\": \"select\",\n      \"expr\": \"user.active == true\"\n    }\n  ]\n}\n</code></pre>"},{"location":"future-plans/#execution-options","title":"Execution Options","text":"<pre><code># Build and save a query plan\nja query --build &lt; orders.jsonl &gt; plan.json\n\n# Execute a saved plan\nja query --execute plan.json &lt; orders.jsonl\n\n# Show what would happen (dry run)\nja query --explain plan.json\n</code></pre>"},{"location":"future-plans/#stdin-handling","title":"stdin Handling","text":"<p>For stdin sources, we keep it simple:</p> <pre><code># If stdin is too large for lazy mode\n$ cat huge_file.jsonl | ja query --build\nError: stdin too large for query building (&gt;10MB)\n\nOptions:\n1. Save to a file first: cat huge_file.jsonl &gt; data.jsonl\n2. Use eager mode: ja select ... (without --build)\n3. Use existing tools: cat huge_file.jsonl | head -10000 | ja query --build\n</code></pre>"},{"location":"future-plans/#integration-with-unix-tools","title":"Integration with Unix Tools","text":"<p>The beauty is that JSON query plans work well with existing tools:</p> <pre><code># Use jq to modify query plans\nja query --build &lt; data.jsonl | jq '.operations += [{\"op\": \"limit\", \"n\": 100}]' | ja query --execute\n\n# Version control your queries\ngit add queries/monthly_report.json\n\n# Generate queries programmatically\npython generate_query.py | ja query --execute &lt; data.jsonl\n</code></pre>"},{"location":"future-plans/#potential-benefits","title":"Potential Benefits","text":"<ol> <li> <p>Query Optimization <code>json    {      \"original\": [        {\"op\": \"join\", \"with\": \"huge_file.jsonl\"},        {\"op\": \"select\", \"expr\": \"amount &gt; 1000\"}      ],      \"optimized\": [        {\"op\": \"select\", \"expr\": \"amount &gt; 1000\"},        {\"op\": \"join\", \"with\": \"huge_file.jsonl\"}      ]    }</code></p> </li> <li> <p>Debugging <code>bash    ja query --explain plan.json    # Step 1: Load stdin (est. 10,000 rows)    # Step 2: Filter where amount &gt; 100 (est. 2,000 rows)    # Step 3: Group by customer (est. 500 groups)    # Step 4: Aggregate sum(amount)</code></p> </li> <li> <p>Alternative Execution Engines <code>bash    # Future: Convert to SQL    ja query --to-sql plan.json    # SELECT customer, SUM(amount) as total    # FROM stdin    # WHERE amount &gt; 100    # GROUP BY customer</code></p> </li> </ol>"},{"location":"future-plans/#open-questions","title":"Open Questions","text":"<ol> <li>Should this be part of <code>ja</code> or a separate tool?</li> <li>How much optimization is worth the complexity?</li> <li>What's the right balance between lazy and eager execution?</li> </ol>"},{"location":"future-plans/#next-steps","title":"Next Steps","text":"<p>Start simple: 1. Add <code>--dry-run</code> to show what an operation would do 2. Add <code>--explain</code> to show row count estimates 3. Gather feedback on whether full lazy evaluation is needed</p> <p>The goal is to enhance <code>ja</code>'s power while maintaining its simplicity. JSON query plans could be the bridge between simple command-line usage and more complex data processing needs.</p>"},{"location":"future-plans/#streaming-mode-and-window-processing","title":"Streaming Mode and Window Processing","text":""},{"location":"future-plans/#the-streaming-flag","title":"The --streaming Flag","text":"<p>Add a <code>--streaming</code> flag that enforces streaming constraints:</p> <pre><code># This would error\nja sort --streaming data.jsonl\nError: sort operation requires seeing all data and cannot be performed in streaming mode\n\n# This would work\nja select 'amount &gt; 100' --streaming data.jsonl\n</code></pre> <p>Benefits: - Explicit about memory usage expectations - Fail fast when streaming isn't possible - Good for production pipelines with memory constraints</p>"},{"location":"future-plans/#window-based-processing","title":"Window-Based Processing","text":"<p>For operations that normally require seeing all data, add <code>--window-size</code> support:</p> <pre><code># Sort within 1000-row windows\nja sort amount --window-size 1000 huge.jsonl\n\n# Collect groups within windows\nja groupby region huge.jsonl | ja collect --window-size 5000\n\n# Remove duplicates within windows\nja distinct --window-size 10000 huge.jsonl\n</code></pre> <p>This provides a middle ground: - Process arbitrarily large files - Trade completeness for memory efficiency - Useful for approximate results on huge datasets</p>"},{"location":"future-plans/#operations-by-streaming-capability","title":"Operations by Streaming Capability","text":""},{"location":"future-plans/#always-streaming","title":"Always Streaming","text":"<ul> <li><code>select</code> - Row-by-row filtering</li> <li><code>project</code> - Row-by-row transformation</li> <li><code>rename</code> - Row-by-row field renaming</li> <li><code>groupby</code> (without --agg) - Adds metadata only</li> </ul>"},{"location":"future-plans/#never-streaming-need-full-data","title":"Never Streaming (Need Full Data)","text":"<ul> <li><code>sort</code> - Must compare all rows</li> <li><code>distinct</code> - Must track all seen values</li> <li><code>groupby --agg</code> - Must see all groups</li> <li><code>collect</code> - Must gather all group members</li> <li><code>join</code> (currently) - Needs to load right side</li> </ul>"},{"location":"future-plans/#could-be-streaming","title":"Could Be Streaming","text":"<ul> <li><code>join</code> with pre-sorted data and merge join</li> <li><code>union</code> with duplicate handling disabled</li> <li><code>intersection/difference</code> with bloom filters</li> </ul>"},{"location":"future-plans/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Phase 1: Add <code>--streaming</code> flag to enforce constraints</li> <li>Phase 2: Implement <code>--window-size</code> for sort, distinct, collect</li> <li>Phase 3: Document streaming characteristics in help text</li> <li>Phase 4: Add approximate algorithms for streaming versions</li> </ol>"},{"location":"future-plans/#example-memory-conscious-pipeline","title":"Example: Memory-Conscious Pipeline","text":"<pre><code># Process 1TB log file with memory constraints\ncat huge_log.jsonl \\\n  | ja select 'level == \"ERROR\"' --streaming \\\n  | ja project timestamp,message,host \\\n  | ja groupby host \\\n  | ja collect --window-size 10000 \\\n  | ja agg errors=count --window-size 10000\n</code></pre> <p>This processes the entire file while never holding more than 10,000 rows in memory.</p>"},{"location":"future-plans/#path-like-matching","title":"Path-like Matching","text":"<p>We provide dot notation. We will extend this to support more complex matching.</p> <p>The path <code>field1.*.field2[&lt;condition-predicate&gt;].field4</code> can point to many values at <code>field4</code>. Maybe the value is a simple string or integer, or maybe it is arbititrarily complex JSON values. When we group by such a field, many values may be returned for a single JSONL line (JSON).</p> <p>When we perform a group-by operation, we group by the value at the end of that path. If two JSON lines have say the  same value associated to <code>field4</code> in the above example, then they placed in the same group. Right?</p> <p>Not so fast.</p> <p>Suppose we have a JSONL file with 3 entries:</p> <pre><code>{ \"a\": { \"key\" : \"value\" } } \n{ \"b\": { \"key\" : \"value\" } } \n{ \"c\": { \"key\" : \"other-value\" } } \n</code></pre> <p>If we group by <code>a.key</code>, we get one group with the value <code>value</code> and the other gru</p>"},{"location":"getting-started/","title":"Getting Started with ja","text":"<p>This guide walks you through jsonl-algebra's core concepts using realistic example data.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Basic installation\npip install jsonl-algebra\n\n# With dataset generation tools\npip install \"jsonl-algebra[dataset]\"\n</code></pre>"},{"location":"getting-started/#generate-example-data","title":"Generate Example Data","text":"<p>Let's start by creating some example data to work with:</p> <pre><code># Generate sample datasets\nja-generate-dataset --num-companies 8 --num-people 30 --output-dir examples/\n\n# This creates:\n# - examples/companies.jsonl (8 companies with nested headquarters data)\n# - examples/people.jsonl (30 people with nested personal info, jobs, and household relationships)\n</code></pre>"},{"location":"getting-started/#explore-the-data","title":"Explore the Data","text":""},{"location":"getting-started/#look-at-the-structure","title":"Look at the Structure","text":"<pre><code># See the first few records\nja examples/people.jsonl --head 2 --pretty\n\n# Check companies structure  \nja examples/companies.jsonl --head 2 --pretty\n</code></pre>"},{"location":"getting-started/#basic-filtering","title":"Basic Filtering","text":"<pre><code># Find people over 30\nja examples/people.jsonl --where 'person.age &gt; 30' --select person.name,person.age\n\n# Find tech companies\nja examples/companies.jsonl --where 'industry == \"Technology\"' --select name,size\n</code></pre>"},{"location":"getting-started/#working-with-nested-data","title":"Working with Nested Data","text":"<pre><code># Extract names and locations\nja examples/people.jsonl --select person.name,person.location.state,person.location.city\n\n# Group by location\nja examples/people.jsonl --group-by person.location.state --count\n</code></pre>"},{"location":"getting-started/#core-operations","title":"Core Operations","text":""},{"location":"getting-started/#selection-and-projection","title":"Selection and Projection","text":"<pre><code># Select high earners in California\nja examples/people.jsonl \\\\\n  --where 'person.job.salary &gt;= 80000 and person.location.state == \"CA\"' \\\\\n  --select person.name,person.job.title,person.job.salary\n\n# Project just essential job info\nja examples/people.jsonl \\\\\n  --select person.name.first,person.name.last,person.job.title,person.job.company_name\n</code></pre>"},{"location":"getting-started/#grouping-and-aggregation","title":"Grouping and Aggregation","text":"<pre><code># Count employees by company\nja examples/people.jsonl --group-by person.job.company_name --count\n\n# Average salary by company\nja examples/people.jsonl \\\\\n  --group-by person.job.company_name \\\\\n  --agg 'avg_salary=avg(person.job.salary),count=count(*)' \\\\\n  --sort-by avg_salary --reverse\n\n# Salary statistics by state\nja examples/people.jsonl \\\\\n  --group-by person.location.state \\\\\n  --agg 'avg_salary=avg(person.job.salary),min_salary=min(person.job.salary),max_salary=max(person.job.salary),count=count(*)'\n</code></pre>"},{"location":"getting-started/#joins","title":"Joins","text":"<pre><code># Join people with their companies\nja examples/people.jsonl \\\\\n  --join examples/companies.jsonl \\\\\n  --on 'person.job.company_name = name' \\\\\n  --select person.name,person.job.title,name,industry,headquarters.city\n\n# Find people working for large tech companies\nja examples/people.jsonl \\\\\n  --join examples/companies.jsonl \\\\\n  --on 'person.job.company_name = name' \\\\\n  --where 'industry == \"Technology\" and size &gt; 1000' \\\\\n  --select person.name,person.job.title,name,size\n</code></pre>"},{"location":"getting-started/#advanced-examples","title":"Advanced Examples","text":""},{"location":"getting-started/#household-analysis","title":"Household Analysis","text":"<pre><code># Find households with multiple people\nja examples/people.jsonl \\\\\n  --group-by household_id \\\\\n  --agg 'count=count(*),members=collect(person.name.first)' \\\\\n  --where 'count &gt; 1' \\\\\n  --select household_id,count,members\n\n# Average age by household\nja examples/people.jsonl \\\\\n  --group-by household_id \\\\\n  --agg 'avg_age=avg(person.age),family_name=first(person.name.last)' \\\\\n  --select family_name,avg_age \\\\\n  --sort-by avg_age --reverse\n</code></pre>"},{"location":"getting-started/#multi-step-pipelines","title":"Multi-step Pipelines","text":"<pre><code># Complex analysis: High earners by industry and location\nja examples/people.jsonl \\\\\n  --join examples/companies.jsonl --on 'person.job.company_name = name' \\\\\n  --where 'person.job.salary &gt;= 70000' \\\\\n  --group-by 'industry,person.location.state' \\\\\n  --agg 'count=count(*),avg_salary=avg(person.job.salary)' \\\\\n  --where 'count &gt;= 2' \\\\\n  --sort-by avg_salary --reverse\n</code></pre>"},{"location":"getting-started/#chained-grouping","title":"Chained Grouping","text":"<pre><code># Group by industry, then by job title within each industry\nja examples/people.jsonl \\\\\n  --join examples/companies.jsonl --on 'person.job.company_name = name' \\\\\n  --group-by industry \\\\\n  --group-by person.job.title \\\\\n  --agg 'count=count(*),avg_salary=avg(person.job.salary)' \\\\\n  --sort-by avg_salary --reverse\n</code></pre>"},{"location":"getting-started/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/#nested-json-navigation","title":"Nested JSON Navigation","text":"<p>ja uses JSONPath-like syntax to navigate nested structures:</p> <ul> <li><code>person.name.first</code> - Access nested fields with dots</li> <li><code>person.location.state</code> - Navigate deep into objects</li> <li><code>headquarters.city</code> - Works with any level of nesting</li> </ul>"},{"location":"getting-started/#algebraic-operations","title":"Algebraic Operations","text":"<p>ja operations are composable and can be chained:</p> <pre><code># Each operation produces a new relation\nja data.jsonl --where 'age &gt; 25' | ja --group-by department | ja --agg 'count=count(*)'\n</code></pre>"},{"location":"getting-started/#streaming-processing","title":"Streaming Processing","text":"<p>ja processes data in a streaming fashion, making it efficient for large datasets:</p> <pre><code># Generate larger dataset for testing\nja-generate-dataset --num-companies 100 --num-people 10000 --output-dir large/\n\n# Still processes efficiently\nja large/people.jsonl --group-by person.location.state --count\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Concepts Guide for deeper understanding</li> <li>Explore Chained Groups for advanced grouping</li> <li>Check out the Cookbook for real-world examples</li> <li>Try the Interactive REPL for experimentation</li> </ul>"},{"location":"getting-started/#generated-data-reference","title":"Generated Data Reference","text":"<p>The <code>ja-generate-dataset</code> command creates two related files:</p>"},{"location":"getting-started/#companiesjsonl-structure","title":"companies.jsonl Structure","text":"<pre><code>{\n  \"id\": \"uuid\",\n  \"name\": \"Company Name\",\n  \"industry\": \"Technology\",\n  \"headquarters\": {\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"},\n  \"size\": 1500,\n  \"founded\": 2010\n}\n</code></pre>"},{"location":"getting-started/#peoplejsonl-structure","title":"people.jsonl Structure","text":"<pre><code>{\n  \"id\": \"uuid\",\n  \"created_at\": \"2023-05-15T10:30:00Z\",\n  \"status\": \"active\",\n  \"household_id\": \"household-uuid\",\n  \"person\": {\n    \"name\": {\"first\": \"Sarah\", \"last\": \"Johnson\"},\n    \"age\": 32,\n    \"gender\": \"female\",\n    \"email\": \"sarah.johnson@gmail.com\",\n    \"phone\": \"555-123-4567\",\n    \"location\": {\"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"},\n    \"interests\": [\"hiking\", \"photography\"],\n    \"job\": {\n      \"title\": \"Software Engineer\",\n      \"company_name\": \"Tech Solutions Inc\", \n      \"salary\": 95000.0\n    }\n  }\n}\n</code></pre> <p>The data includes realistic relationships:</p> <ul> <li>Employment: <code>person.job.company_name</code> matches company <code>name</code></li> <li>Households: People with the same <code>household_id</code> share last names and locations</li> <li>Geographic: Hierarchical city/state/country structure with <code>ja</code></li> </ul> <p>Welcome to <code>ja</code>, the JSONL Algebra toolkit! This guide will walk you through the basics of using <code>ja</code> to manipulate JSONL data with the power of relational algebra.</p>"},{"location":"getting-started/#what-is-jsonl","title":"What is JSONL?","text":"<p>JSONL (JSON Lines) is a convenient format for storing structured data where each line is a valid JSON object. It's perfect for streaming and processing large datasets because you can read it line by line.</p> <p>Example JSONL file:</p> <pre><code>{\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n{\"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"}\n{\"name\": \"Charlie\", \"age\": 35, \"city\": \"New York\"}\n</code></pre>"},{"location":"getting-started/#installation_1","title":"Installation","text":"<pre><code>pip install jsonl-algebra\n</code></pre> <p>This installs the <code>ja</code> command-line tool and the Python library.</p>"},{"location":"getting-started/#your-first-commands","title":"Your First Commands","text":""},{"location":"getting-started/#1-viewing-data","title":"1. Viewing Data","text":"<p>The simplest operation is to view your data:</p> <pre><code>cat data.jsonl\n</code></pre>"},{"location":"getting-started/#2-selecting-rows-filtering","title":"2. Selecting Rows (Filtering)","text":"<p>Use <code>select</code> to filter rows based on conditions:</p> <pre><code># Select people over 30\nja select 'age &gt; `30`' data.jsonl\n\n# Select people from New York\nja select 'city == `\"New York\"`' data.jsonl\n</code></pre>"},{"location":"getting-started/#3-projecting-columns","title":"3. Projecting Columns","text":"<p>Use <code>project</code> to select specific fields:</p> <pre><code># Get just names and ages\nja project name,age data.jsonl\n\n# Project nested fields\nja project user.name,user.email users.jsonl\n</code></pre>"},{"location":"getting-started/#4-working-with-nested-data","title":"4. Working with Nested Data","text":"<p><code>ja</code> excels at handling nested JSON structures:</p> <pre><code># Given nested data like:\n# {\"person\": {\"name\": {\"first\": \"Alice\", \"last\": \"Smith\"}, \"age\": 30}}\n\n# Project nested fields\nja project person.name.first data.jsonl\n\n# Flatten nested structures\nja project person.name.first,person.age --flatten data.jsonl\n# Output: {\"person.name.first\": \"Alice\", \"person.age\": 30}\n</code></pre>"},{"location":"getting-started/#a-complete-example","title":"A Complete Example","text":"<p>Let's walk through a real-world scenario. You have two files:</p> <p>users.jsonl:</p> <pre><code>{\"id\": 1, \"name\": \"Alice\", \"department\": \"Sales\"}\n{\"id\": 2, \"name\": \"Bob\", \"department\": \"Engineering\"}\n{\"id\": 3, \"name\": \"Charlie\", \"department\": \"Sales\"}\n</code></pre> <p>sales.jsonl:</p> <pre><code>{\"user_id\": 1, \"amount\": 1000, \"date\": \"2024-01-01\"}\n{\"user_id\": 1, \"amount\": 1500, \"date\": \"2024-01-02\"}\n{\"user_id\": 2, \"amount\": 2000, \"date\": \"2024-01-01\"}\n{\"user_id\": 3, \"amount\": 1200, \"date\": \"2024-01-01\"}\n</code></pre> <p>Goal: Find total sales by department.</p> <pre><code># Step 1: Join users with their sales\nja join users.jsonl sales.jsonl --on id=user_id &gt; joined.jsonl\n\n# Step 2: Group by department and sum amounts\nja groupby department --agg total=sum(amount) joined.jsonl\n\n# Or do it all in one pipeline:\nja join users.jsonl sales.jsonl --on id=user_id | \\\n  ja groupby department --agg total=sum(amount)\n</code></pre> <p>Output:</p> <pre><code>{\"department\": \"Sales\", \"total\": 3700}\n{\"department\": \"Engineering\", \"total\": 2000}\n</code></pre>"},{"location":"getting-started/#command-chaining","title":"Command Chaining","text":"<p>One of <code>ja</code>'s most powerful features is the ability to chain commands using Unix pipes:</p> <pre><code># Complex data pipeline\ncat data.jsonl | \\\n  ja select 'status == `\"active\"`' | \\\n  ja project user.name,amount | \\\n  ja sort amount --desc | \\\n  head -10\n</code></pre>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#1-finding-unique-values","title":"1. Finding Unique Values","text":"<pre><code>ja project category data.jsonl | ja distinct\n</code></pre>"},{"location":"getting-started/#2-data-validation","title":"2. Data Validation","text":"<pre><code># Find records with missing fields\nja select '!email' users.jsonl\n</code></pre>"},{"location":"getting-started/#3-computing-statistics","title":"3. Computing Statistics","text":"<pre><code># Average order amount by customer\nja groupby customer_id --agg avg_amount=avg(amount) orders.jsonl\n</code></pre>"},{"location":"getting-started/#4-data-transformation","title":"4. Data Transformation","text":"<pre><code># Rename fields\nja rename old_name=new_name,price=cost data.jsonl\n\n# Flatten nested structures for export\nja project user.name,user.email,order.total --flatten data.jsonl | \\\n  ja export csv &gt; output.csv\n</code></pre>"},{"location":"getting-started/#next-steps_1","title":"Next Steps","text":"<ul> <li>Learn about Advanced Operations</li> <li>Explore the Interactive REPL</li> <li>Read about Schema Management</li> <li>Check out the API Reference</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<pre><code># General help\nja --help\n\n# Command-specific help\nja select --help\nja groupby --help\n</code></pre> <p>Remember: <code>ja</code> is designed to be intuitive. If you know SQL or have used Unix tools like <code>awk</code> or <code>sed</code>, you'll feel right at home!</p>"},{"location":"quickstart/","title":"Quickstart: Your First 5 Minutes with ja","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install jsonl-algebra\n</code></pre> <p>Verify it's working:</p> <pre><code>ja --version\n</code></pre>"},{"location":"quickstart/#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's analyze some order data. Create <code>orders.jsonl</code>:</p> <pre><code>{\"order_id\": 1, \"customer\": \"Alice\", \"amount\": 99.99, \"status\": \"shipped\"}\n{\"order_id\": 2, \"customer\": \"Bob\", \"amount\": 149.99, \"status\": \"pending\"}\n{\"order_id\": 3, \"customer\": \"Alice\", \"amount\": 79.99, \"status\": \"shipped\"}\n{\"order_id\": 4, \"customer\": \"Charlie\", \"amount\": 199.99, \"status\": \"shipped\"}\n{\"order_id\": 5, \"customer\": \"Bob\", \"amount\": 59.99, \"status\": \"cancelled\"}\n</code></pre>"},{"location":"quickstart/#1-filter-orders","title":"1. Filter Orders","text":"<p>Get only shipped orders:</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl\n</code></pre>"},{"location":"quickstart/#2-calculate-totals","title":"2. Calculate Totals","text":"<p>Total revenue from shipped orders:</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl | ja agg total=sum(amount)\n</code></pre> <p>Output:</p> <pre><code>{\"total\": 379.97}\n</code></pre>"},{"location":"quickstart/#3-group-by-customer","title":"3. Group by Customer","text":"<p>Revenue per customer (shipped only):</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl \\\n  | ja groupby customer \\\n  | ja agg revenue=sum(amount),orders=count\n</code></pre> <p>Output:</p> <pre><code>{\"customer\": \"Alice\", \"revenue\": 179.98, \"orders\": 2}\n{\"customer\": \"Charlie\", \"revenue\": 199.99, \"orders\": 1}\n</code></pre>"},{"location":"quickstart/#4-join-with-customer-data","title":"4. Join with Customer Data","text":"<p>Create <code>customers.jsonl</code>:</p> <pre><code>{\"name\": \"Alice\", \"tier\": \"gold\", \"region\": \"west\"}\n{\"name\": \"Bob\", \"tier\": \"silver\", \"region\": \"east\"}\n{\"name\": \"Charlie\", \"tier\": \"gold\", \"region\": \"west\"}\n</code></pre> <p>Join and analyze:</p> <pre><code>ja join customers.jsonl orders.jsonl --on name=customer \\\n  | ja select 'status == \"shipped\"' \\\n  | ja groupby region \\\n  | ja agg revenue=sum(amount)\n</code></pre> <p>Output:</p> <pre><code>{\"region\": \"west\", \"revenue\": 379.97}\n</code></pre>"},{"location":"quickstart/#5-multi-level-grouping","title":"5. Multi-Level Grouping","text":"<p>Showcase the power of chained groupby:</p> <pre><code>ja select 'status == \"shipped\"' orders.jsonl \\\n  | ja groupby customer \\\n  | ja groupby amount \\\n  | ja agg count\n</code></pre> <p>Output:</p> <pre><code>{\"customer\": \"Alice\", \"amount\": 99.99, \"count\": 1}\n{\"customer\": \"Alice\", \"amount\": 79.99, \"count\": 1}\n{\"customer\": \"Charlie\", \"amount\": 199.99, \"count\": 1}\n</code></pre>"},{"location":"quickstart/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ol> <li>Filtering: Use <code>select</code> with expressions</li> <li>Aggregation: Use <code>agg</code> for calculations</li> <li>Grouping: Use <code>groupby</code> to segment data</li> <li>Joining: Combine data from multiple files</li> <li>Chaining: Use pipes to build complex pipelines</li> <li>Multi-level Grouping: Chain groupby operations for hierarchical analysis</li> </ol>"},{"location":"quickstart/#interactive-mode","title":"Interactive Mode","text":"<p>Want to explore? Try the REPL:</p> <pre><code>ja repl\n\nja&gt; from orders.jsonl\nInput source set to: orders.jsonl\nja&gt; select status == \"shipped\"\nAdded: select status == \"shipped\"\nja&gt; groupby customer\nAdded: groupby customer\nja&gt; agg revenue=sum(amount)\nAdded: agg revenue=sum(amount)\nja&gt; execute\nExecuting: ja select 'status == \"shipped\"' orders.jsonl | ja groupby customer - | ja agg revenue=sum(amount) -\n\n--- Output ---\n{\"customer\": \"Alice\", \"revenue\": 179.98}\n{\"customer\": \"Charlie\", \"revenue\": 199.99}\n--------------\n</code></pre>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#data-exploration","title":"Data Exploration","text":"<pre><code># See the structure\nja project customer,amount orders.jsonl | head -5\n\n# Find unique values\nja project status orders.jsonl | ja distinct\n\n# Quick statistics\nja agg count,avg_amount=avg(amount),total=sum(amount) orders.jsonl\n</code></pre>"},{"location":"quickstart/#filtering-and-aggregation","title":"Filtering and Aggregation","text":"<pre><code># Conditional aggregation\nja agg shipped_revenue=sum_if(amount,status==\"shipped\") orders.jsonl\n\n# Top customers\nja groupby customer orders.jsonl \\\n  | ja agg total=sum(amount) \\\n  | ja sort total --desc \\\n  | head -5\n</code></pre>"},{"location":"quickstart/#working-with-nested-data","title":"Working with Nested Data","text":"<pre><code># Assuming nested structure like {\"user\": {\"id\": 1, \"name\": \"Alice\"}}\nja project user.name,user.id nested.jsonl\nja groupby user.region nested.jsonl | ja agg count\nja select 'user.age &gt; 30' nested.jsonl\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Learn all operations \u2192</li> <li>Work with nested data \u2192</li> <li>Understand the theory \u2192</li> <li>See real examples \u2192</li> </ul>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<pre><code># General help\nja --help\n\n# Operation help\nja select --help\nja groupby --help\nja agg --help\n\n# Interactive help\nja repl\nja&gt; help\n</code></pre> <p>Welcome to the world of JSONL algebra! \ud83c\udf89</p>"},{"location":"reference/","title":"API Reference","text":"<p>Welcome to the complete API reference for JSONL Algebra. Every function and class is listed here with its full documentation.</p>"},{"location":"reference/#ja.core","title":"<code>ja.core</code>","text":"<p>Core relational operations for the JSONL algebra system.</p> <p>This module implements the fundamental set and relational operations that form the algebra for manipulating collections of JSON objects. All operations are designed to work with lists of dictionaries, making them suitable for processing JSONL data.</p>"},{"location":"reference/#ja.core.collect","title":"<code>collect(data)</code>","text":"<p>Collect metadata-grouped rows into actual groups.</p> <p>This function takes rows with _groups metadata (from groupby operations) and collects them into explicit groups. Each output row represents one group with all its members in a _rows array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries with _groups metadata</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List where each dict represents a group with _rows array</p> Example <p>Input: [     {\"id\": 1, \"region\": \"North\", \"_groups\": [{\"field\": \"region\", \"value\": \"North\"}]},     {\"id\": 2, \"region\": \"North\", \"_groups\": [{\"field\": \"region\", \"value\": \"North\"}]},     {\"id\": 3, \"region\": \"South\", \"_groups\": [{\"field\": \"region\", \"value\": \"South\"}]} ]</p> <p>Output: [     {\"region\": \"North\", \"_rows\": [{\"id\": 1, \"region\": \"North\"}, {\"id\": 2, \"region\": \"North\"}]},     {\"region\": \"South\", \"_rows\": [{\"id\": 3, \"region\": \"South\"}]} ]</p> Source code in <code>ja/core.py</code> <pre><code>def collect(data: Relation) -&gt; Relation:\n    \"\"\"Collect metadata-grouped rows into actual groups.\n\n    This function takes rows with _groups metadata (from groupby operations)\n    and collects them into explicit groups. Each output row represents one\n    group with all its members in a _rows array.\n\n    Args:\n        data: List of dictionaries with _groups metadata\n\n    Returns:\n        List where each dict represents a group with _rows array\n\n    Example:\n        Input:\n        [\n            {\"id\": 1, \"region\": \"North\", \"_groups\": [{\"field\": \"region\", \"value\": \"North\"}]},\n            {\"id\": 2, \"region\": \"North\", \"_groups\": [{\"field\": \"region\", \"value\": \"North\"}]},\n            {\"id\": 3, \"region\": \"South\", \"_groups\": [{\"field\": \"region\", \"value\": \"South\"}]}\n        ]\n\n        Output:\n        [\n            {\"region\": \"North\", \"_rows\": [{\"id\": 1, \"region\": \"North\"}, {\"id\": 2, \"region\": \"North\"}]},\n            {\"region\": \"South\", \"_rows\": [{\"id\": 3, \"region\": \"South\"}]}\n        ]\n    \"\"\"\n    if not data:\n        return []\n\n    # Check if data has grouping metadata\n    if \"_groups\" not in data[0]:\n        # No grouping metadata - treat entire dataset as one group\n        return [{\"_rows\": data}]\n\n    # Collect rows by their group keys\n    groups = defaultdict(list)\n\n    for row in data:\n        # Build group key from metadata\n        group_key = tuple((g[\"field\"], g[\"value\"]) for g in row[\"_groups\"])\n\n        # Create clean row without metadata\n        clean_row = {k: v for k, v in row.items() if not k.startswith(\"_\")}\n\n        groups[group_key].append(clean_row)\n\n    # Build output with one row per group\n    result = []\n    for group_key, rows in groups.items():\n        group_dict = {}\n\n        # Add group fields to output\n        for field, value in group_key:\n            group_dict[field] = value\n\n        # Add collected rows\n        group_dict[\"_rows\"] = rows\n\n        result.append(group_dict)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.difference","title":"<code>difference(left, right)</code>","text":"<p>Compute the difference of two collections.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>Relation</code> <p>First collection</p> required <code>right</code> <code>Relation</code> <p>Second collection</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>Elements in left but not in right</p> Source code in <code>ja/core.py</code> <pre><code>def difference(\n    left: Relation, right: Relation\n) -&gt; Relation:\n    \"\"\"Compute the difference of two collections.\n\n    Args:\n        left: First collection\n        right: Second collection\n\n    Returns:\n        Elements in left but not in right\n    \"\"\"\n    # Convert right to a set of tuples for efficient lookup\n    right_set = {tuple(sorted(row.items())) for row in right}\n\n    result = []\n    for row in left:\n        if tuple(sorted(row.items())) not in right_set:\n            result.append(row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.distinct","title":"<code>distinct(data)</code>","text":"<p>Remove duplicate rows from a collection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List with duplicates removed</p> Source code in <code>ja/core.py</code> <pre><code>def distinct(data: Relation) -&gt; Relation:\n    \"\"\"Remove duplicate rows from a collection.\n\n    Args:\n        data: List of dictionaries\n\n    Returns:\n        List with duplicates removed\n    \"\"\"\n    seen = set()\n    result = []\n\n    for row in data:\n        # Convert to tuple for hashability\n        row_tuple = tuple(sorted(row.items()))\n        if row_tuple not in seen:\n            seen.add(row_tuple)\n            result.append(row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.intersection","title":"<code>intersection(left, right)</code>","text":"<p>Compute the intersection of two collections.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>Relation</code> <p>First collection</p> required <code>right</code> <code>Relation</code> <p>Second collection</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>Intersection of the two collections</p> Source code in <code>ja/core.py</code> <pre><code>def intersection(\n    left: Relation, right: Relation\n) -&gt; Relation:\n    \"\"\"Compute the intersection of two collections.\n\n    Args:\n        left: First collection\n        right: Second collection\n\n    Returns:\n        Intersection of the two collections\n    \"\"\"\n    # Convert right to a set of tuples for efficient lookup\n    right_set = {tuple(sorted(row.items())) for row in right}\n\n    result = []\n    for row in left:\n        if tuple(sorted(row.items())) in right_set:\n            result.append(row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.join","title":"<code>join(left, right, on)</code>","text":"<p>Inner join with nested-key support.</p> Source code in <code>ja/core.py</code> <pre><code>def join(left: Relation,\n         right: Relation,\n         on: List[Tuple[str, str]]) -&gt; Relation:\n    \"\"\"Inner join with nested-key support.\"\"\"\n    parser = ExprEval()\n\n    # index right side\n    right_index: Dict[Tuple[Any, ...], List[Row]] = defaultdict(list)\n    for r in right:\n        key = tuple(parser.get_field_value(r, rk) for _, rk in on)\n        if all(v is not None for v in key):\n            right_index[key].append(r)\n\n    # roots of every RHS join path (e.g. 'user.id' \u2192 'user')\n    rhs_roots = {re.split(r\"[.\\[]\", rk, 1)[0] for _, rk in on}\n\n    joined: Relation = []\n    for l in left:\n        l_key = tuple(parser.get_field_value(l, lk) for lk, _ in on)\n        if not all(v is not None for v in l_key):\n            continue\n        for r in right_index.get(l_key, []):\n            merged = r.copy()\n            merged.update(l)          # left wins\n            # drop right-side join columns\n            for root in rhs_roots:\n                merged.pop(root, None)\n            joined.append(merged)\n    return joined\n</code></pre>"},{"location":"reference/#ja.core.product","title":"<code>product(left, right)</code>","text":"<p>Cartesian product; colliding keys from right are prefixed with <code>b_</code>.</p> Source code in <code>ja/core.py</code> <pre><code>def product(left: Relation, right: Relation) -&gt; Relation:\n    \"\"\"Cartesian product; colliding keys from *right* are prefixed with ``b_``.\"\"\"\n    result: Relation = []\n    for l in left:\n        for r in right:\n            merged = l.copy()\n            for k, v in r.items():\n                if k in merged:\n                    merged[f\"b_{k}\"] = v\n                else:\n                    merged[k] = v\n            result.append(merged)\n    return result\n</code></pre>"},{"location":"reference/#ja.core.project","title":"<code>project(data, fields, use_jmespath=False)</code>","text":"<p>Project specific fields from each row.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries to project</p> required <code>fields</code> <code>List[str] | str</code> <p>Comma-separated field names or expressions</p> required <code>use_jmespath</code> <code>bool</code> <p>If True, use JMESPath for projection</p> <code>False</code> <p>Returns:</p> Type Description <code>Relation</code> <p>List of dictionaries with only the specified fields</p> Source code in <code>ja/core.py</code> <pre><code>def project(\n    data: Relation, fields: List[str] | str, use_jmespath: bool = False\n) -&gt; Relation:\n    \"\"\"Project specific fields from each row.\n\n    Args:\n        data: List of dictionaries to project\n        fields: Comma-separated field names or expressions\n        use_jmespath: If True, use JMESPath for projection\n\n    Returns:\n        List of dictionaries with only the specified fields\n    \"\"\"\n\n    if use_jmespath:\n        compiled_expr = jmespath.compile(fields)\n        return [compiled_expr.search(row) for row in data]\n\n    # Parse field specifications\n    result = []\n    parser = ExprEval()\n    field_specs = fields if isinstance(fields, list) else fields.split(\",\")\n\n    for row in data:\n        new_row = {}\n\n        for spec in field_specs:\n            if \"=\" in spec:\n                # Computed field: \"total=amount*1.1\" or \"is_adult=age&gt;=18\"\n                name, expr = spec.split(\"=\", 1)\n                name = name.strip()\n                expr = expr.strip()\n\n                # Check if it's an arithmetic expression\n                arith_result = parser.evaluate_arithmetic(expr, row)\n                if arith_result is not None:\n                    new_row[name] = arith_result\n                else:\n                    # Try as boolean expression\n                    new_row[name] = parser.evaluate(expr, row)\n            else:\n                # Simple field projection\n                value = parser.get_field_value(row, spec)\n                if value is not None:\n                    # Build nested structure\n                    parser.set_field_value(new_row, spec, value)\n\n        result.append(new_row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.rename","title":"<code>rename(data, mapping)</code>","text":"<p>Rename fields in each row.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries</p> required <code>mapping</code> <code>Dict[str, str]</code> <p>Dictionary mapping old names to new names</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List with renamed fields</p> Source code in <code>ja/core.py</code> <pre><code>def rename(data: Relation, mapping: Dict[str, str]) -&gt; Relation:\n    \"\"\"Rename fields in each row.\n\n    Args:\n        data: List of dictionaries\n        mapping: Dictionary mapping old names to new names\n\n    Returns:\n        List with renamed fields\n    \"\"\"\n    result = []\n    for row in data:\n        new_row = {}\n        for key, value in row.items():\n            new_key = mapping.get(key, key)\n            new_row[new_key] = value\n        result.append(new_row)\n    return result\n</code></pre>"},{"location":"reference/#ja.core.select","title":"<code>select(data, expr, use_jmespath=False)</code>","text":"<p>Filter rows based on an expression.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries to filter</p> required <code>expr</code> <code>str</code> <p>Expression to evaluate (simple expression or JMESPath)</p> required <code>use_jmespath</code> <code>bool</code> <p>If True, use JMESPath evaluation</p> <code>False</code> <p>Returns:</p> Type Description <code>Relation</code> <p>List of rows where the expression evaluates to true</p> Source code in <code>ja/core.py</code> <pre><code>def select(\n    data: Relation, expr: str, use_jmespath: bool = False\n) -&gt; Relation:\n    \"\"\"Filter rows based on an expression.\n\n    Args:\n        data: List of dictionaries to filter\n        expr: Expression to evaluate (simple expression or JMESPath)\n        use_jmespath: If True, use JMESPath evaluation\n\n    Returns:\n        List of rows where the expression evaluates to true\n    \"\"\"\n    if use_jmespath:\n        compiled_expr = jmespath.compile(expr)\n        return [row for row in data if compiled_expr.search(row)]\n\n    # Use simple expression parser\n    parser = ExprEval()\n    result = []\n\n    # Handle 'and' at the command level for simplicity\n    if \" and \" in expr:\n        # Multiple conditions with 'and'\n        conditions = expr.split(\" and \")\n        for row in data:\n            if all(parser.evaluate(cond.strip(), row) for cond in conditions):\n                result.append(row)\n    elif \" or \" in expr:\n        # Multiple conditions with 'or'\n        conditions = expr.split(\" or \")\n        for row in data:\n            if any(parser.evaluate(cond.strip(), row) for cond in conditions):\n                result.append(row)\n    else:\n        # Single condition\n        for row in data:\n            if parser.evaluate(expr, row):\n                result.append(row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.core.union","title":"<code>union(left, right)</code>","text":"<p>Compute the union of two collections.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>Relation</code> <p>First collection</p> required <code>right</code> <code>Relation</code> <p>Second collection</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>Union of the two collections</p> Source code in <code>ja/core.py</code> <pre><code>def union(\n    left: Relation, right: Relation\n) -&gt; Relation:\n    \"\"\"Compute the union of two collections.\n\n    Args:\n        left: First collection\n        right: Second collection\n\n    Returns:\n        Union of the two collections\n    \"\"\"\n    return left + right\n</code></pre>"},{"location":"reference/#ja.cli","title":"<code>ja.cli</code>","text":"<p>Command-line interface for JSONL algebra operations.</p> <p>This module provides the main CLI entry point and argument parsing for all JSONL algebra operations including relational algebra, schema inference, data import/export, and interactive REPL mode.</p>"},{"location":"reference/#ja.cli.handle_export_command_group","title":"<code>handle_export_command_group(args)</code>","text":"<p>Handle export subcommands by delegating to appropriate handlers.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Parsed command line arguments with export_cmd attribute.</p> required Source code in <code>ja/cli.py</code> <pre><code>def handle_export_command_group(args):\n    \"\"\"Handle export subcommands by delegating to appropriate handlers.\n\n    Args:\n        args: Parsed command line arguments with export_cmd attribute.\n    \"\"\"\n    export_command_handlers = {\n        \"array\": handle_to_array,\n        \"jsonl\": handle_to_jsonl,\n        \"explode\": handle_explode,\n        \"csv\": handle_to_csv,\n    }\n    handler = export_command_handlers.get(args.export_cmd)\n    if handler:\n        handler(args)\n    else:\n        # This should not be reached if subcommands are handled correctly in main\n        print(f\"Unknown export command: {args.export_cmd}\", file=sys.stderr)\n        sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.cli.handle_import_command_group","title":"<code>handle_import_command_group(args)</code>","text":"<p>Handle import subcommands by delegating to appropriate handlers.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Parsed command line arguments with import_cmd attribute.</p> required Source code in <code>ja/cli.py</code> <pre><code>def handle_import_command_group(args):\n    \"\"\"Handle import subcommands by delegating to appropriate handlers.\n\n    Args:\n        args: Parsed command line arguments with import_cmd attribute.\n    \"\"\"\n    import_command_handlers = {\n        \"csv\": handle_import_csv,\n        \"implode\": handle_implode,\n    }\n    handler = import_command_handlers.get(args.import_cmd)\n    if handler:\n        handler(args)\n    else:\n        # This should not be reached if subcommands are handled correctly in main\n        print(f\"Unknown import command: {args.import_cmd}\", file=sys.stderr)\n        sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.cli.json_error","title":"<code>json_error(error_type, message, details=None, exit_code=1)</code>","text":"<p>Output error in JSON format and exit.</p> <p>Parameters:</p> Name Type Description Default <code>error_type</code> <p>Type of error (e.g., \"ParseError\", \"IOError\")</p> required <code>message</code> <p>Human-readable error message</p> required <code>details</code> <p>Optional dict with additional error details</p> <code>None</code> <code>exit_code</code> <p>Exit code (default: 1)</p> <code>1</code> Source code in <code>ja/cli.py</code> <pre><code>def json_error(error_type, message, details=None, exit_code=1):\n    \"\"\"Output error in JSON format and exit.\n\n    Args:\n        error_type: Type of error (e.g., \"ParseError\", \"IOError\")\n        message: Human-readable error message\n        details: Optional dict with additional error details\n        exit_code: Exit code (default: 1)\n    \"\"\"\n    error_obj = {\"error\": {\"type\": error_type, \"message\": message}}\n    if details:\n        error_obj[\"error\"][\"details\"] = details\n\n    # If stderr is not a tty (redirected/piped), always output JSON\n    # If stderr is a tty, output human-readable unless JA_JSON_ERRORS is set\n    if not sys.stderr.isatty() or os.environ.get(\"JA_JSON_ERRORS\"):\n        print(json.dumps(error_obj), file=sys.stderr)\n    else:\n        # Human-readable format for terminal\n        print(f\"ja: error: {message}\", file=sys.stderr)\n        if details:\n            for key, value in details.items():\n                if value is not None and key != \"traceback\":\n                    print(f\"  {key}: {value}\", file=sys.stderr)\n\n    sys.exit(exit_code)\n</code></pre>"},{"location":"reference/#ja.repl","title":"<code>ja.repl</code>","text":"<p>Interactive REPL (Read-Eval-Print Loop) for JSONL algebra operations.</p> <p>This module provides a friendly, interactive shell for chaining JSONL algebra operations together. It's a great way to explore your data, build up complex transformation pipelines step-by-step, and see the results instantly.</p> <p>Think of it as a command-line laboratory for your JSONL data!</p>"},{"location":"reference/#ja.repl.ReplCompiler","title":"<code>ReplCompiler</code>","text":"<p>Compiles and executes a sequence of JSONL algebra commands.</p> <p>This class is the engine of the REPL. It manages the state of the command pipeline, parses user input, and translates the pipeline into a shell command that can be executed. It's designed to provide an intuitive, interactive experience for building data workflows.</p> Source code in <code>ja/repl.py</code> <pre><code>class ReplCompiler:\n    \"\"\"Compiles and executes a sequence of JSONL algebra commands.\n\n    This class is the engine of the REPL. It manages the state of the command\n    pipeline, parses user input, and translates the pipeline into a shell command\n    that can be executed. It's designed to provide an intuitive, interactive\n    experience for building data workflows.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the REPL compiler with an empty pipeline.\"\"\"\n        self.pipeline = []\n        self.current_input_source = None  # Can be a file path or None (implying stdin)\n        self.handlers = {}  # Command handlers are registered in the `run` method.\n\n    def parse_command(self, line):\n        \"\"\"Parse a line of input into a command and its arguments.\n\n        Uses `shlex` to handle quoted arguments correctly.\n\n        Args:\n            line (str): The raw input line from the user.\n\n        Returns:\n            A tuple of (command, args_list), or (None, None) if parsing fails.\n        \"\"\"\n        try:\n            parts = shlex.split(line)\n        except ValueError as e:\n            print(f\"Error parsing command: {e}. Check your quotes.\")\n            return None, None\n        if not parts:\n            return None, None\n        command = parts[0].lower()\n        args = parts[1:]\n        return command, args\n\n    def handle_from(self, args):\n        \"\"\"Set the initial data source for the pipeline (e.g., a file).\n\n        This command must be the first one used when starting a new pipeline\n        with a file source.\n\n        Args:\n            args (list): A list containing the file path or \"stdin\".\n        \"\"\"\n        if not args:\n            print(\"Error: 'from' requires a file path (or 'stdin').\")\n            return\n        if self.pipeline:\n            print(\n                \"Error: 'from' can only be used at the beginning of a new pipeline. Use 'reset' first.\"\n            )\n            return\n        self.current_input_source = args[0]\n        if self.current_input_source.lower() == \"stdin\":\n            self.current_input_source = None  # Internally, None means stdin for clarity\n            print(\"Input source set to: stdin\")\n        else:\n            print(f\"Input source set to: {self.current_input_source}\")\n\n    def add_to_pipeline(self, command_name, args, cli_command_name=None):\n        \"\"\"Add a new command step to the current pipeline.\n\n        Args:\n            command_name (str): The name of the REPL command (e.g., \"project\").\n            args (list): The list of arguments for the command.\n            cli_command_name (str, optional): The corresponding `ja` CLI command name.\n                                             Defaults to `command_name`.\n        \"\"\"\n        if not cli_command_name:\n            cli_command_name = command_name\n        # Ensure 'from' is not added to the pipeline steps directly\n        if command_name.lower() == \"from\":\n            print(\n                \"Error: 'from' is a directive, not a pipeline step. Use 'reset' then 'from &lt;file&gt;'.\"\n            )\n            return\n        self.pipeline.append(\n            {\n                \"repl_command\": command_name,\n                \"cli_command\": cli_command_name,\n                \"args\": args,\n            }\n        )\n        print(f\"Added: {command_name} {' '.join(shlex.quote(a) for a in args)}\")\n\n    def handle_select(self, args):\n        \"\"\"Handle the 'select' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'select' requires an expression.\")\n            return\n        self.add_to_pipeline(\"select\", args)\n\n    def handle_project(self, args):\n        \"\"\"Handle the 'project' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'project' requires column names.\")\n            return\n        self.add_to_pipeline(\"project\", args)\n\n    def handle_join(self, args):\n        \"\"\"Handle the 'join' command by adding it to the pipeline.\"\"\"\n        if len(args) &lt; 3 or args[-2].lower() != \"--on\":\n            print(\"Error: 'join' requires &lt;right_file&gt; --on &lt;key_map&gt;.\")\n            print(\"Example: join orders.jsonl --on user.id=customer_id\")\n            return\n        self.add_to_pipeline(\"join\", args)\n\n    def handle_rename(self, args):\n        \"\"\"Handle the 'rename' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'rename' requires a mapping (e.g., old_name=new_name).\")\n            return\n        self.add_to_pipeline(\"rename\", args)\n\n    def handle_distinct(self, args):\n        \"\"\"Handle the 'distinct' command by adding it to the pipeline.\"\"\"\n        if args:\n            print(\"Warning: 'distinct' does not take arguments in REPL mode. Ignoring.\")\n        self.add_to_pipeline(\"distinct\", [])\n\n    def handle_sort(self, args):\n        \"\"\"Handle the 'sort' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'sort' requires column names.\")\n            return\n        self.add_to_pipeline(\"sort\", args)\n\n    def handle_groupby(self, args):\n        \"\"\"Handle the 'groupby' command by adding it to the pipeline.\"\"\"\n        # Support both chained groupby (no --agg) and immediate aggregation (with --agg)\n        if \"--agg\" in args:\n            # Traditional groupby with immediate aggregation\n            if len(args) &lt; 3 or args[-2].lower() != \"--agg\":\n                print(\"Error: 'groupby --agg' requires &lt;key&gt; --agg &lt;spec&gt;.\")\n                print(\"Example: groupby user.location --agg count,sum(amount)\")\n                return\n        else:\n            # Chained groupby mode\n            if not args:\n                print(\"Error: 'groupby' requires a key.\")\n                print(\"Example: groupby region\")\n                return\n        self.add_to_pipeline(\"groupby\", args)\n\n    def handle_agg(self, args):\n        \"\"\"Handle the 'agg' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'agg' requires an aggregation specification.\")\n            print(\"Example: agg count,total=sum(amount)\")\n            return\n        self.add_to_pipeline(\"agg\", args)\n\n    def handle_product(self, args):\n        \"\"\"Handle the 'product' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'product' requires a right file path.\")\n            return\n        self.add_to_pipeline(\"product\", args)\n\n    def handle_union(self, args):\n        \"\"\"Handle the 'union' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'union' requires a file path.\")\n            return\n        self.add_to_pipeline(\"union\", args)\n\n    def handle_intersection(self, args):\n        \"\"\"Handle the 'intersection' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'intersection' requires a file path.\")\n            return\n        self.add_to_pipeline(\"intersection\", args)\n\n    def handle_difference(self, args):\n        \"\"\"Handle the 'difference' command by adding it to the pipeline.\"\"\"\n        if not args:\n            print(\"Error: 'difference' requires a file path.\")\n            return\n        self.add_to_pipeline(\"difference\", args)\n\n    def _generate_pipeline_command_string_and_segments(self):\n        \"\"\"Construct the full shell command string from the pipeline steps.\n\n        This is the core logic that translates the user's interactive steps into\n        a runnable `ja ... | ja ...` shell command.\n\n        Returns:\n            A tuple containing:\n            - The full, executable shell command string.\n            - A list of individual command segments for display.\n            - An error message string, if any.\n        \"\"\"\n        if not self.pipeline:\n            return None, None, \"Pipeline is empty.\"\n\n        display_segments = []\n        execution_segments = []\n\n        for i, step in enumerate(self.pipeline):\n            current_ja_cmd_parts = [\"ja\", step[\"cli_command\"]]\n            is_first_command_in_pipe = i == 0\n\n            if step[\"cli_command\"] in [\"join\", \"product\", \"union\", \"intersection\", \"difference\"]:\n                # REPL args: &lt;right_file&gt; [--on &lt;key_map&gt;] for join\n                # REPL args: &lt;right_file&gt; for product\n                # CLI: ja join &lt;left&gt; &lt;right&gt; --on &lt;key_map&gt;\n                # CLI: ja product &lt;left&gt; &lt;right&gt;\n                right_file_repl_arg = step[\"args\"][0]\n\n                if is_first_command_in_pipe:\n                    left_input_for_cli = (\n                        self.current_input_source if self.current_input_source else \"-\"\n                    )\n                else:\n                    left_input_for_cli = \"-\"\n\n                current_ja_cmd_parts.append(left_input_for_cli)\n                current_ja_cmd_parts.append(right_file_repl_arg)\n\n                if step[\"cli_command\"] == \"join\":\n                    current_ja_cmd_parts.extend(step[\"args\"][1:])  # --on &lt;key_map&gt;\n\n            elif step[\"cli_command\"] == \"groupby\":\n                # REPL args: &lt;key&gt; [--agg &lt;spec&gt;]\n                # CLI: ja groupby &lt;key&gt; &lt;file_or_stdin&gt; [--agg &lt;spec&gt;]\n                key_repl_arg = step[\"args\"][0]\n                other_args = step[\"args\"][1:]\n\n                current_ja_cmd_parts.append(key_repl_arg)\n\n                if is_first_command_in_pipe:\n                    input_file_for_cli = (\n                        self.current_input_source if self.current_input_source else \"-\"\n                    )\n                else:\n                    input_file_for_cli = \"-\"\n                current_ja_cmd_parts.append(input_file_for_cli)\n                current_ja_cmd_parts.extend(other_args)\n\n            elif step[\"cli_command\"] == \"agg\":\n                # REPL args: &lt;spec&gt;\n                # CLI: ja agg &lt;spec&gt; [file_or_stdin]\n                current_ja_cmd_parts.extend(step[\"args\"])\n\n                if is_first_command_in_pipe:\n                    if self.current_input_source:\n                        current_ja_cmd_parts.append(self.current_input_source)\n\n            else:  # select, project, rename, distinct, sort\n                # REPL args: &lt;command_specific_args&gt;\n                # CLI: ja &lt;command&gt; [command_specific_args...] [file_if_first_and_not_stdin]\n                current_ja_cmd_parts.extend(step[\"args\"])\n\n                if is_first_command_in_pipe:\n                    if self.current_input_source:\n                        current_ja_cmd_parts.append(self.current_input_source)\n\n            joined_segment = shlex.join(current_ja_cmd_parts)\n            display_segments.append(joined_segment)\n            execution_segments.append(joined_segment)\n\n        executable_command_string = \" | \".join(execution_segments)\n        return executable_command_string, display_segments, None\n\n    def handle_compile(self, cmd_args):\n        \"\"\"Generate and print a bash script for the current pipeline.\"\"\"\n        _executable_cmd_str, display_segments, error_msg = (\n            self._generate_pipeline_command_string_and_segments()\n        )\n\n        if error_msg:\n            print(error_msg)\n            return\n\n        print(\"\\n--- Compiled Bash Script ---\")\n        print(\"#!/bin/bash\")\n        print(\"# Generated by ja REPL\")\n\n        if not display_segments:\n            print(\"# Pipeline is empty.\")\n        elif len(display_segments) == 1:\n            print(display_segments[0])\n        else:\n            # Build the pretty-printed pipeline string\n            script_str = display_segments[0]\n            for i in range(1, len(display_segments)):\n                script_str += f\" | \\\\\\n  {display_segments[i]}\"\n            print(script_str)\n        print(\"--------------------------\\n\")\n\n    def handle_execute(self, cmd_args):\n        \"\"\"Execute the current pipeline and display the output.\"\"\"\n        limit_lines = None\n        if cmd_args:\n            if cmd_args[0].startswith(\"--lines=\"):\n                try:\n                    limit_lines = int(cmd_args[0].split(\"=\")[1])\n                    if limit_lines &lt;= 0:\n                        print(\"Error: --lines must be a positive integer.\")\n                        return\n                except (ValueError, IndexError):\n                    print(\n                        \"Error: Invalid format for --lines. Use --lines=N (e.g., --lines=10).\"\n                    )\n                    return\n            else:\n                print(\n                    f\"Warning: Unknown argument '{cmd_args[0]}' for execute. Ignoring. Did you mean --lines=N?\"\n                )\n\n        command_to_execute, _display_segments, error_msg = (\n            self._generate_pipeline_command_string_and_segments()\n        )\n\n        if error_msg:\n            print(error_msg)\n            return\n        if not command_to_execute:\n            print(\n                \"Internal error: No command to execute.\"\n            )  # Should be caught by error_msg\n            return\n\n        print(f\"Executing: {command_to_execute}\")\n\n        try:\n            process = subprocess.run(\n                command_to_execute,\n                shell=True,  # Essential for pipes\n                capture_output=True,\n                text=True,\n                check=False,  # Manually check returncode\n            )\n\n            print(\"\\n--- Output ---\")\n            if process.stdout:\n                output_lines_list = process.stdout.splitlines()\n                if limit_lines is not None:\n                    for i, line_content in enumerate(output_lines_list):\n                        if i &lt; limit_lines:\n                            print(line_content)\n                        else:\n                            print(\n                                f\"... (output truncated to {limit_lines} lines, total {len(output_lines_list)})\"\n                            )\n                            break\n                else:\n                    print(process.stdout.strip())\n            elif process.returncode == 0:\n                print(\"(No output produced)\")\n\n            if process.stderr:\n                print(\"\\n--- Errors ---\")\n                print(process.stderr.strip())\n\n            if process.returncode != 0:\n                print(f\"\\nCommand exited with status {process.returncode}\")\n            elif not process.stdout and not process.stderr and process.returncode == 0:\n                print(\"(Execution successful: No output and no errors)\")\n\n            print(\"--------------\\n\")\n\n        except FileNotFoundError:  # pragma: no cover\n            print(f\"Error: Command 'ja' not found. Make sure it's in your PATH.\")\n        except Exception as e:  # pragma: no cover\n            print(\n                f\"An unexpected error occurred while trying to execute the command: {e}\"\n            )\n            # import traceback\n            # traceback.print_exc()\n\n    def handle_reset(self, args):\n        \"\"\"Clear the current pipeline and reset the input source.\"\"\"\n        self.pipeline = []\n        self.current_input_source = None\n        print(\"Pipeline reset.\")\n\n    def handle_pipeline_show(self, args):\n        \"\"\"Display the steps in the current pipeline.\"\"\"\n        if not self.pipeline:\n            print(\"Pipeline is empty.\")\n        else:\n            print(\"Current pipeline:\")\n            if self.current_input_source:\n                print(f\"  Input: {self.current_input_source}\")\n            else:\n                print(\n                    f\"  Input: stdin (assumed for the first command if 'from' not used)\"\n                )\n            for idx, step in enumerate(self.pipeline):\n                print(\n                    f\"  {idx + 1}. {step['repl_command']} {' '.join(shlex.quote(a) for a in step['args'])}\"\n                )\n        print(\"\")\n\n    def handle_help(self, args):\n        \"\"\"Display the help message with all available REPL commands.\"\"\"\n        print(\"\\nWelcome to the `ja` REPL! Build data pipelines interactively.\")\n        print(\"Here are the available commands:\\n\")\n        print(\"  from &lt;file|stdin&gt;      : Start a new pipeline from a file or stdin.\")\n        print(\"                           Example: from users.jsonl\")\n        print(\"  select '&lt;expr&gt;'        : Filter rows with a Python expression.\")\n        print(\"                           Example: select 'user.age &gt; 30'\")\n        print(\n            \"  project &lt;cols&gt;         : Pick columns, supporting nested data with dot notation.\"\n        )\n        print(\"                           Example: project id,user.name,user.location\")\n        print(\"  join &lt;file&gt; --on &lt;L=R&gt; : Join with another file on one or more keys.\")\n        print(\"                           Example: join orders.jsonl --on id=user_id\")\n        print(\"  rename &lt;old=new,...&gt;   : Rename columns. Supports dot notation.\")\n        print(\"                           Example: rename user.id=user_id,location=loc\")\n        print(\"  distinct               : Remove duplicate rows based on all columns.\")\n        print(\n            \"  sort &lt;cols&gt;            : Sort rows by one or more columns (supports dot notation).\"\n        )\n        print(\"                           Example: sort user.age,id\")\n        print(\"  groupby &lt;key&gt; --agg &lt;&gt; : Group rows and aggregate data.\")\n        print(\n            \"                           Example: groupby cat --agg count,avg:user.score\"\n        )\n        print(\n            \"  product &lt;file&gt;         : Create a Cartesian product with another file.\"\n        )\n        print(\"                           Example: product features.jsonl\")\n        print(\"  union &lt;file&gt;           : Combine rows from another file (deduplicated).\")\n        print(\"                           Example: union archived_users.jsonl\")\n        print(\"  intersection &lt;file&gt;    : Keep only rows present in another file.\")\n        print(\"                           Example: intersection active_users.jsonl\")\n        print(\"  difference &lt;file&gt;      : Remove rows present in another file.\")\n        print(\"                           Example: difference temp_users.jsonl\")\n\n        print(\"\\n--- Pipeline Control ---\")\n        print(\n            \"  execute [--lines=N]    : Run the pipeline and see output (e.g., execute --lines=10).\"\n        )\n        print(\n            \"  compile                : Show the bash script for the current pipeline.\"\n        )\n        print(\"  pipeline               : Show the steps in the current pipeline.\")\n        print(\"  reset                  : Clear the current pipeline.\")\n        print(\"  help                   : Show this help message.\")\n        print(\"  exit                   : Exit the REPL.\\n\")\n\n        print(\"Tips:\")\n        print(\"- Use dot notation (e.g., `user.address.city`) for nested JSON fields.\")\n        print(\n            \"\"\"- Wrap arguments with spaces in quotes (e.g., select 'name == \"John Doe\"').\\n\"\"\"\n        )\n\n    def process(self, line):\n        \"\"\"Process a single line of input from the REPL.\n\n        This method parses the line, finds the appropriate handler for the\n        command, and invokes it.\n\n        Args:\n            line (str): The line of input to process.\n        \"\"\"\n        try:\n            if not line:\n                return\n\n            command, cmd_args = self.parse_command(line)\n            if command is None:  # Parsing error\n                return\n\n            if command in self.handlers:\n                self.handlers[command](cmd_args)\n            elif command:\n                print(\n                    f\"Unknown command: '{command}'. Type 'help' for available commands.\"\n                )\n\n        except EOFError:\n            print(\"\\nExiting...\")\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted. Use 'exit' to quit.\")\n        except Exception as e:\n            print(f\"An unexpected error occurred: {e}\")\n            import traceback\n\n            traceback.print_exc()\n\n    def run(self, initial_command_list=None):  # Renamed 'args' for clarity\n        \"\"\"Start the main REPL event loop.\n\n        This method prints a welcome message, registers all command handlers,\n        and enters an infinite loop to read and process user input.\n\n        Args:\n            initial_command_list (list, optional): A list of command-line arguments\n                                                   to process before starting the\n                                                   interactive loop.\n        \"\"\"\n        print(\"Welcome to ja REPL. Type 'help' for commands, 'exit' to quit.\")\n        self.handlers = {  # Assign to self.handlers so self.process() can use it\n            \"from\": self.handle_from,\n            \"select\": self.handle_select,\n            \"project\": self.handle_project,\n            \"join\": self.handle_join,\n            \"rename\": self.handle_rename,\n            \"distinct\": self.handle_distinct,\n            \"sort\": self.handle_sort,\n            \"groupby\": self.handle_groupby,\n            \"product\": self.handle_product,\n            \"union\": self.handle_union,\n            \"intersection\": self.handle_intersection,\n            \"difference\": self.handle_difference,\n            \"compile\": self.handle_compile,\n            \"execute\": self.handle_execute,\n            \"agg\": self.handle_agg,\n            \"reset\": self.handle_reset,\n            \"pipeline\": self.handle_pipeline_show,\n            \"help\": self.handle_help,\n            \"exit\": lambda _args: sys.exit(\n                0\n            ),  # Consistent signature with other handlers\n        }\n\n        if initial_command_list and len(initial_command_list) &gt; 0:\n            processed_initial_parts = list(initial_command_list)\n            # If the first token of initial_command_list is not a known REPL command,\n            # assume 'from' should be prepended.\n            # This allows `ja repl myfile.jsonl` to be treated as `from myfile.jsonl`.\n            if processed_initial_parts[0].lower() not in self.handlers:\n                processed_initial_parts.insert(0, \"from\")\n\n            initial_line = shlex.join(\n                processed_initial_parts\n            )  # Use shlex.join for safety\n            self.process(initial_line)\n\n        while True:\n            try:\n                line = input(\"ja&gt; \").strip()\n                if not line:\n                    continue\n                # 'exit' command will be handled by self.process -&gt; self.handlers['exit']\n                self.process(line)\n            except EOFError:\n                print(\"\\nExiting...\")\n                sys.exit(0)  # Ensure clean exit\n            except KeyboardInterrupt:\n                print(\"\\nInterrupted. Type 'exit' or Ctrl-D to quit.\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the REPL compiler with an empty pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the REPL compiler with an empty pipeline.\"\"\"\n    self.pipeline = []\n    self.current_input_source = None  # Can be a file path or None (implying stdin)\n    self.handlers = {}  # Command handlers are registered in the `run` method.\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.add_to_pipeline","title":"<code>add_to_pipeline(command_name, args, cli_command_name=None)</code>","text":"<p>Add a new command step to the current pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>command_name</code> <code>str</code> <p>The name of the REPL command (e.g., \"project\").</p> required <code>args</code> <code>list</code> <p>The list of arguments for the command.</p> required <code>cli_command_name</code> <code>str</code> <p>The corresponding <code>ja</code> CLI command name.                              Defaults to <code>command_name</code>.</p> <code>None</code> Source code in <code>ja/repl.py</code> <pre><code>def add_to_pipeline(self, command_name, args, cli_command_name=None):\n    \"\"\"Add a new command step to the current pipeline.\n\n    Args:\n        command_name (str): The name of the REPL command (e.g., \"project\").\n        args (list): The list of arguments for the command.\n        cli_command_name (str, optional): The corresponding `ja` CLI command name.\n                                         Defaults to `command_name`.\n    \"\"\"\n    if not cli_command_name:\n        cli_command_name = command_name\n    # Ensure 'from' is not added to the pipeline steps directly\n    if command_name.lower() == \"from\":\n        print(\n            \"Error: 'from' is a directive, not a pipeline step. Use 'reset' then 'from &lt;file&gt;'.\"\n        )\n        return\n    self.pipeline.append(\n        {\n            \"repl_command\": command_name,\n            \"cli_command\": cli_command_name,\n            \"args\": args,\n        }\n    )\n    print(f\"Added: {command_name} {' '.join(shlex.quote(a) for a in args)}\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_agg","title":"<code>handle_agg(args)</code>","text":"<p>Handle the 'agg' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_agg(self, args):\n    \"\"\"Handle the 'agg' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'agg' requires an aggregation specification.\")\n        print(\"Example: agg count,total=sum(amount)\")\n        return\n    self.add_to_pipeline(\"agg\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_compile","title":"<code>handle_compile(cmd_args)</code>","text":"<p>Generate and print a bash script for the current pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_compile(self, cmd_args):\n    \"\"\"Generate and print a bash script for the current pipeline.\"\"\"\n    _executable_cmd_str, display_segments, error_msg = (\n        self._generate_pipeline_command_string_and_segments()\n    )\n\n    if error_msg:\n        print(error_msg)\n        return\n\n    print(\"\\n--- Compiled Bash Script ---\")\n    print(\"#!/bin/bash\")\n    print(\"# Generated by ja REPL\")\n\n    if not display_segments:\n        print(\"# Pipeline is empty.\")\n    elif len(display_segments) == 1:\n        print(display_segments[0])\n    else:\n        # Build the pretty-printed pipeline string\n        script_str = display_segments[0]\n        for i in range(1, len(display_segments)):\n            script_str += f\" | \\\\\\n  {display_segments[i]}\"\n        print(script_str)\n    print(\"--------------------------\\n\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_difference","title":"<code>handle_difference(args)</code>","text":"<p>Handle the 'difference' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_difference(self, args):\n    \"\"\"Handle the 'difference' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'difference' requires a file path.\")\n        return\n    self.add_to_pipeline(\"difference\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_distinct","title":"<code>handle_distinct(args)</code>","text":"<p>Handle the 'distinct' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_distinct(self, args):\n    \"\"\"Handle the 'distinct' command by adding it to the pipeline.\"\"\"\n    if args:\n        print(\"Warning: 'distinct' does not take arguments in REPL mode. Ignoring.\")\n    self.add_to_pipeline(\"distinct\", [])\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_execute","title":"<code>handle_execute(cmd_args)</code>","text":"<p>Execute the current pipeline and display the output.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_execute(self, cmd_args):\n    \"\"\"Execute the current pipeline and display the output.\"\"\"\n    limit_lines = None\n    if cmd_args:\n        if cmd_args[0].startswith(\"--lines=\"):\n            try:\n                limit_lines = int(cmd_args[0].split(\"=\")[1])\n                if limit_lines &lt;= 0:\n                    print(\"Error: --lines must be a positive integer.\")\n                    return\n            except (ValueError, IndexError):\n                print(\n                    \"Error: Invalid format for --lines. Use --lines=N (e.g., --lines=10).\"\n                )\n                return\n        else:\n            print(\n                f\"Warning: Unknown argument '{cmd_args[0]}' for execute. Ignoring. Did you mean --lines=N?\"\n            )\n\n    command_to_execute, _display_segments, error_msg = (\n        self._generate_pipeline_command_string_and_segments()\n    )\n\n    if error_msg:\n        print(error_msg)\n        return\n    if not command_to_execute:\n        print(\n            \"Internal error: No command to execute.\"\n        )  # Should be caught by error_msg\n        return\n\n    print(f\"Executing: {command_to_execute}\")\n\n    try:\n        process = subprocess.run(\n            command_to_execute,\n            shell=True,  # Essential for pipes\n            capture_output=True,\n            text=True,\n            check=False,  # Manually check returncode\n        )\n\n        print(\"\\n--- Output ---\")\n        if process.stdout:\n            output_lines_list = process.stdout.splitlines()\n            if limit_lines is not None:\n                for i, line_content in enumerate(output_lines_list):\n                    if i &lt; limit_lines:\n                        print(line_content)\n                    else:\n                        print(\n                            f\"... (output truncated to {limit_lines} lines, total {len(output_lines_list)})\"\n                        )\n                        break\n            else:\n                print(process.stdout.strip())\n        elif process.returncode == 0:\n            print(\"(No output produced)\")\n\n        if process.stderr:\n            print(\"\\n--- Errors ---\")\n            print(process.stderr.strip())\n\n        if process.returncode != 0:\n            print(f\"\\nCommand exited with status {process.returncode}\")\n        elif not process.stdout and not process.stderr and process.returncode == 0:\n            print(\"(Execution successful: No output and no errors)\")\n\n        print(\"--------------\\n\")\n\n    except FileNotFoundError:  # pragma: no cover\n        print(f\"Error: Command 'ja' not found. Make sure it's in your PATH.\")\n    except Exception as e:  # pragma: no cover\n        print(\n            f\"An unexpected error occurred while trying to execute the command: {e}\"\n        )\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_from","title":"<code>handle_from(args)</code>","text":"<p>Set the initial data source for the pipeline (e.g., a file).</p> <p>This command must be the first one used when starting a new pipeline with a file source.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>A list containing the file path or \"stdin\".</p> required Source code in <code>ja/repl.py</code> <pre><code>def handle_from(self, args):\n    \"\"\"Set the initial data source for the pipeline (e.g., a file).\n\n    This command must be the first one used when starting a new pipeline\n    with a file source.\n\n    Args:\n        args (list): A list containing the file path or \"stdin\".\n    \"\"\"\n    if not args:\n        print(\"Error: 'from' requires a file path (or 'stdin').\")\n        return\n    if self.pipeline:\n        print(\n            \"Error: 'from' can only be used at the beginning of a new pipeline. Use 'reset' first.\"\n        )\n        return\n    self.current_input_source = args[0]\n    if self.current_input_source.lower() == \"stdin\":\n        self.current_input_source = None  # Internally, None means stdin for clarity\n        print(\"Input source set to: stdin\")\n    else:\n        print(f\"Input source set to: {self.current_input_source}\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_groupby","title":"<code>handle_groupby(args)</code>","text":"<p>Handle the 'groupby' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_groupby(self, args):\n    \"\"\"Handle the 'groupby' command by adding it to the pipeline.\"\"\"\n    # Support both chained groupby (no --agg) and immediate aggregation (with --agg)\n    if \"--agg\" in args:\n        # Traditional groupby with immediate aggregation\n        if len(args) &lt; 3 or args[-2].lower() != \"--agg\":\n            print(\"Error: 'groupby --agg' requires &lt;key&gt; --agg &lt;spec&gt;.\")\n            print(\"Example: groupby user.location --agg count,sum(amount)\")\n            return\n    else:\n        # Chained groupby mode\n        if not args:\n            print(\"Error: 'groupby' requires a key.\")\n            print(\"Example: groupby region\")\n            return\n    self.add_to_pipeline(\"groupby\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_help","title":"<code>handle_help(args)</code>","text":"<p>Display the help message with all available REPL commands.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_help(self, args):\n    \"\"\"Display the help message with all available REPL commands.\"\"\"\n    print(\"\\nWelcome to the `ja` REPL! Build data pipelines interactively.\")\n    print(\"Here are the available commands:\\n\")\n    print(\"  from &lt;file|stdin&gt;      : Start a new pipeline from a file or stdin.\")\n    print(\"                           Example: from users.jsonl\")\n    print(\"  select '&lt;expr&gt;'        : Filter rows with a Python expression.\")\n    print(\"                           Example: select 'user.age &gt; 30'\")\n    print(\n        \"  project &lt;cols&gt;         : Pick columns, supporting nested data with dot notation.\"\n    )\n    print(\"                           Example: project id,user.name,user.location\")\n    print(\"  join &lt;file&gt; --on &lt;L=R&gt; : Join with another file on one or more keys.\")\n    print(\"                           Example: join orders.jsonl --on id=user_id\")\n    print(\"  rename &lt;old=new,...&gt;   : Rename columns. Supports dot notation.\")\n    print(\"                           Example: rename user.id=user_id,location=loc\")\n    print(\"  distinct               : Remove duplicate rows based on all columns.\")\n    print(\n        \"  sort &lt;cols&gt;            : Sort rows by one or more columns (supports dot notation).\"\n    )\n    print(\"                           Example: sort user.age,id\")\n    print(\"  groupby &lt;key&gt; --agg &lt;&gt; : Group rows and aggregate data.\")\n    print(\n        \"                           Example: groupby cat --agg count,avg:user.score\"\n    )\n    print(\n        \"  product &lt;file&gt;         : Create a Cartesian product with another file.\"\n    )\n    print(\"                           Example: product features.jsonl\")\n    print(\"  union &lt;file&gt;           : Combine rows from another file (deduplicated).\")\n    print(\"                           Example: union archived_users.jsonl\")\n    print(\"  intersection &lt;file&gt;    : Keep only rows present in another file.\")\n    print(\"                           Example: intersection active_users.jsonl\")\n    print(\"  difference &lt;file&gt;      : Remove rows present in another file.\")\n    print(\"                           Example: difference temp_users.jsonl\")\n\n    print(\"\\n--- Pipeline Control ---\")\n    print(\n        \"  execute [--lines=N]    : Run the pipeline and see output (e.g., execute --lines=10).\"\n    )\n    print(\n        \"  compile                : Show the bash script for the current pipeline.\"\n    )\n    print(\"  pipeline               : Show the steps in the current pipeline.\")\n    print(\"  reset                  : Clear the current pipeline.\")\n    print(\"  help                   : Show this help message.\")\n    print(\"  exit                   : Exit the REPL.\\n\")\n\n    print(\"Tips:\")\n    print(\"- Use dot notation (e.g., `user.address.city`) for nested JSON fields.\")\n    print(\n        \"\"\"- Wrap arguments with spaces in quotes (e.g., select 'name == \"John Doe\"').\\n\"\"\"\n    )\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_intersection","title":"<code>handle_intersection(args)</code>","text":"<p>Handle the 'intersection' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_intersection(self, args):\n    \"\"\"Handle the 'intersection' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'intersection' requires a file path.\")\n        return\n    self.add_to_pipeline(\"intersection\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_join","title":"<code>handle_join(args)</code>","text":"<p>Handle the 'join' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_join(self, args):\n    \"\"\"Handle the 'join' command by adding it to the pipeline.\"\"\"\n    if len(args) &lt; 3 or args[-2].lower() != \"--on\":\n        print(\"Error: 'join' requires &lt;right_file&gt; --on &lt;key_map&gt;.\")\n        print(\"Example: join orders.jsonl --on user.id=customer_id\")\n        return\n    self.add_to_pipeline(\"join\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_pipeline_show","title":"<code>handle_pipeline_show(args)</code>","text":"<p>Display the steps in the current pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_pipeline_show(self, args):\n    \"\"\"Display the steps in the current pipeline.\"\"\"\n    if not self.pipeline:\n        print(\"Pipeline is empty.\")\n    else:\n        print(\"Current pipeline:\")\n        if self.current_input_source:\n            print(f\"  Input: {self.current_input_source}\")\n        else:\n            print(\n                f\"  Input: stdin (assumed for the first command if 'from' not used)\"\n            )\n        for idx, step in enumerate(self.pipeline):\n            print(\n                f\"  {idx + 1}. {step['repl_command']} {' '.join(shlex.quote(a) for a in step['args'])}\"\n            )\n    print(\"\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_product","title":"<code>handle_product(args)</code>","text":"<p>Handle the 'product' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_product(self, args):\n    \"\"\"Handle the 'product' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'product' requires a right file path.\")\n        return\n    self.add_to_pipeline(\"product\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_project","title":"<code>handle_project(args)</code>","text":"<p>Handle the 'project' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_project(self, args):\n    \"\"\"Handle the 'project' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'project' requires column names.\")\n        return\n    self.add_to_pipeline(\"project\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_rename","title":"<code>handle_rename(args)</code>","text":"<p>Handle the 'rename' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_rename(self, args):\n    \"\"\"Handle the 'rename' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'rename' requires a mapping (e.g., old_name=new_name).\")\n        return\n    self.add_to_pipeline(\"rename\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_reset","title":"<code>handle_reset(args)</code>","text":"<p>Clear the current pipeline and reset the input source.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_reset(self, args):\n    \"\"\"Clear the current pipeline and reset the input source.\"\"\"\n    self.pipeline = []\n    self.current_input_source = None\n    print(\"Pipeline reset.\")\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_select","title":"<code>handle_select(args)</code>","text":"<p>Handle the 'select' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_select(self, args):\n    \"\"\"Handle the 'select' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'select' requires an expression.\")\n        return\n    self.add_to_pipeline(\"select\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_sort","title":"<code>handle_sort(args)</code>","text":"<p>Handle the 'sort' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_sort(self, args):\n    \"\"\"Handle the 'sort' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'sort' requires column names.\")\n        return\n    self.add_to_pipeline(\"sort\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.handle_union","title":"<code>handle_union(args)</code>","text":"<p>Handle the 'union' command by adding it to the pipeline.</p> Source code in <code>ja/repl.py</code> <pre><code>def handle_union(self, args):\n    \"\"\"Handle the 'union' command by adding it to the pipeline.\"\"\"\n    if not args:\n        print(\"Error: 'union' requires a file path.\")\n        return\n    self.add_to_pipeline(\"union\", args)\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.parse_command","title":"<code>parse_command(line)</code>","text":"<p>Parse a line of input into a command and its arguments.</p> <p>Uses <code>shlex</code> to handle quoted arguments correctly.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str</code> <p>The raw input line from the user.</p> required <p>Returns:</p> Type Description <p>A tuple of (command, args_list), or (None, None) if parsing fails.</p> Source code in <code>ja/repl.py</code> <pre><code>def parse_command(self, line):\n    \"\"\"Parse a line of input into a command and its arguments.\n\n    Uses `shlex` to handle quoted arguments correctly.\n\n    Args:\n        line (str): The raw input line from the user.\n\n    Returns:\n        A tuple of (command, args_list), or (None, None) if parsing fails.\n    \"\"\"\n    try:\n        parts = shlex.split(line)\n    except ValueError as e:\n        print(f\"Error parsing command: {e}. Check your quotes.\")\n        return None, None\n    if not parts:\n        return None, None\n    command = parts[0].lower()\n    args = parts[1:]\n    return command, args\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.process","title":"<code>process(line)</code>","text":"<p>Process a single line of input from the REPL.</p> <p>This method parses the line, finds the appropriate handler for the command, and invokes it.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str</code> <p>The line of input to process.</p> required Source code in <code>ja/repl.py</code> <pre><code>def process(self, line):\n    \"\"\"Process a single line of input from the REPL.\n\n    This method parses the line, finds the appropriate handler for the\n    command, and invokes it.\n\n    Args:\n        line (str): The line of input to process.\n    \"\"\"\n    try:\n        if not line:\n            return\n\n        command, cmd_args = self.parse_command(line)\n        if command is None:  # Parsing error\n            return\n\n        if command in self.handlers:\n            self.handlers[command](cmd_args)\n        elif command:\n            print(\n                f\"Unknown command: '{command}'. Type 'help' for available commands.\"\n            )\n\n    except EOFError:\n        print(\"\\nExiting...\")\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted. Use 'exit' to quit.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        import traceback\n\n        traceback.print_exc()\n</code></pre>"},{"location":"reference/#ja.repl.ReplCompiler.run","title":"<code>run(initial_command_list=None)</code>","text":"<p>Start the main REPL event loop.</p> <p>This method prints a welcome message, registers all command handlers, and enters an infinite loop to read and process user input.</p> <p>Parameters:</p> Name Type Description Default <code>initial_command_list</code> <code>list</code> <p>A list of command-line arguments                                    to process before starting the                                    interactive loop.</p> <code>None</code> Source code in <code>ja/repl.py</code> <pre><code>def run(self, initial_command_list=None):  # Renamed 'args' for clarity\n    \"\"\"Start the main REPL event loop.\n\n    This method prints a welcome message, registers all command handlers,\n    and enters an infinite loop to read and process user input.\n\n    Args:\n        initial_command_list (list, optional): A list of command-line arguments\n                                               to process before starting the\n                                               interactive loop.\n    \"\"\"\n    print(\"Welcome to ja REPL. Type 'help' for commands, 'exit' to quit.\")\n    self.handlers = {  # Assign to self.handlers so self.process() can use it\n        \"from\": self.handle_from,\n        \"select\": self.handle_select,\n        \"project\": self.handle_project,\n        \"join\": self.handle_join,\n        \"rename\": self.handle_rename,\n        \"distinct\": self.handle_distinct,\n        \"sort\": self.handle_sort,\n        \"groupby\": self.handle_groupby,\n        \"product\": self.handle_product,\n        \"union\": self.handle_union,\n        \"intersection\": self.handle_intersection,\n        \"difference\": self.handle_difference,\n        \"compile\": self.handle_compile,\n        \"execute\": self.handle_execute,\n        \"agg\": self.handle_agg,\n        \"reset\": self.handle_reset,\n        \"pipeline\": self.handle_pipeline_show,\n        \"help\": self.handle_help,\n        \"exit\": lambda _args: sys.exit(\n            0\n        ),  # Consistent signature with other handlers\n    }\n\n    if initial_command_list and len(initial_command_list) &gt; 0:\n        processed_initial_parts = list(initial_command_list)\n        # If the first token of initial_command_list is not a known REPL command,\n        # assume 'from' should be prepended.\n        # This allows `ja repl myfile.jsonl` to be treated as `from myfile.jsonl`.\n        if processed_initial_parts[0].lower() not in self.handlers:\n            processed_initial_parts.insert(0, \"from\")\n\n        initial_line = shlex.join(\n            processed_initial_parts\n        )  # Use shlex.join for safety\n        self.process(initial_line)\n\n    while True:\n        try:\n            line = input(\"ja&gt; \").strip()\n            if not line:\n                continue\n            # 'exit' command will be handled by self.process -&gt; self.handlers['exit']\n            self.process(line)\n        except EOFError:\n            print(\"\\nExiting...\")\n            sys.exit(0)  # Ensure clean exit\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted. Type 'exit' or Ctrl-D to quit.\")\n</code></pre>"},{"location":"reference/#ja.repl.repl","title":"<code>repl(parsed_cli_args)</code>","text":"<p>Entry point for the <code>ja repl</code> command.</p> <p>Initializes and runs the ReplCompiler.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_cli_args</code> <code>Namespace</code> <p>The parsed command-line arguments,                                   which may include an initial file                                   to load.</p> required Source code in <code>ja/repl.py</code> <pre><code>def repl(parsed_cli_args):  # Receives the argparse.Namespace object\n    \"\"\"Entry point for the `ja repl` command.\n\n    Initializes and runs the ReplCompiler.\n\n    Args:\n        parsed_cli_args (argparse.Namespace): The parsed command-line arguments,\n                                              which may include an initial file\n                                              to load.\n    \"\"\"\n    compiler = ReplCompiler()\n    # Get the list of initial arguments passed to `ja repl ...`\n    # getattr default to empty list if 'initial_args' is not present (it will be due to nargs=\"*\")\n    initial_repl_args_list = getattr(parsed_cli_args, \"initial_args\", [])\n    compiler.run(initial_command_list=initial_repl_args_list)\n</code></pre>"},{"location":"reference/#ja.commands","title":"<code>ja.commands</code>","text":"<p>Command handlers for the JSONL algebra CLI.</p> <p>This module connects the command-line interface to the core data processing functions. Each <code>handle_*</code> function is responsible for reading input data, calling the appropriate core function, and writing the results to stdout.</p>"},{"location":"reference/#ja.commands.get_input_stream","title":"<code>get_input_stream(file_path)</code>","text":"<p>Yield a readable file-like object.</p> <ul> <li>If file_path is None or '-', yield sys.stdin.</li> <li>Otherwise open the given path for reading.</li> </ul> Source code in <code>ja/commands.py</code> <pre><code>@contextmanager\ndef get_input_stream(file_path):\n    \"\"\"\n    Yield a readable file-like object.\n\n    - If file_path is None or '-', yield sys.stdin.\n    - Otherwise open the given path for reading.\n    \"\"\"\n    if file_path is not None and file_path != \"-\":\n        f = open(file_path, \"r\")\n        try:\n            yield f\n        finally:\n            f.close()\n    else:\n        yield sys.stdin\n</code></pre>"},{"location":"reference/#ja.commands.handle_agg","title":"<code>handle_agg(args)</code>","text":"<p>Handle agg command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_agg(args):\n    \"\"\"Handle agg command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    if not data:\n        write_jsonl([])\n        return\n\n    # Check if input has group metadata - use new format\n    if \"_groups\" in data[0]:\n        # Process grouped data\n        result = aggregate_grouped_data(data, args.agg)\n    else:\n        # Process ungrouped data\n        result = [aggregate_single_group(data, args.agg)]\n\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_collect","title":"<code>handle_collect(args)</code>","text":"<p>Handle collect command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_collect(args):\n    \"\"\"Handle collect command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    if not data:\n        write_jsonl([])\n        return\n\n    # Check for streaming flag\n    if hasattr(args, \"streaming\") and args.streaming:\n        json_error(\n            \"StreamingError\",\n            \"Collect operation requires seeing all data and cannot be performed in streaming mode. \"\n            \"Remove --streaming flag or use window-based processing with --window-size\",\n        )\n        return\n\n    # Handle window-based collection\n    if hasattr(args, \"window_size\") and args.window_size:\n        # Process data in windows\n        window_size = args.window_size\n        for i in range(0, len(data), window_size):\n            window = data[i : i + window_size]\n            result = collect(window)\n            write_jsonl(result)\n    else:\n        # Collect all data at once\n        result = collect(data)\n        write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_difference","title":"<code>handle_difference(args)</code>","text":"<p>Handle difference command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_difference(args):\n    \"\"\"Handle difference command.\"\"\"\n    with get_input_stream(args.left) as f:\n        left_data = read_jsonl(f)\n    with get_input_stream(args.right) as f:\n        right_data = read_jsonl(f)\n\n    result = difference(left_data, right_data)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_distinct","title":"<code>handle_distinct(args)</code>","text":"<p>Handle distinct command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_distinct(args):\n    \"\"\"Handle distinct command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    result = distinct(data)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_explode","title":"<code>handle_explode(args)</code>","text":"<p>Handle explode command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_explode(args):\n    \"\"\"Handle explode command.\"\"\"\n    input_filename_stem = \"jsonl_output\"  # Default stem\n    if args.file and args.file != \"-\":\n        input_filename_stem = Path(args.file).stem\n\n    output_directory = args.output_dir if args.output_dir else input_filename_stem\n\n    with get_input_stream(args.file) as input_stream:\n        try:\n            jsonl_to_dir(input_stream, output_directory, input_filename_stem)\n        except Exception as e:\n            print(f\"Error during explode operation: {e}\", file=sys.stderr)\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_groupby","title":"<code>handle_groupby(args)</code>","text":"<p>Handle groupby command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_groupby(args):\n    \"\"\"Handle groupby command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    if hasattr(args, \"agg\") and args.agg:\n        # Traditional groupby with aggregation\n        result = groupby_agg(data, args.key, args.agg)\n    else:\n        # Check if input is already grouped - look for new format\n        if data and \"_groups\" in data[0]:\n            # This is a chained groupby\n            result = groupby_chained(data, args.key)\n        else:\n            # First groupby\n            result = groupby_with_metadata(data, args.key)\n\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_implode","title":"<code>handle_implode(args)</code>","text":"<p>Handle implode command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_implode(args):\n    \"\"\"Handle implode command.\"\"\"\n    try:\n        for line in dir_to_jsonl_lines(\n            args.input_dir, args.add_filename_key, args.recursive\n        ):\n            print(line)\n    except ValueError as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An unexpected error occurred during implode: {e}\", file=sys.stderr)\n        sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_import_csv","title":"<code>handle_import_csv(args)</code>","text":"<p>Handle import-csv command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_import_csv(args):\n    \"\"\"Handle import-csv command.\"\"\"\n    with get_input_stream(args.file) as input_stream:\n        try:\n            for line in csv_to_jsonl_lines(\n                input_stream, has_header=args.has_header, infer_types=args.infer_types\n            ):\n                print(line)\n        except Exception as e:\n            print(\n                f\"An unexpected error occurred during CSV import: {e}\", file=sys.stderr\n            )\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_intersection","title":"<code>handle_intersection(args)</code>","text":"<p>Handle intersection command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_intersection(args):\n    \"\"\"Handle intersection command.\"\"\"\n    with get_input_stream(args.left) as f:\n        left_data = read_jsonl(f)\n    with get_input_stream(args.right) as f:\n        right_data = read_jsonl(f)\n\n    result = intersection(left_data, right_data)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_join","title":"<code>handle_join(args)</code>","text":"<p>Handle join command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_join(args):\n    \"\"\"Handle join command.\"\"\"\n    with get_input_stream(args.left) as f:\n        left = read_jsonl(f)\n    with get_input_stream(args.right) as f:\n        right = read_jsonl(f)\n\n    lcol_str, rcol_str = args.on.split(\"=\", 1)\n    lcol = lcol_str.strip()\n    rcol = rcol_str.strip()\n\n    result = join(left, right, [(lcol, rcol)])\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_product","title":"<code>handle_product(args)</code>","text":"<p>Handle product command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_product(args):\n    \"\"\"Handle product command.\"\"\"\n    with get_input_stream(args.left) as f:\n        left_data = read_jsonl(f)\n    with get_input_stream(args.right) as f:\n        right_data = read_jsonl(f)\n\n    result = product(left_data, right_data)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_project","title":"<code>handle_project(args)</code>","text":"<p>Handle project command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_project(args):\n    \"\"\"Handle project command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    use_jmespath = hasattr(args, \"jmespath\") and args.jmespath\n\n    try:\n        result = project(data, args.expr, use_jmespath=use_jmespath)\n        write_jsonl(result)\n    except jmespath.exceptions.ParseError as e:\n        json_error(\n            \"JMESPathParseError\",\n            f\"Invalid JMESPath expression: {e}\",\n            {\"expression\": args.expr},\n        )\n</code></pre>"},{"location":"reference/#ja.commands.handle_rename","title":"<code>handle_rename(args)</code>","text":"<p>Handle rename command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_rename(args):\n    \"\"\"Handle rename command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    mapping_pairs = args.mapping.split(\",\")\n    mapping = {}\n    for pair_str in mapping_pairs:\n        parts = pair_str.split(\"=\", 1)\n        if len(parts) == 2:\n            old_name, new_name = parts\n            mapping[old_name.strip()] = new_name.strip()\n        else:\n            print(\n                f\"Warning: Malformed rename pair '{pair_str.strip()}' ignored.\",\n                file=sys.stderr,\n            )\n\n    result = rename(data, mapping)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_schema_infer","title":"<code>handle_schema_infer(args)</code>","text":"<p>Handle schema infer command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_schema_infer(args):\n    \"\"\"Handle schema infer command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    schema = infer_schema(data)\n    write_json_object(schema)\n</code></pre>"},{"location":"reference/#ja.commands.handle_schema_validate","title":"<code>handle_schema_validate(args)</code>","text":"<p>Handle schema validate command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_schema_validate(args):\n    \"\"\"Handle schema validate command.\"\"\"\n    try:\n        import jsonschema\n    except ImportError:\n        print(\n            \"jsonschema is not installed. Please install it with: pip install jsonschema\",\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    # Can't read both from stdin\n    if args.schema == \"-\" and (not args.file or args.file == \"-\"):\n        print(\n            \"Error: When reading schema from stdin, a file argument for the data to validate must be provided.\",\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    try:\n        with get_input_stream(args.schema) as f:\n            schema = json.load(f)\n    except (IOError, json.JSONDecodeError) as e:\n        print(\n            f\"Error reading or parsing schema file {args.schema}: {e}\", file=sys.stderr\n        )\n        sys.exit(1)\n\n    # If schema was from stdin, the file MUST be from a file, not stdin.\n    data_source = args.file if args.schema == \"-\" else (args.file or \"-\")\n\n    with get_input_stream(data_source) as lines:\n        validation_failed = False\n        for i, line in enumerate(lines, 1):\n            try:\n                instance = json.loads(line)\n                jsonschema.validate(instance=instance, schema=schema)\n                print(line.strip())\n            except json.JSONDecodeError as e:\n                print(f\"Error decoding JSON on line {i}: {e}\", file=sys.stderr)\n                validation_failed = True\n            except jsonschema.exceptions.ValidationError as e:\n                print(f\"Validation error on line {i}: {e.message}\", file=sys.stderr)\n                validation_failed = True\n\n    if validation_failed:\n        sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_select","title":"<code>handle_select(args)</code>","text":"<p>Handle select command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_select(args):\n    \"\"\"Handle select command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    use_jmespath = hasattr(args, \"jmespath\") and args.jmespath\n\n    try:\n        result = select(data, args.expr, use_jmespath=use_jmespath)\n        write_jsonl(result)\n    except jmespath.exceptions.ParseError as e:\n        json_error(\n            \"JMESPathParseError\",\n            f\"Invalid JMESPath expression: {e}\",\n            {\"expression\": args.expr},\n        )\n</code></pre>"},{"location":"reference/#ja.commands.handle_sort","title":"<code>handle_sort(args)</code>","text":"<p>Handle sort command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_sort(args):\n    \"\"\"Handle sort command.\"\"\"\n    with get_input_stream(args.file) as f:\n        data = read_jsonl(f)\n\n    result = sort_by(data, args.keys, descending=args.desc)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.handle_to_array","title":"<code>handle_to_array(args)</code>","text":"<p>Handle to-array command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_to_array(args):\n    \"\"\"Handle to-array command.\"\"\"\n    with get_input_stream(args.file) as input_stream:\n        array_string = jsonl_to_json_array_string(input_stream)\n        print(array_string)\n</code></pre>"},{"location":"reference/#ja.commands.handle_to_csv","title":"<code>handle_to_csv(args)</code>","text":"<p>Handle to-csv command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_to_csv(args):\n    \"\"\"Handle to-csv command.\"\"\"\n    column_functions = {}\n    if args.apply:\n        for col, expr_str in args.apply:\n            try:\n                # WARNING: eval() is a security risk if the expression is not from a trusted source.\n                func = eval(expr_str)\n                if not callable(func):\n                    raise ValueError(\n                        f\"Expression for column '{col}' did not evaluate to a callable function.\"\n                    )\n                column_functions[col] = func\n            except Exception as e:\n                print(\n                    f\"Error parsing --apply expression for column '{col}': {e}\",\n                    file=sys.stderr,\n                )\n                sys.exit(1)\n\n    with get_input_stream(args.file) as input_stream:\n        try:\n            jsonl_to_csv_stream(\n                input_stream,\n                sys.stdout,\n                flatten=args.flatten,\n                flatten_sep=args.flatten_sep,\n                column_functions=column_functions,\n            )\n        except Exception as e:\n            print(\n                f\"An unexpected error occurred during CSV export: {e}\", file=sys.stderr\n            )\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_to_jsonl","title":"<code>handle_to_jsonl(args)</code>","text":"<p>Handle to-jsonl command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_to_jsonl(args):\n    \"\"\"Handle to-jsonl command.\"\"\"\n    with get_input_stream(args.file) as input_stream:\n        try:\n            for line in json_array_to_jsonl_lines(input_stream):\n                print(line)\n        except ValueError as e:\n            print(f\"Error: {e}\", file=sys.stderr)\n            sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.handle_union","title":"<code>handle_union(args)</code>","text":"<p>Handle union command.</p> Source code in <code>ja/commands.py</code> <pre><code>def handle_union(args):\n    \"\"\"Handle union command.\"\"\"\n    with get_input_stream(args.left) as f:\n        left_data = read_jsonl(f)\n    with get_input_stream(args.right) as f:\n        right_data = read_jsonl(f)\n\n    result = union(left_data, right_data)\n    write_jsonl(result)\n</code></pre>"},{"location":"reference/#ja.commands.json_error","title":"<code>json_error(error_type, message, details=None)</code>","text":"<p>Print a JSON error message to stderr and exit.</p> Source code in <code>ja/commands.py</code> <pre><code>def json_error(error_type: str, message: str, details: Dict[str, Any] = None) -&gt; None:\n    \"\"\"Print a JSON error message to stderr and exit.\"\"\"\n    error_info = {\n        \"error\": {\n            \"type\": error_type,\n            \"message\": message,\n        }\n    }\n    if details:\n        error_info[\"error\"][\"details\"] = details\n    print(json.dumps(error_info), file=sys.stderr)\n    sys.exit(1)\n</code></pre>"},{"location":"reference/#ja.commands.read_jsonl","title":"<code>read_jsonl(input_stream)</code>","text":"<p>Read JSONL data from a file-like object.</p> Source code in <code>ja/commands.py</code> <pre><code>def read_jsonl(input_stream) -&gt; List[Dict[str, Any]]:\n    \"\"\"Read JSONL data from a file-like object.\"\"\"\n    return [json.loads(line) for line in input_stream]\n</code></pre>"},{"location":"reference/#ja.commands.write_json_object","title":"<code>write_json_object(obj)</code>","text":"<p>Write a single object as pretty-printed JSON to stdout.</p> Source code in <code>ja/commands.py</code> <pre><code>def write_json_object(obj: Any) -&gt; None:\n    \"\"\"Write a single object as pretty-printed JSON to stdout.\"\"\"\n    print(json.dumps(obj, indent=2))\n</code></pre>"},{"location":"reference/#ja.commands.write_jsonl","title":"<code>write_jsonl(rows)</code>","text":"<p>Write a collection of objects as JSONL to stdout.</p> Source code in <code>ja/commands.py</code> <pre><code>def write_jsonl(rows: List[Dict[str, Any]]) -&gt; None:\n    \"\"\"Write a collection of objects as JSONL to stdout.\"\"\"\n    for row in rows:\n        print(json.dumps(row))\n</code></pre>"},{"location":"reference/#ja.group","title":"<code>ja.group</code>","text":"<p>Grouping operations for JSONL algebra.</p> <p>This module provides grouping functionality that supports both immediate aggregation and metadata-based chaining for multi-level grouping.</p>"},{"location":"reference/#ja.group.groupby_agg","title":"<code>groupby_agg(data, group_key, agg_spec)</code>","text":"<p>Group and aggregate in one operation.</p> <p>This function is kept for backward compatibility and for the --agg flag. It's more efficient for simple cases but less flexible than chaining.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries to group and aggregate</p> required <code>group_key</code> <code>str</code> <p>Field to group by</p> required <code>agg_spec</code> <code>str</code> <p>Aggregation specification</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List of aggregated results, one per group</p> Source code in <code>ja/group.py</code> <pre><code>def groupby_agg(data: Relation, group_key: str, agg_spec: str) -&gt; Relation:\n    \"\"\"Group and aggregate in one operation.\n\n    This function is kept for backward compatibility and for the --agg flag.\n    It's more efficient for simple cases but less flexible than chaining.\n\n    Args:\n        data: List of dictionaries to group and aggregate\n        group_key: Field to group by\n        agg_spec: Aggregation specification\n\n    Returns:\n        List of aggregated results, one per group\n    \"\"\"\n    parser = ExprEval()\n\n    # Group data\n    groups = defaultdict(list)\n    for row in data:\n        key = parser.get_field_value(row, group_key)\n        groups[key].append(row)\n\n    # Apply aggregations\n    result = []\n    agg_specs = parse_agg_specs(agg_spec)\n\n    for key, group_rows in groups.items():\n        row_result = {group_key: key}\n        for spec in agg_specs:\n            row_result.update(apply_single_agg(spec, group_rows))\n        result.append(row_result)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.group.groupby_chained","title":"<code>groupby_chained(grouped_data, new_group_key)</code>","text":"<p>Apply groupby to already-grouped data.</p> <p>This function handles multi-level grouping by building on existing group metadata.</p> <p>Parameters:</p> Name Type Description Default <code>grouped_data</code> <code>Relation</code> <p>Data with existing group metadata</p> required <code>new_group_key</code> <code>str</code> <p>Field to group by</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List with nested group metadata</p> Source code in <code>ja/group.py</code> <pre><code>def groupby_chained(grouped_data: Relation, new_group_key: str) -&gt; Relation:\n    \"\"\"Apply groupby to already-grouped data.\n\n    This function handles multi-level grouping by building on existing\n    group metadata.\n\n    Args:\n        grouped_data: Data with existing group metadata\n        new_group_key: Field to group by\n\n    Returns:\n        List with nested group metadata\n    \"\"\"\n    parser = ExprEval()\n\n    # Group within existing groups\n    nested_groups = defaultdict(list)\n\n    import sys\n    print(\"hi\", file=sys.stderr)\n\n    for row in grouped_data:\n        # Get existing groups\n        existing_groups = row.get(\"_groups\", [])\n        try:\n            # print(row, file=sys.stderr)\n            key_value = parser.get_field_value(row, new_group_key)\n            nested_groups[key_value].append(row)\n        except Exception as e:\n            key_value = json.dumps(key_value, ensure_ascii=False, sort_keys=True)\n            nested_groups[key_value].append(row)\n\n        new_key_value = parser.get_field_value(row, new_group_key)\n\n        print(\"Processing row:\", row, \"New group key:\", new_group_key, \"Value:\", new_key_value, file=sys.stderr)\n\n        # Create a tuple key for grouping (for internal use only)\n\n        group_tuple = tuple((g[\"field\"], g[\"value\"]) for g in existing_groups)\n        group_tuple += ((new_group_key, new_key_value),)\n\n        print(\"Hmm...\", file=sys.stderr)\n        try:\n\n            nested_groups[group_tuple].append(row)\n        except Exception as e:\n            # make group_tuple hashable\n            # here is how to do it: \n            group_tuple = tuple(map(str, group_tuple))\n            nested_groups[group_tuple].append(row)\n\n    import sys\n    print(\"hi\", file=sys.stderr)\n    # Add new metadata\n    result = []\n    for group_tuple, group_rows in nested_groups.items():\n        group_size = len(group_rows)\n\n        for index, row in enumerate(group_rows):\n            new_row = row.copy()\n\n            value = parser.get_field_value(row, new_group_key)\n\n            # Extend the groups list\n            new_row[\"_groups\"] = row.get(\"_groups\", []).copy()\n            new_row[\"_groups\"].append({\n                \"field\": new_group_key,\n                \"value\": value\n            })\n\n            new_row[\"_group_size\"] = group_size\n            new_row[\"_group_index\"] = index\n            result.append(new_row)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.group.groupby_with_metadata","title":"<code>groupby_with_metadata(data, group_key)</code>","text":"<p>Group data and add metadata fields.</p> <p>This function enables chained groupby operations by adding special metadata fields to each row: - _groups: List of {field, value} objects representing the grouping hierarchy - _group_size: Total number of rows in this group - _group_index: This row's index within its group</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries to group</p> required <code>group_key</code> <code>str</code> <p>Field to group by (supports dot notation)</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List with group metadata added to each row</p> Source code in <code>ja/group.py</code> <pre><code>def groupby_with_metadata(data: Relation, group_key: str) -&gt; Relation:\n    \"\"\"Group data and add metadata fields.\n\n    This function enables chained groupby operations by adding special\n    metadata fields to each row:\n    - _groups: List of {field, value} objects representing the grouping hierarchy\n    - _group_size: Total number of rows in this group\n    - _group_index: This row's index within its group\n\n    Args:\n        data: List of dictionaries to group\n        group_key: Field to group by (supports dot notation)\n\n    Returns:\n        List with group metadata added to each row\n    \"\"\"\n    parser = ExprEval()\n\n    # First pass: collect groups\n    groups = defaultdict(list)\n    for row in data:\n        try:\n            # print(row, file=sys.stderr)\n            key_value = parser.get_field_value(row, group_key)\n            groups[key_value].append(row)\n        except Exception as e:\n            key_value = json.dumps(key_value, ensure_ascii=False, sort_keys=True)\n            groups[key_value].append(row)\n\n    import sys\n    print(\"hi\", file=sys.stderr)\n    # print(\"Huh?\")\n    # Second pass: add metadata and flatten\n    result = []\n    last_index = len(groups) - 1\n\n    for i, (group_value, group_rows) in enumerate(groups.items()):\n        print(\"Processing group:\", group_value, \"Size:\", len(group_rows), \"Index:\", i,   \"Last Index:\", last_index, file=sys.stderr)\n        group_size = len(group_rows)\n        for index, row in enumerate(group_rows):\n            # Create new row with metadata\n            new_row = row.copy()\n            # check if group_value is a serialized json value\n            if isinstance(group_value, str):\n                try:\n                    group_value = json.loads(group_value)\n                    # print(group_value)\n                    # print(\"Deserialized group value:\", group_value)\n                except json.JSONDecodeError:\n                    pass\n                    # print({\"Huh?\"})\n            # print(\"Processing group:\", group_value, \"Size:\", group_size)\n            new_row[\"_groups\"] = [{\"field\": group_key, \"value\": group_value}]\n            new_row[\"_group_size\"] = group_size\n            new_row[\"_group_index\"] = index\n            result.append(new_row)\n\n    print(\"Done processing groups\", file=sys.stderr)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.agg","title":"<code>ja.agg</code>","text":"<p>Aggregation engine for JSONL algebra operations.</p> <p>This module provides all aggregation functionality including parsing aggregation specifications, applying aggregations to data, and all built-in aggregation functions (sum, avg, min, max, etc.).</p>"},{"location":"reference/#ja.agg.aggregate_grouped_data","title":"<code>aggregate_grouped_data(grouped_data, agg_spec)</code>","text":"<p>Aggregate data that has group metadata.</p> <p>Parameters:</p> Name Type Description Default <code>grouped_data</code> <code>Relation</code> <p>Data with group metadata</p> required <code>agg_spec</code> <code>str</code> <p>Aggregation specification</p> required <p>Returns:</p> Type Description <code>Relation</code> <p>List of aggregated results</p> Source code in <code>ja/agg.py</code> <pre><code>def aggregate_grouped_data(grouped_data: Relation, agg_spec: str) -&gt; Relation:\n    \"\"\"Aggregate data that has group metadata.\n\n    Args:\n        grouped_data: Data with group metadata\n        agg_spec: Aggregation specification\n\n    Returns:\n        List of aggregated results\n    \"\"\"\n    from collections import defaultdict\n\n    # Group by the combination of all grouping fields\n    groups = defaultdict(list)\n    group_keys = {}\n\n    for row in grouped_data:\n        # Use the _groups list to create a grouping key\n        groups_list = row.get(\"_groups\", [])\n\n        # Create a tuple key for internal grouping\n        group_tuple = tuple((g[\"field\"], g[\"value\"]) for g in groups_list)\n\n        # Store the groups for this tuple\n        if group_tuple not in group_keys:\n            group_keys[group_tuple] = groups_list\n\n        # Remove metadata for aggregation\n        clean_row = {k: v for k, v in row.items() if not k.startswith(\"_group\")}\n        groups[group_tuple].append(clean_row)\n\n    # Apply aggregations\n    result = []\n\n    for group_tuple, group_rows in groups.items():\n        # Start with all grouping fields\n        agg_result = {}\n\n        # Add all grouping fields from the metadata\n        for group_info in group_keys[group_tuple]:\n            agg_result[group_info[\"field\"]] = group_info[\"value\"]\n\n        # Parse and apply aggregations\n        agg_specs = parse_agg_specs(agg_spec)\n        for spec in agg_specs:\n            agg_result.update(apply_single_agg(spec, group_rows))\n\n        result.append(agg_result)\n\n    return result\n</code></pre>"},{"location":"reference/#ja.agg.aggregate_single_group","title":"<code>aggregate_single_group(data, agg_spec)</code>","text":"<p>Aggregate ungrouped data as a single group.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Relation</code> <p>List of dictionaries</p> required <code>agg_spec</code> <code>str</code> <p>Aggregation specification</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with aggregation results</p> Source code in <code>ja/agg.py</code> <pre><code>def aggregate_single_group(data: Relation, agg_spec: str) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate ungrouped data as a single group.\n\n    Args:\n        data: List of dictionaries\n        agg_spec: Aggregation specification\n\n    Returns:\n        Dictionary with aggregation results\n    \"\"\"\n    agg_specs = parse_agg_specs(agg_spec)\n    result = {}\n\n    for spec in agg_specs:\n        result.update(apply_single_agg(spec, data))\n\n    return result\n</code></pre>"},{"location":"reference/#ja.agg.apply_single_agg","title":"<code>apply_single_agg(spec, data)</code>","text":"<p>Apply a single aggregation to data.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>Tuple[str, str]</code> <p>(name, expression) tuple</p> required <code>data</code> <code>Relation</code> <p>List of dictionaries</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with aggregation result</p> Source code in <code>ja/agg.py</code> <pre><code>def apply_single_agg(spec: Tuple[str, str], data: Relation) -&gt; Dict[str, Any]:\n    \"\"\"Apply a single aggregation to data.\n\n    Args:\n        spec: (name, expression) tuple\n        data: List of dictionaries\n\n    Returns:\n        Dictionary with aggregation result\n    \"\"\"\n    name, expr = spec\n    parser = ExprEval()\n\n    # Parse the aggregation expression\n    if \"(\" in expr and expr.endswith(\")\"):\n        func_name = expr[:expr.index(\"(\")]\n        field_expr = expr[expr.index(\"(\") + 1:-1].strip()\n    else:\n        func_name = expr\n        field_expr = \"\"\n\n    # Handle conditional aggregations\n    if \"_if\" in func_name:\n        # e.g., count_if(status == active) or sum_if(amount, status == paid)\n        base_func = func_name.replace(\"_if\", \"\")\n\n        if \",\" in field_expr:\n            # sum_if(amount, status == paid)\n            field, condition = field_expr.split(\",\", 1)\n            field = field.strip()\n            condition = condition.strip()\n\n            # Filter data based on condition\n            filtered_data = [row for row in data if parser.evaluate(condition, row)]\n\n            # Apply base aggregation to filtered data\n            if base_func == \"sum\":\n                values = [parser.get_field_value(row, field) for row in filtered_data]\n                return {name: sum(v for v in values if v is not None)}\n            elif base_func == \"avg\":\n                values = [parser.get_field_value(row, field) for row in filtered_data]\n                values = [v for v in values if v is not None]\n                return {name: sum(values) / len(values) if values else 0}\n            elif base_func == \"count\":\n                return {name: len(filtered_data)}\n        else:\n            # count_if(status == active)\n            filtered_data = [row for row in data if parser.evaluate(field_expr, row)]\n            return {name: len(filtered_data)}\n\n    # Regular aggregations\n    if func_name == \"count\":\n        return {name: len(data)}\n\n    elif func_name in AGGREGATION_FUNCTIONS:\n        if func_name in [\"first\", \"last\"]:\n            # Special handling for first/last\n            if not data:\n                return {name: None}\n            row = data[0] if func_name == \"first\" else data[-1]\n            value = parser.get_field_value(row, field_expr) if field_expr else row\n            return {name: value}\n        else:\n            # Collect values for aggregation\n            values = []\n            for row in data:\n                if field_expr:\n                    # Try arithmetic evaluation first\n                    val = parser.evaluate_arithmetic(field_expr, row)\n                    if val is None:\n                        val = parser.get_field_value(row, field_expr)\n                else:\n                    val = row\n                if val is not None:\n                    values.append(val)\n\n            # Apply aggregation functionsum(_agg_numeric_values(values))\n            result = AGGREGATION_FUNCTIONS[func_name](values)\n            return {name: result}\n\n    return {name: None}\n</code></pre>"},{"location":"reference/#ja.agg.parse_agg_specs","title":"<code>parse_agg_specs(agg_spec)</code>","text":"<p>Parse aggregation specification string.</p> <p>Parameters:</p> Name Type Description Default <code>agg_spec</code> <code>str</code> <p>Aggregation specification (e.g., \"count, avg_age=avg(age)\")</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List of (name, expression) tuples</p> Source code in <code>ja/agg.py</code> <pre><code>def parse_agg_specs(agg_spec: str) -&gt; List[Tuple[str, str]]:\n    \"\"\"Parse aggregation specification string.\n\n    Args:\n        agg_spec: Aggregation specification (e.g., \"count, avg_age=avg(age)\")\n\n    Returns:\n        List of (name, expression) tuples\n    \"\"\"\n    specs = []\n    for part in agg_spec.split(\",\"):\n        part = part.strip()\n        if \"=\" in part:\n            # Named aggregation: avg_age=avg(age)\n            name, expr = part.split(\"=\", 1)\n            specs.append((name.strip(), expr.strip()))\n        else:\n            # Simple aggregation: count\n            specs.append((part, part))\n    return specs\n</code></pre>"},{"location":"reference/#ja.export","title":"<code>ja.export</code>","text":"<p>Utilities for exporting JSONL data to other formats.</p> <p>This module provides a collection of functions for converting JSONL data into various other formats. It powers the <code>ja export</code> command group, enabling transformations like converting JSONL to a standard JSON array or \"exploding\" a JSONL file into a directory of individual JSON files.</p>"},{"location":"reference/#ja.export.dir_to_jsonl","title":"<code>dir_to_jsonl(input_dir_path_str, add_filename_key=None, recursive=False)</code>","text":"<p>Converts JSON files in a directory to JSONL lines. Files are sorted by 'item-.json' pattern if applicable, otherwise lexicographically. Optionally adds filename as a key to each JSON object. Source code in <code>ja/export.py</code> <pre><code>def dir_to_jsonl(\n    input_dir_path_str: str, add_filename_key: str = None, recursive: bool = False\n):\n    \"\"\"\n    Converts JSON files in a directory to JSONL lines.\n    Files are sorted by 'item-&lt;index&gt;.json' pattern if applicable, otherwise lexicographically.\n    Optionally adds filename as a key to each JSON object.\n    \"\"\"\n    input_dir = pathlib.Path(input_dir_path_str)\n    if not input_dir.is_dir():\n        raise ValueError(f\"Input path is not a directory: {input_dir_path_str}\")\n\n    json_files_paths = []\n    if recursive:\n        for root, _, files in os.walk(input_dir):\n            for file in files:\n                if file.lower().endswith(\".json\"):\n                    json_files_paths.append(pathlib.Path(root) / file)\n    else:\n        for item in input_dir.iterdir():\n            if item.is_file() and item.name.lower().endswith(\".json\"):\n                json_files_paths.append(item)\n\n    sorted_file_paths = _sort_files_for_implode(json_files_paths)\n\n    for file_path in sorted_file_paths:\n        try:\n            with open(file_path, \"r\") as f:\n                data = json.load(f)\n\n            if add_filename_key:\n                # Use relative path from the input_dir to keep it cleaner\n                relative_filename = str(file_path.relative_to(input_dir))\n                actual_key = _ensure_unique_key(data, add_filename_key)\n                data[actual_key] = relative_filename\n\n            yield json.dumps(data)\n        except json.JSONDecodeError as e:\n            print(\n                f\"Skipping invalid JSON file: {file_path} - Error: {e}\", file=sys.stderr\n            )\n            continue\n        except Exception as e:\n            print(f\"Error processing file {file_path}: {e}\", file=sys.stderr)\n            continue\n</code></pre>"},{"location":"reference/#ja.export.json_array_to_jsonl_lines","title":"<code>json_array_to_jsonl_lines(json_array_input_stream)</code>","text":"<p>Read a JSON array from a stream and yield each element as a JSONL line.</p> <p>Parameters:</p> Name Type Description Default <code>json_array_input_stream</code> <p>Input stream containing a JSON array.</p> required <p>Yields:</p> Type Description <p>JSON strings representing each array element.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a valid JSON array.</p> Source code in <code>ja/export.py</code> <pre><code>def json_array_to_jsonl_lines(json_array_input_stream):\n    \"\"\"Read a JSON array from a stream and yield each element as a JSONL line.\n\n    Args:\n        json_array_input_stream: Input stream containing a JSON array.\n\n    Yields:\n        JSON strings representing each array element.\n\n    Raises:\n        ValueError: If the input is not a valid JSON array.\n    \"\"\"\n    try:\n        json_string = \"\".join(json_array_input_stream)\n        data = json.loads(json_string)\n        if not isinstance(data, list):\n            raise ValueError(\"Input is not a JSON array.\")\n        for record in data:\n            yield json.dumps(record)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON array input: {e}\")\n    except ValueError as e:\n        raise e\n</code></pre>"},{"location":"reference/#ja.export.jsonl_to_dir","title":"<code>jsonl_to_dir(jsonl_input_stream, output_dir_path_str, input_filename_stem='data')</code>","text":"<p>Exports JSONL lines to individual JSON files in a directory. The output directory is named after input_filename_stem if output_dir_path_str is not specific. Files are named item-.json. Source code in <code>ja/export.py</code> <pre><code>def jsonl_to_dir(\n    jsonl_input_stream, output_dir_path_str: str, input_filename_stem: str = \"data\"\n):\n    \"\"\"\n    Exports JSONL lines to individual JSON files in a directory.\n    The output directory is named after input_filename_stem if output_dir_path_str is not specific.\n    Files are named item-&lt;index&gt;.json.\n    \"\"\"\n    output_dir = pathlib.Path(output_dir_path_str)\n\n    # If output_dir_path_str was just a name (not a path), it might be used as the stem.\n    # If it's a directory, we use the provided input_filename_stem for the sub-directory.\n    if (\n        output_dir.is_dir() and not output_dir.exists()\n    ):  # A path like \"output/my_data\" where \"output\" exists\n        # This case is tricky. Let's assume output_dir_path_str is the target directory.\n        pass\n    elif not output_dir.name.endswith((\".jsonl\", \".json\")) and not output_dir.exists():\n        # Treat as a new directory to be created directly\n        pass\n    else:  # Default behavior: create a subdirectory based on the input stem\n        output_dir = output_dir / input_filename_stem\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    count = 0\n    for i, line in enumerate(jsonl_input_stream):\n        try:\n            record = json.loads(line)\n            file_path = output_dir / f\"item-{i}.json\"\n            with open(file_path, \"w\") as f:\n                json.dump(record, f, indent=2)\n            count += 1\n        except json.JSONDecodeError as e:\n            print(\n                f\"Skipping invalid JSON line during export: {line.strip()} - Error: {e}\",\n                file=sys.stderr,\n            )\n            continue\n    print(f\"Exported {count} items to {output_dir.resolve()}\", file=sys.stderr)\n</code></pre>"},{"location":"reference/#ja.export.jsonl_to_json_array_string","title":"<code>jsonl_to_json_array_string(jsonl_input_stream)</code>","text":"<p>Read JSONL from a stream and return a JSON array string.</p> <p>Parameters:</p> Name Type Description Default <code>jsonl_input_stream</code> <p>Input stream containing JSONL data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A JSON array string containing all records.</p> Source code in <code>ja/export.py</code> <pre><code>def jsonl_to_json_array_string(jsonl_input_stream) -&gt; str:\n    \"\"\"Read JSONL from a stream and return a JSON array string.\n\n    Args:\n        jsonl_input_stream: Input stream containing JSONL data.\n\n    Returns:\n        A JSON array string containing all records.\n    \"\"\"\n    records = []\n    for line in jsonl_input_stream:\n        try:\n            records.append(json.loads(line))\n        except json.JSONDecodeError as e:\n            print(\n                f\"Skipping invalid JSON line: {line.strip()} - Error: {e}\",\n                file=sys.stderr,\n            )\n            continue\n    return json.dumps(records, indent=2)\n</code></pre>"},{"location":"reference/#ja.exporter","title":"<code>ja.exporter</code>","text":"<p>Export your JSONL data to other popular formats like CSV.</p> <p>This module is your gateway to the wider data ecosystem. It provides powerful and flexible tools to convert your JSONL data into formats that are easy to use with spreadsheets, traditional databases, or other data analysis tools.</p> <p>The key feature is its intelligent handling of nested JSON, which can be \"flattened\" into separate columns, making complex data accessible in a simple CSV format.</p>"},{"location":"reference/#ja.exporter.jsonl_to_csv_stream","title":"<code>jsonl_to_csv_stream(jsonl_stream, output_stream, flatten=True, flatten_sep='.', column_functions=None)</code>","text":"<p>Convert a stream of JSONL data into a CSV stream.</p> <p>This is a highly flexible function for exporting your data. It reads JSONL records, intelligently discovers all possible headers (even if they vary between lines), and writes to a CSV format.</p> <p>It shines when dealing with nested data. By default, it will flatten structures like <code>{\"user\": {\"name\": \"X\"}}</code> into a <code>user.name</code> column. You can also provide custom functions to transform data on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>jsonl_stream</code> <p>An input stream (like a file handle) yielding JSONL strings.</p> required <code>output_stream</code> <p>An output stream (like <code>sys.stdout</code> or a file handle)            where the CSV data will be written.</p> required <code>flatten</code> <code>bool</code> <p>If <code>True</code>, nested dictionaries are flattened into columns             with dot-separated keys. Defaults to <code>True</code>.</p> <code>True</code> <code>flatten_sep</code> <code>str</code> <p>The separator to use when flattening keys.                Defaults to \".\".</p> <code>'.'</code> <code>column_functions</code> <code>dict</code> <p>A dictionary mapping column names to functions                      that will be applied to that column's data                      before writing to CSV. For example,                      <code>{\"price\": float}</code>.</p> <code>None</code> Source code in <code>ja/exporter.py</code> <pre><code>def jsonl_to_csv_stream(\n    jsonl_stream,\n    output_stream,\n    flatten: bool = True,\n    flatten_sep: str = \".\",\n    column_functions: dict = None,\n):\n    \"\"\"Convert a stream of JSONL data into a CSV stream.\n\n    This is a highly flexible function for exporting your data. It reads JSONL\n    records, intelligently discovers all possible headers (even if they vary\n    between lines), and writes to a CSV format.\n\n    It shines when dealing with nested data. By default, it will flatten\n    structures like `{\"user\": {\"name\": \"X\"}}` into a `user.name` column.\n    You can also provide custom functions to transform data on the fly.\n\n    Args:\n        jsonl_stream: An input stream (like a file handle) yielding JSONL strings.\n        output_stream: An output stream (like `sys.stdout` or a file handle)\n                       where the CSV data will be written.\n        flatten (bool): If `True`, nested dictionaries are flattened into columns\n                        with dot-separated keys. Defaults to `True`.\n        flatten_sep (str): The separator to use when flattening keys.\n                           Defaults to \".\".\n        column_functions (dict): A dictionary mapping column names to functions\n                                 that will be applied to that column's data\n                                 before writing to CSV. For example,\n                                 `{\"price\": float}`.\n    \"\"\"\n    if column_functions is None:\n        column_functions = {}\n\n    # First pass: Discover all possible headers from the entire stream\n    records = [json.loads(line) for line in jsonl_stream if line.strip()]\n    if not records:\n        return\n\n    # Apply column functions before flattening\n    for rec in records:\n        for col, func in column_functions.items():\n            if col in rec:\n                try:\n                    rec[col] = func(rec[col])\n                except Exception as e:\n                    # Optionally, log this error or handle it as needed\n                    print(\n                        f\"Error applying function to column '{col}' for a record: {e}\",\n                        file=sys.stderr,\n                    )\n\n    if flatten:\n        processed_records = [(_flatten_dict(rec, sep=flatten_sep)) for rec in records]\n    else:\n        processed_records = []\n        for rec in records:\n            processed_rec = {}\n            for k, v in rec.items():\n                if isinstance(v, (dict, list)):\n                    processed_rec[k] = json.dumps(v)\n                else:\n                    processed_rec[k] = v\n            processed_records.append(processed_rec)\n\n    # Discover all unique keys to form the CSV header\n    headers = []\n    header_set = set()\n    for rec in processed_records:\n        for key in rec.keys():\n            if key not in header_set:\n                header_set.add(key)\n                headers.append(key)\n\n    # Second pass: Write to the output stream\n    writer = csv.DictWriter(output_stream, fieldnames=headers, lineterminator=\"\\n\")\n    writer.writeheader()\n    writer.writerows(processed_records)\n</code></pre>"},{"location":"reference/#ja.importer","title":"<code>ja.importer</code>","text":"<p>Import data from other formats like CSV into the world of JSONL.</p> <p>This module is the bridge that brings your existing data into the JSONL Algebra ecosystem. It provides a collection of powerful functions for converting various data formats\u2014such as CSV or directories of individual JSON files\u2014into the clean, line-oriented JSONL format that <code>ja</code> is built to handle.</p>"},{"location":"reference/#ja.importer.csv_to_jsonl_lines","title":"<code>csv_to_jsonl_lines(csv_input_stream, has_header, infer_types=False)</code>","text":"<p>Convert a stream of CSV data into a stream of JSONL lines.</p> <p>This function reads CSV data and transforms each row into a JSON object. It can automatically handle headers to use as keys and can even infer the data types of your values, converting them from strings to numbers or booleans where appropriate.</p> <p>Parameters:</p> Name Type Description Default <code>csv_input_stream</code> <p>An input stream (like a file handle) containing CSV data.</p> required <code>has_header</code> <code>bool</code> <p>Set to <code>True</code> if the first row of the CSV is a header                that should be used for JSON keys.</p> required <code>infer_types</code> <code>bool</code> <p>If <code>True</code>, automatically convert values to <code>int</code>,                 <code>float</code>, <code>bool</code>, or <code>None</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Yields:</p> Type Description <p>A JSON-formatted string for each row in the CSV data.</p> Source code in <code>ja/importer.py</code> <pre><code>def csv_to_jsonl_lines(csv_input_stream, has_header: bool, infer_types: bool = False):\n    \"\"\"Convert a stream of CSV data into a stream of JSONL lines.\n\n    This function reads CSV data and transforms each row into a JSON object.\n    It can automatically handle headers to use as keys and can even infer the\n    data types of your values, converting them from strings to numbers or\n    booleans where appropriate.\n\n    Args:\n        csv_input_stream: An input stream (like a file handle) containing CSV data.\n        has_header (bool): Set to `True` if the first row of the CSV is a header\n                           that should be used for JSON keys.\n        infer_types (bool): If `True`, automatically convert values to `int`,\n                            `float`, `bool`, or `None`. Defaults to `False`.\n\n    Yields:\n        A JSON-formatted string for each row in the CSV data.\n    \"\"\"\n\n    def process_row(row):\n        if not infer_types:\n            return row\n        return {k: _infer_value(v) for k, v in row.items()}\n\n    if has_header:\n        # Use DictReader which handles headers automatically\n        reader = csv.DictReader(csv_input_stream)\n        for row in reader:\n            yield json.dumps(process_row(row))\n    else:\n        # Use the standard reader and manually create dictionaries\n        reader = csv.reader(csv_input_stream)\n        headers = []\n        try:\n            first_row = next(reader)\n            # Generate headers based on the number of columns in the first row\n            headers = [f\"col_{i}\" for i in range(len(first_row))]\n            # Yield the first row which we've already consumed\n            row_dict = dict(zip(headers, first_row))\n            yield json.dumps(process_row(row_dict))\n        except StopIteration:\n            return  # Handle empty file\n\n        # Yield the rest of the rows\n        for row in reader:\n            row_dict = dict(zip(headers, row))\n            yield json.dumps(process_row(row_dict))\n</code></pre>"},{"location":"reference/#ja.importer.dir_to_jsonl_lines","title":"<code>dir_to_jsonl_lines(dir_path)</code>","text":"<p>Stream a directory of .json or .jsonl files as a single JSONL stream.</p> <p>A handy utility for consolidating data. It reads all files ending in <code>.json</code> or <code>.jsonl</code> from a specified directory and yields each JSON object as a separate line. This is perfect for preparing a dataset that has been stored as many small files.</p> <ul> <li>For <code>.json</code> files, the entire file is treated as a single JSON object.</li> <li>For <code>.jsonl</code> files, each line is treated as a separate JSON object.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>The path to the directory to read.</p> required <p>Yields:</p> Type Description <p>A string for each JSON object found, ready for processing.</p> Source code in <code>ja/importer.py</code> <pre><code>def dir_to_jsonl_lines(dir_path):\n    \"\"\"Stream a directory of .json or .jsonl files as a single JSONL stream.\n\n    A handy utility for consolidating data. It reads all files ending in `.json`\n    or `.jsonl` from a specified directory and yields each JSON object as a\n    separate line. This is perfect for preparing a dataset that has been\n    stored as many small files.\n\n    - For `.json` files, the entire file is treated as a single JSON object.\n    - For `.jsonl` files, each line is treated as a separate JSON object.\n\n    Args:\n        dir_path (str): The path to the directory to read.\n\n    Yields:\n        A string for each JSON object found, ready for processing.\n    \"\"\"\n    for filename in sorted(os.listdir(dir_path)):\n        file_path = os.path.join(dir_path, filename)\n        if filename.endswith(\".json\"):\n            try:\n                with open(file_path, \"r\") as f:\n                    yield f.read().strip()\n            except (IOError, json.JSONDecodeError) as e:\n                print(f\"Error reading or parsing {file_path}: {e}\", file=sys.stderr)\n        elif filename.endswith(\".jsonl\"):\n            try:\n                with open(file_path, \"r\") as f:\n                    for line in f:\n                        yield line.strip()\n            except IOError as e:\n                print(f\"Error reading {file_path}: {e}\", file=sys.stderr)\n</code></pre>"},{"location":"reference/#ja.schema","title":"<code>ja.schema</code>","text":"<p>Discover the structure of your data automatically.</p> <p>This module provides powerful tools to infer a JSON Schema from your JSONL files. A schema acts as a blueprint for your data, describing its fields, types, and which fields are required. This is incredibly useful for validation, documentation, and ensuring data quality.</p>"},{"location":"reference/#ja.schema.add_required_fields","title":"<code>add_required_fields(schema, data_samples)</code>","text":"<p>Refine a schema by identifying which fields are always present.</p> <p>This function analyzes a list of data samples and updates the schema to mark fields as 'required' if they appear in every single sample. This process is applied recursively to nested objects, making the resulting schema more precise.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <p>The schema dictionary to modify in place.</p> required <code>data_samples</code> <p>A list of data samples to analyze for required fields.</p> required Example <p>If all samples in <code>data_samples</code> have 'name' and 'age' fields, this function adds <code>{\"required\": [\"age\", \"name\"]}</code> to the schema.</p> Source code in <code>ja/schema.py</code> <pre><code>def add_required_fields(schema, data_samples):\n    \"\"\"Refine a schema by identifying which fields are always present.\n\n    This function analyzes a list of data samples and updates the schema to mark\n    fields as 'required' if they appear in every single sample. This process is\n    applied recursively to nested objects, making the resulting schema more precise.\n\n    Args:\n        schema: The schema dictionary to modify in place.\n        data_samples: A list of data samples to analyze for required fields.\n\n    Example:\n        If all samples in `data_samples` have 'name' and 'age' fields, this\n        function adds `{\"required\": [\"age\", \"name\"]}` to the schema.\n    \"\"\"\n    if schema.get(\"type\") == \"object\" and \"properties\" in schema:\n        # For object schemas, find fields present in all samples\n        dict_samples = [s for s in data_samples if isinstance(s, dict)]\n        if dict_samples:\n            required_keys = set(dict_samples[0].keys())\n            for sample in dict_samples[1:]:\n                required_keys.intersection_update(sample.keys())\n            if required_keys:\n                schema[\"required\"] = sorted(list(required_keys))\n\n        # Recursively add required fields to nested object properties\n        for prop_name, prop_schema in schema[\"properties\"].items():\n            prop_samples = [s.get(prop_name) for s in dict_samples if prop_name in s]\n            if prop_samples:\n                add_required_fields(prop_schema, prop_samples)\n\n    elif schema.get(\"type\") == \"array\" and \"items\" in schema:\n        # For array schemas, collect all array items and add required fields\n        array_items = []\n        for sample in data_samples:\n            if isinstance(sample, list):\n                array_items.extend(sample)\n        if array_items:\n            add_required_fields(schema[\"items\"], array_items)\n</code></pre>"},{"location":"reference/#ja.schema.get_json_type","title":"<code>get_json_type(value)</code>","text":"<p>Determine the appropriate JSON Schema type for a given Python value.</p> <p>Maps Python types to their corresponding JSON Schema type names.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Any Python value.</p> required <p>Returns:</p> Type Description <p>The JSON Schema type name as a string.</p> Example <p>get_json_type(\"hello\") 'string' get_json_type(42) 'integer'</p> Source code in <code>ja/schema.py</code> <pre><code>def get_json_type(value):\n    \"\"\"Determine the appropriate JSON Schema type for a given Python value.\n\n    Maps Python types to their corresponding JSON Schema type names.\n\n    Args:\n        value: Any Python value.\n\n    Returns:\n        The JSON Schema type name as a string.\n\n    Example:\n        &gt;&gt;&gt; get_json_type(\"hello\")\n        'string'\n        &gt;&gt;&gt; get_json_type(42)\n        'integer'\n    \"\"\"\n    if isinstance(value, str):\n        return \"string\"\n    if isinstance(value, bool):\n        return \"boolean\"\n    if isinstance(value, int):\n        return \"integer\"\n    if isinstance(value, float):\n        return \"number\"\n    if value is None:\n        return \"null\"\n    if isinstance(value, list):\n        return \"array\"\n    if isinstance(value, dict):\n        return \"object\"\n    return \"unknown\"\n</code></pre>"},{"location":"reference/#ja.schema.infer_schema","title":"<code>infer_schema(data)</code>","text":"<p>Infer a complete JSON schema from a collection of data records.</p> <p>This is the main entry point for schema inference. Give it an iterable of JSON objects (like a list of dictionaries), and it will return a complete JSON Schema that describes the entire dataset. It automatically handles varying fields, mixed types, nested structures, and identifies required fields.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>An iterable of data records (typically dictionaries).</p> required <p>Returns:</p> Type Description <p>A JSON schema dictionary with <code>$schema</code>, type, properties, and required fields.</p> Example <p>data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}] schema = infer_schema(data) schema[\"properties\"][\"name\"] {'type': 'string'} schema[\"required\"]['age', 'name']</p> Source code in <code>ja/schema.py</code> <pre><code>def infer_schema(data):\n    \"\"\"Infer a complete JSON schema from a collection of data records.\n\n    This is the main entry point for schema inference. Give it an iterable of\n    JSON objects (like a list of dictionaries), and it will return a complete\n    JSON Schema that describes the entire dataset. It automatically handles\n    varying fields, mixed types, nested structures, and identifies required fields.\n\n    Args:\n        data: An iterable of data records (typically dictionaries).\n\n    Returns:\n        A JSON schema dictionary with `$schema`, type, properties, and required fields.\n\n    Example:\n        &gt;&gt;&gt; data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n        &gt;&gt;&gt; schema = infer_schema(data)\n        &gt;&gt;&gt; schema[\"properties\"][\"name\"]\n        {'type': 'string'}\n        &gt;&gt;&gt; schema[\"required\"]\n        ['age', 'name']\n    \"\"\"\n    records = list(data)\n    if not records:\n        return {\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"type\": \"object\",\n            \"properties\": {},\n        }\n\n    # Infer schema for each record\n    inferred_schemas = [infer_value_schema(rec) for rec in records]\n\n    # Merge all inferred schemas\n    merged_schema = None\n    for s in inferred_schemas:\n        merged_schema = merge_schemas(merged_schema, s)\n\n    # Add required fields recursively\n    if merged_schema:\n        add_required_fields(merged_schema, records)\n\n    # Add the meta-schema URL\n    final_schema = {\"$schema\": \"http://json-schema.org/draft-07/schema#\"}\n    if merged_schema:\n        final_schema.update(merged_schema)\n\n    return final_schema\n</code></pre>"},{"location":"reference/#ja.schema.infer_value_schema","title":"<code>infer_value_schema(value)</code>","text":"<p>Infer a JSON Schema for a single Python value.</p> <p>Creates a schema that describes the structure and type of the given value, handling nested objects and arrays recursively.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Any JSON-serializable Python value.</p> required <p>Returns:</p> Type Description <p>A JSON schema dictionary describing the value.</p> Example <p>infer_value_schema({\"name\": \"Alice\", \"age\": 30}) {'type': 'object', 'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}}}</p> Source code in <code>ja/schema.py</code> <pre><code>def infer_value_schema(value):\n    \"\"\"Infer a JSON Schema for a single Python value.\n\n    Creates a schema that describes the structure and type of the given value,\n    handling nested objects and arrays recursively.\n\n    Args:\n        value: Any JSON-serializable Python value.\n\n    Returns:\n        A JSON schema dictionary describing the value.\n\n    Example:\n        &gt;&gt;&gt; infer_value_schema({\"name\": \"Alice\", \"age\": 30})\n        {'type': 'object', 'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}}}\n    \"\"\"\n    type_name = get_json_type(value)\n    schema = {\"type\": type_name}\n    if type_name == \"object\":\n        schema[\"properties\"] = {k: infer_value_schema(v) for k, v in value.items()}\n    elif type_name == \"array\":\n        if value:\n            item_schema = None\n            for item in value:\n                item_schema = merge_schemas(item_schema, infer_value_schema(item))\n            if item_schema:\n                schema[\"items\"] = item_schema\n    return schema\n</code></pre>"},{"location":"reference/#ja.schema.merge_schemas","title":"<code>merge_schemas(s1, s2)</code>","text":"<p>Intelligently merge two JSON schemas into one.</p> <p>This is the secret sauce that allows schema inference to work across many different JSON objects, even if they have different fields or types. It handles type unions (e.g., a field that is sometimes a string, sometimes an integer) and recursively merges nested object properties and array item schemas.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <p>First JSON schema dictionary or None.</p> required <code>s2</code> <p>Second JSON schema dictionary or None.</p> required <p>Returns:</p> Type Description <p>A merged schema dictionary combining both inputs.</p> Example <p>s1 = {\"type\": \"string\"} s2 = {\"type\": \"integer\"} merge_schemas(s1, s2) {'type': ['integer', 'string']}</p> Source code in <code>ja/schema.py</code> <pre><code>def merge_schemas(s1, s2):\n    \"\"\"Intelligently merge two JSON schemas into one.\n\n    This is the secret sauce that allows schema inference to work across many\n    different JSON objects, even if they have different fields or types. It handles\n    type unions (e.g., a field that is sometimes a string, sometimes an integer)\n    and recursively merges nested object properties and array item schemas.\n\n    Args:\n        s1: First JSON schema dictionary or None.\n        s2: Second JSON schema dictionary or None.\n\n    Returns:\n        A merged schema dictionary combining both inputs.\n\n    Example:\n        &gt;&gt;&gt; s1 = {\"type\": \"string\"}\n        &gt;&gt;&gt; s2 = {\"type\": \"integer\"}\n        &gt;&gt;&gt; merge_schemas(s1, s2)\n        {'type': ['integer', 'string']}\n    \"\"\"\n    if s1 is None:\n        return s2\n    if s2 is None:\n        return s1\n    if s1 == s2:\n        return s1\n\n    # Merge types\n    type1 = s1.get(\"type\", [])\n    if not isinstance(type1, list):\n        type1 = [type1]\n    type2 = s2.get(\"type\", [])\n    if not isinstance(type2, list):\n        type2 = [type2]\n\n    merged_types = sorted(list(set(type1) | set(type2)))\n    if \"integer\" in merged_types and \"number\" in merged_types:\n        merged_types.remove(\"integer\")\n\n    merged_schema = {}\n    if len(merged_types) == 1:\n        merged_schema[\"type\"] = merged_types[0]\n    else:\n        merged_schema[\"type\"] = merged_types\n\n    # If both schemas could be objects, merge properties\n    if \"object\" in type1 and \"object\" in type2:\n        props1 = s1.get(\"properties\", {})\n        props2 = s2.get(\"properties\", {})\n        all_keys = set(props1.keys()) | set(props2.keys())\n        merged_props = {\n            key: merge_schemas(props1.get(key), props2.get(key)) for key in all_keys\n        }\n        if merged_props:\n            merged_schema[\"properties\"] = merged_props\n\n    # If both schemas could be arrays, merge items\n    if \"array\" in type1 and \"array\" in type2:\n        items1 = s1.get(\"items\")\n        items2 = s2.get(\"items\")\n        merged_items = merge_schemas(items1, items2)\n        if merged_items:\n            merged_schema[\"items\"] = merged_items\n\n    return merged_schema\n</code></pre>"},{"location":"reference/#ja.expr","title":"<code>ja.expr</code>","text":"<p>Expression parser for ja commands.</p> <p>This module provides a lightweight expression parser that allows intuitive syntax without quotes for most common cases.</p>"},{"location":"reference/#ja.expr.ExprEval","title":"<code>ExprEval</code>","text":"<p>Parse and evaluate expressions for filtering, comparison, and arithmetic.</p> Source code in <code>ja/expr.py</code> <pre><code>class ExprEval:\n    \"\"\"Parse and evaluate expressions for filtering, comparison, and arithmetic.\"\"\"\n\n    def __init__(self):\n        # Operators in precedence order (longest first to handle &gt;= before &gt;)\n        self.operators = [\n            (\"==\", operator.eq),\n            (\"!=\", operator.ne),\n            (\"&gt;=\", operator.ge),\n            (\"&lt;=\", operator.le),\n            (\"&gt;\", operator.gt),\n            (\"&lt;\", operator.lt),\n        ]\n\n    def parse_value(self, value_str: str) -&gt; Any:\n        \"\"\"Parse a value string into appropriate Python type.\n\n        Examples:\n            \"123\" -&gt; 123\n            \"12.5\" -&gt; 12.5\n            \"true\" -&gt; True\n            \"false\" -&gt; False\n            \"null\" -&gt; None\n            \"active\" -&gt; \"active\" (string)\n        \"\"\"\n        value_str = value_str.strip()\n\n        # Empty string\n        if not value_str:\n            return \"\"\n\n        # Boolean literals (case-insensitive)\n        if value_str.lower() == \"true\":\n            return True\n        if value_str.lower() == \"false\":\n            return False\n\n        # Null literal\n        if value_str.lower() in (\"null\", \"none\"):\n            return None\n\n        # Numbers\n        try:\n            if \".\" in value_str:\n                return float(value_str)\n            return int(value_str)\n        except ValueError:\n            pass\n\n        # Quoted strings (remove quotes)\n        if (value_str.startswith('\"') and value_str.endswith('\"')) or (\n            value_str.startswith(\"'\") and value_str.endswith(\"'\")\n        ):\n            return value_str[1:-1]\n\n        # Unquoted strings (the nice default!)\n        return value_str\n\n    def get_field_value(self, obj: Dict[str, Any], field_path: str) -&gt; Any:\n        \"\"\"Get value from nested object using dot notation.\n\n        Examples:\n            get_field_value({\"user\": {\"name\": \"Alice\"}}, \"user.name\") -&gt; \"Alice\"\n            get_field_value({\"items\": [{\"id\": 1}]}, \"items[0].id\") -&gt; 1\n        \"\"\"\n        if not field_path:\n            return obj\n\n        current = obj\n\n        # Handle array indexing and dots\n        parts = re.split(r\"\\.|\\[|\\]\", field_path)\n        parts = [p for p in parts if p]  # Remove empty strings\n\n        for part in parts:\n            if current is None:\n                return None\n\n            # Try as dict key\n            if isinstance(current, dict):\n                current = current.get(part)\n            # Try as array index\n            elif isinstance(current, list):\n                try:\n                    idx = int(part)\n                    current = current[idx] if 0 &lt;= idx &lt; len(current) else None\n                except (ValueError, IndexError):\n                    return None\n            else:\n                return None\n\n        return current\n\n    def set_field_value(self, obj: Dict[str, Any], field_path: str, value: Any) -&gt; None:\n        \"\"\"Set value in nested object using dot notation.\"\"\"\n        if not field_path:\n            return\n\n        parts = field_path.split(\".\")\n        current = obj\n\n        # Navigate to the parent of the target field\n        for part in parts[:-1]:\n            if part not in current:\n                current[part] = {}\n            current = current[part]\n\n        # Set the value\n        current[parts[-1]] = value\n\n    def evaluate_comparison(self, left: Any, op_str: str, right: Any) -&gt; bool:\n        \"\"\"Evaluate a comparison operation.\"\"\"\n        op_func = None\n        for op, func in self.operators:\n            if op == op_str:\n                op_func = func\n                break\n\n        if op_func is None:\n            raise ValueError(f\"Unknown operator: {op_str}\")\n\n        # Special handling for null comparisons\n        if left is None or right is None:\n            if op_str == \"==\":\n                return left == right\n            elif op_str == \"!=\":\n                return left != right\n            else:\n                return False\n\n        # Type coercion for comparison\n        try:\n            return op_func(left, right)\n        except (TypeError, ValueError):\n            # If comparison fails, try string comparison\n            try:\n                return op_func(str(left), str(right))\n            except:\n                return False\n\n    def evaluate(self, expr: str, context: Dict[str, Any]) -&gt; bool:\n        \"\"\"Parse and evaluate an expression.\n\n        Examples:\n            \"status == active\"\n            \"age &gt; 30\"\n            \"user.type == premium\"\n        \"\"\"\n        expr = expr.strip()\n\n        # Empty expression is false\n        if not expr:\n            return False\n\n        # Check for operators\n        for op_str, op_func in self.operators:\n            if op_str in expr:\n                # Split on the FIRST occurrence of the operator\n                parts = expr.split(op_str, 1)\n                if len(parts) == 2:\n                    left_expr = parts[0].strip()\n                    right_expr = parts[1].strip()\n\n                    # Left side is always a field path\n                    left_val = self.get_field_value(context, left_expr)\n\n                    # Right side: check if it's a field or a literal\n                    # A token is a field if it exists as a key in the context\n                    # and is not a boolean/null keyword.\n                    if right_expr in context and right_expr.lower() not in ['true', 'false', 'null', 'none']:\n                        # It's a field reference\n                        right_val = self.get_field_value(context, right_expr)\n                    else:\n                        # It's a literal value\n                        right_val = self.parse_value(right_expr)\n\n                    return self.evaluate_comparison(left_val, op_str, right_val)\n\n        # No operator found - treat as existence/truthiness check\n        value = self.get_field_value(context, expr)\n        return bool(value)\n\n    def evaluate_arithmetic(\n        self, expr: str, context: Dict[str, Any]\n    ) -&gt; Optional[float]:\n        \"\"\"Evaluate simple arithmetic expressions.\n\n        Examples:\n            \"amount * 1.1\"\n            \"score + bonus\"\n        \"\"\"\n        # Simple arithmetic support\n        for op, func in [\n            (\"*\", operator.mul),\n            (\"+\", operator.add),\n            (\"-\", operator.sub),\n            (\"/\", operator.truediv),\n        ]:\n            if op in expr:\n                parts = expr.split(op, 1)\n                if len(parts) == 2:\n                    left_str = parts[0].strip()\n                    right_str = parts[1].strip()\n\n                    # Get left value (field or literal)\n                    left_val = self.get_field_value(context, left_str)\n                    if left_val is None:\n                        left_val = self.parse_value(left_str)\n\n                    # Get right value (field or literal)\n                    right_val = self.get_field_value(context, right_str)\n                    if right_val is None:\n                        right_val = self.parse_value(right_str)\n\n                    try:\n                        return func(float(left_val), float(right_val))\n                    except (TypeError, ValueError):\n                        return None\n\n        # No operator - try as field or literal\n        val = self.get_field_value(context, expr)\n        if val is None:\n            val = self.parse_value(expr)\n\n        try:\n            return float(val)\n        except (TypeError, ValueError):\n            return None\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.evaluate","title":"<code>evaluate(expr, context)</code>","text":"<p>Parse and evaluate an expression.</p> <p>Examples:</p> <p>\"status == active\" \"age &gt; 30\" \"user.type == premium\"</p> Source code in <code>ja/expr.py</code> <pre><code>def evaluate(self, expr: str, context: Dict[str, Any]) -&gt; bool:\n    \"\"\"Parse and evaluate an expression.\n\n    Examples:\n        \"status == active\"\n        \"age &gt; 30\"\n        \"user.type == premium\"\n    \"\"\"\n    expr = expr.strip()\n\n    # Empty expression is false\n    if not expr:\n        return False\n\n    # Check for operators\n    for op_str, op_func in self.operators:\n        if op_str in expr:\n            # Split on the FIRST occurrence of the operator\n            parts = expr.split(op_str, 1)\n            if len(parts) == 2:\n                left_expr = parts[0].strip()\n                right_expr = parts[1].strip()\n\n                # Left side is always a field path\n                left_val = self.get_field_value(context, left_expr)\n\n                # Right side: check if it's a field or a literal\n                # A token is a field if it exists as a key in the context\n                # and is not a boolean/null keyword.\n                if right_expr in context and right_expr.lower() not in ['true', 'false', 'null', 'none']:\n                    # It's a field reference\n                    right_val = self.get_field_value(context, right_expr)\n                else:\n                    # It's a literal value\n                    right_val = self.parse_value(right_expr)\n\n                return self.evaluate_comparison(left_val, op_str, right_val)\n\n    # No operator found - treat as existence/truthiness check\n    value = self.get_field_value(context, expr)\n    return bool(value)\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.evaluate_arithmetic","title":"<code>evaluate_arithmetic(expr, context)</code>","text":"<p>Evaluate simple arithmetic expressions.</p> <p>Examples:</p> <p>\"amount * 1.1\" \"score + bonus\"</p> Source code in <code>ja/expr.py</code> <pre><code>def evaluate_arithmetic(\n    self, expr: str, context: Dict[str, Any]\n) -&gt; Optional[float]:\n    \"\"\"Evaluate simple arithmetic expressions.\n\n    Examples:\n        \"amount * 1.1\"\n        \"score + bonus\"\n    \"\"\"\n    # Simple arithmetic support\n    for op, func in [\n        (\"*\", operator.mul),\n        (\"+\", operator.add),\n        (\"-\", operator.sub),\n        (\"/\", operator.truediv),\n    ]:\n        if op in expr:\n            parts = expr.split(op, 1)\n            if len(parts) == 2:\n                left_str = parts[0].strip()\n                right_str = parts[1].strip()\n\n                # Get left value (field or literal)\n                left_val = self.get_field_value(context, left_str)\n                if left_val is None:\n                    left_val = self.parse_value(left_str)\n\n                # Get right value (field or literal)\n                right_val = self.get_field_value(context, right_str)\n                if right_val is None:\n                    right_val = self.parse_value(right_str)\n\n                try:\n                    return func(float(left_val), float(right_val))\n                except (TypeError, ValueError):\n                    return None\n\n    # No operator - try as field or literal\n    val = self.get_field_value(context, expr)\n    if val is None:\n        val = self.parse_value(expr)\n\n    try:\n        return float(val)\n    except (TypeError, ValueError):\n        return None\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.evaluate_comparison","title":"<code>evaluate_comparison(left, op_str, right)</code>","text":"<p>Evaluate a comparison operation.</p> Source code in <code>ja/expr.py</code> <pre><code>def evaluate_comparison(self, left: Any, op_str: str, right: Any) -&gt; bool:\n    \"\"\"Evaluate a comparison operation.\"\"\"\n    op_func = None\n    for op, func in self.operators:\n        if op == op_str:\n            op_func = func\n            break\n\n    if op_func is None:\n        raise ValueError(f\"Unknown operator: {op_str}\")\n\n    # Special handling for null comparisons\n    if left is None or right is None:\n        if op_str == \"==\":\n            return left == right\n        elif op_str == \"!=\":\n            return left != right\n        else:\n            return False\n\n    # Type coercion for comparison\n    try:\n        return op_func(left, right)\n    except (TypeError, ValueError):\n        # If comparison fails, try string comparison\n        try:\n            return op_func(str(left), str(right))\n        except:\n            return False\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.get_field_value","title":"<code>get_field_value(obj, field_path)</code>","text":"<p>Get value from nested object using dot notation.</p> <p>Examples:</p> <p>get_field_value({\"user\": {\"name\": \"Alice\"}}, \"user.name\") -&gt; \"Alice\" get_field_value({\"items\": [{\"id\": 1}]}, \"items[0].id\") -&gt; 1</p> Source code in <code>ja/expr.py</code> <pre><code>def get_field_value(self, obj: Dict[str, Any], field_path: str) -&gt; Any:\n    \"\"\"Get value from nested object using dot notation.\n\n    Examples:\n        get_field_value({\"user\": {\"name\": \"Alice\"}}, \"user.name\") -&gt; \"Alice\"\n        get_field_value({\"items\": [{\"id\": 1}]}, \"items[0].id\") -&gt; 1\n    \"\"\"\n    if not field_path:\n        return obj\n\n    current = obj\n\n    # Handle array indexing and dots\n    parts = re.split(r\"\\.|\\[|\\]\", field_path)\n    parts = [p for p in parts if p]  # Remove empty strings\n\n    for part in parts:\n        if current is None:\n            return None\n\n        # Try as dict key\n        if isinstance(current, dict):\n            current = current.get(part)\n        # Try as array index\n        elif isinstance(current, list):\n            try:\n                idx = int(part)\n                current = current[idx] if 0 &lt;= idx &lt; len(current) else None\n            except (ValueError, IndexError):\n                return None\n        else:\n            return None\n\n    return current\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.parse_value","title":"<code>parse_value(value_str)</code>","text":"<p>Parse a value string into appropriate Python type.</p> <p>Examples:</p> <p>\"123\" -&gt; 123 \"12.5\" -&gt; 12.5 \"true\" -&gt; True \"false\" -&gt; False \"null\" -&gt; None \"active\" -&gt; \"active\" (string)</p> Source code in <code>ja/expr.py</code> <pre><code>def parse_value(self, value_str: str) -&gt; Any:\n    \"\"\"Parse a value string into appropriate Python type.\n\n    Examples:\n        \"123\" -&gt; 123\n        \"12.5\" -&gt; 12.5\n        \"true\" -&gt; True\n        \"false\" -&gt; False\n        \"null\" -&gt; None\n        \"active\" -&gt; \"active\" (string)\n    \"\"\"\n    value_str = value_str.strip()\n\n    # Empty string\n    if not value_str:\n        return \"\"\n\n    # Boolean literals (case-insensitive)\n    if value_str.lower() == \"true\":\n        return True\n    if value_str.lower() == \"false\":\n        return False\n\n    # Null literal\n    if value_str.lower() in (\"null\", \"none\"):\n        return None\n\n    # Numbers\n    try:\n        if \".\" in value_str:\n            return float(value_str)\n        return int(value_str)\n    except ValueError:\n        pass\n\n    # Quoted strings (remove quotes)\n    if (value_str.startswith('\"') and value_str.endswith('\"')) or (\n        value_str.startswith(\"'\") and value_str.endswith(\"'\")\n    ):\n        return value_str[1:-1]\n\n    # Unquoted strings (the nice default!)\n    return value_str\n</code></pre>"},{"location":"reference/#ja.expr.ExprEval.set_field_value","title":"<code>set_field_value(obj, field_path, value)</code>","text":"<p>Set value in nested object using dot notation.</p> Source code in <code>ja/expr.py</code> <pre><code>def set_field_value(self, obj: Dict[str, Any], field_path: str, value: Any) -&gt; None:\n    \"\"\"Set value in nested object using dot notation.\"\"\"\n    if not field_path:\n        return\n\n    parts = field_path.split(\".\")\n    current = obj\n\n    # Navigate to the parent of the target field\n    for part in parts[:-1]:\n        if part not in current:\n            current[part] = {}\n        current = current[part]\n\n    # Set the value\n    current[parts[-1]] = value\n</code></pre>"},{"location":"advanced/repl/","title":"Interactive REPL Guide","text":"<p>The <code>ja</code> REPL (Read-Eval-Print Loop) provides an interactive environment for building data pipelines step-by-step. It's perfect for data exploration, prototyping complex queries, and learning the tool.</p>"},{"location":"advanced/repl/#getting-started","title":"Getting Started","text":"<p>Launch the REPL:</p> <pre><code>ja repl\n</code></pre> <p>You'll see:</p> <pre><code>Welcome to ja REPL! \ud83d\ude80 Type 'help' for commands, 'exit' to quit.\nja&gt;\n</code></pre>"},{"location":"advanced/repl/#basic-commands","title":"Basic Commands","text":""},{"location":"advanced/repl/#setting-data-source","title":"Setting Data Source","text":"<pre><code>ja&gt; from orders.jsonl\nInput source set to: orders.jsonl\n\nja&gt; from stdin\nInput source set to: stdin\n</code></pre>"},{"location":"advanced/repl/#building-a-pipeline","title":"Building a Pipeline","text":"<pre><code>ja&gt; from orders.jsonl\nja&gt; select status == \"shipped\"\nAdded: select status == \"shipped\"\nja&gt; groupby customer\nAdded: groupby customer\nja&gt; agg total=sum(amount)\nAdded: agg total=sum(amount)\n</code></pre>"},{"location":"advanced/repl/#viewing-the-pipeline","title":"Viewing the Pipeline","text":"<pre><code>ja&gt; pipeline\nCurrent pipeline:\n  Input: orders.jsonl\n  1. select status == \"shipped\"\n  2. groupby customer\n  3. agg total=sum(amount)\n</code></pre>"},{"location":"advanced/repl/#executing-the-pipeline","title":"Executing the Pipeline","text":"<pre><code>ja&gt; execute\nExecuting: ja select 'status == \"shipped\"' orders.jsonl | ja groupby customer - | ja agg total=sum(amount) -\n\n--- Output ---\n{\"customer\": \"Alice\", \"total\": 179.98}\n{\"customer\": \"Charlie\", \"total\": 199.99}\n--------------\n</code></pre>"},{"location":"advanced/repl/#advanced-features","title":"Advanced Features","text":""},{"location":"advanced/repl/#no-quotes-required","title":"No Quotes Required","text":"<p>Unlike the command line, expressions don't need quotes in the REPL:</p> <pre><code># Command line\nja select 'age &gt; 30 and status == \"active\"' users.jsonl\n\n# REPL\nja&gt; select age &gt; 30 and status == \"active\"\n</code></pre>"},{"location":"advanced/repl/#limited-output","title":"Limited Output","text":"<p>View just a few lines while building your pipeline:</p> <pre><code>ja&gt; execute --lines=5\n</code></pre>"},{"location":"advanced/repl/#generate-scripts","title":"Generate Scripts","text":"<p>See the equivalent bash script:</p> <pre><code>ja&gt; compile\n\n--- Compiled Bash Script ---\n#!/bin/bash\n# Generated by ja REPL\nja select 'status == \"shipped\"' orders.jsonl | \\\n  ja groupby customer - | \\\n  ja agg 'total=sum(amount)' -\n--------------------------\n</code></pre>"},{"location":"advanced/repl/#reset-pipeline","title":"Reset Pipeline","text":"<p>Start over:</p> <pre><code>ja&gt; reset\nPipeline reset.\n</code></pre>"},{"location":"advanced/repl/#data-exploration-workflow","title":"Data Exploration Workflow","text":""},{"location":"advanced/repl/#1-start-with-data-inspection","title":"1. Start with Data Inspection","text":"<pre><code>ja&gt; from large_dataset.jsonl\nja&gt; execute --lines=3\n# See the structure and sample data\n</code></pre>"},{"location":"advanced/repl/#2-build-filters-incrementally","title":"2. Build Filters Incrementally","text":"<pre><code>ja&gt; select timestamp &gt; \"2024-01-01\"\nja&gt; execute --lines=5\n# Check the filtering is working\n\nja&gt; select amount &gt; 100\nja&gt; execute --lines=5\n# Add another filter and verify\n</code></pre>"},{"location":"advanced/repl/#3-add-transformations","title":"3. Add Transformations","text":"<pre><code>ja&gt; project customer,amount,month=timestamp[0:7]\nja&gt; execute --lines=5\n# See the transformed data\n</code></pre>"},{"location":"advanced/repl/#4-group-and-aggregate","title":"4. Group and Aggregate","text":"<pre><code>ja&gt; groupby month\nja&gt; execute --lines=10\n# Inspect the grouping\n\nja&gt; groupby customer\nja&gt; execute --lines=10\n# Add second level grouping\n\nja&gt; agg revenue=sum(amount),orders=count\nja&gt; execute\n# Final aggregation\n</code></pre>"},{"location":"advanced/repl/#complex-query-building","title":"Complex Query Building","text":""},{"location":"advanced/repl/#multi-join-analysis","title":"Multi-Join Analysis","text":"<pre><code>ja&gt; from orders.jsonl\nja&gt; join customers.jsonl --on customer_id=id\nja&gt; execute --lines=3\n# Check the join worked\n\nja&gt; join products.jsonl --on product_id=id  \nja&gt; execute --lines=3\n# Add second join\n\nja&gt; select order_date &gt; \"2024-01-01\"\nja&gt; project customer.name,product.category,total=quantity*price\nja&gt; groupby customer.tier\nja&gt; groupby product.category\nja&gt; agg revenue=sum(total),orders=count\nja&gt; execute\n</code></pre>"},{"location":"advanced/repl/#time-series-analysis","title":"Time Series Analysis","text":"<pre><code>ja&gt; from events.jsonl\nja&gt; project timestamp,user_id,event_type,date=timestamp[0:10]\nja&gt; execute --lines=5\n\nja&gt; groupby date\nja&gt; execute --lines=10\n# Check daily grouping\n\nja&gt; groupby event_type\nja&gt; agg events=count,unique_users=count_distinct(user_id)\nja&gt; sort date,event_type\nja&gt; execute\n</code></pre>"},{"location":"advanced/repl/#debugging-and-troubleshooting","title":"Debugging and Troubleshooting","text":""},{"location":"advanced/repl/#check-each-step","title":"Check Each Step","text":"<pre><code>ja&gt; from data.jsonl\nja&gt; select complicated_condition\nja&gt; execute --lines=1\n# Is anything matching?\n\nja&gt; pipeline\n# Review the steps so far\n\nja&gt; reset\n# Start over if needed\n</code></pre>"},{"location":"advanced/repl/#inspect-intermediate-results","title":"Inspect Intermediate Results","text":"<pre><code>ja&gt; groupby category\nja&gt; execute --lines=5\n# See the grouping metadata\n\nja&gt; select _group_size &gt; 10\nja&gt; execute --lines=5\n# Filter based on group size\n</code></pre>"},{"location":"advanced/repl/#common-issues","title":"Common Issues","text":"<p>No Output After Filtering:</p> <pre><code>ja&gt; from data.jsonl\nja&gt; execute --lines=5\n# Check source data\n\nja&gt; select some_condition\nja&gt; execute --lines=5\n# See if filter is too restrictive\n</code></pre> <p>Unexpected Grouping:</p> <pre><code>ja&gt; groupby field\nja&gt; project field,_groups,_group_size\nja&gt; execute --lines=10\n# Inspect the grouping structure\n</code></pre>"},{"location":"advanced/repl/#repl-specific-features","title":"REPL-Specific Features","text":""},{"location":"advanced/repl/#smart-expression-parsing","title":"Smart Expression Parsing","text":"<p>The REPL intelligently parses expressions:</p> <pre><code># These all work without quotes\nja&gt; select age &gt; 30\nja&gt; select status == \"active\" and score &gt;= 80\nja&gt; project name,total=price*quantity\nja&gt; agg revenue=sum(amount),count\n</code></pre>"},{"location":"advanced/repl/#command-history","title":"Command History","text":"<p>Use arrow keys to navigate command history:</p> <ul> <li>\u2191/\u2193: Previous/next command</li> <li>Ctrl+C: Interrupt current command (continues REPL)</li> <li>Ctrl+D or <code>exit</code>: Exit REPL</li> </ul>"},{"location":"advanced/repl/#auto-completion","title":"Auto-completion","text":"<p>The REPL provides context-aware suggestions for:</p> <ul> <li>Command names</li> <li>Field names (when available)</li> <li>Common aggregation functions</li> </ul>"},{"location":"advanced/repl/#scripting-from-repl","title":"Scripting from REPL","text":""},{"location":"advanced/repl/#save-pipelines","title":"Save Pipelines","text":"<p>Generate scripts for reuse:</p> <pre><code>ja&gt; from orders.jsonl\nja&gt; select status == \"shipped\"\nja&gt; groupby customer\nja&gt; agg total=sum(amount)\nja&gt; compile &gt; analysis_script.sh\n</code></pre>"},{"location":"advanced/repl/#parameterized-queries","title":"Parameterized Queries","text":"<p>Build templates:</p> <pre><code># In the REPL, build your pipeline\nja&gt; from data.jsonl\nja&gt; select 'date &gt;= \"$START_DATE\"'\nja&gt; groupby category  \nja&gt; agg total=sum(amount)\nja&gt; compile\n\n# Then edit the generated script to use variables\n</code></pre>"},{"location":"advanced/repl/#best-practices","title":"Best Practices","text":""},{"location":"advanced/repl/#1-start-small","title":"1. Start Small","text":"<pre><code>ja&gt; from large_file.jsonl\nja&gt; execute --lines=5\n# Always check a sample first\n</code></pre>"},{"location":"advanced/repl/#2-build-incrementally","title":"2. Build Incrementally","text":"<pre><code># Add operations one at a time\nja&gt; select condition1\nja&gt; execute --lines=5\nja&gt; select condition2  \nja&gt; execute --lines=5\nja&gt; groupby field\nja&gt; execute --lines=10\n</code></pre>"},{"location":"advanced/repl/#3-use-the-pipeline-view","title":"3. Use the Pipeline View","text":"<pre><code>ja&gt; pipeline\n# Regular check of what you've built\n</code></pre>"},{"location":"advanced/repl/#4-save-complex-queries","title":"4. Save Complex Queries","text":"<pre><code>ja&gt; compile &gt; my_analysis.sh\n# Don't lose complex work\n</code></pre>"},{"location":"advanced/repl/#5-explore-before-committing","title":"5. Explore Before Committing","text":"<pre><code># Try different approaches\nja&gt; groupby customer\nja&gt; execute --lines=5\nja&gt; reset\nja&gt; groupby product  \nja&gt; execute --lines=5\n# Compare different groupings\n</code></pre>"},{"location":"advanced/repl/#integration-with-external-tools","title":"Integration with External Tools","text":""},{"location":"advanced/repl/#pipe-to-external-commands","title":"Pipe to External Commands","text":"<pre><code>ja&gt; execute | jq '.[] | select(.revenue &gt; 1000)'\nja&gt; execute | head -20\nja&gt; execute | sort -n\n</code></pre>"},{"location":"advanced/repl/#export-results","title":"Export Results","text":"<pre><code>ja&gt; execute &gt; results.jsonl\nja&gt; compile &gt; analysis.sh\n</code></pre>"},{"location":"advanced/repl/#help-and-documentation","title":"Help and Documentation","text":""},{"location":"advanced/repl/#in-repl-help","title":"In-REPL Help","text":"<pre><code>ja&gt; help\n# Comprehensive help with examples\n\nja&gt; help select\n# Command-specific help (if available)\n</code></pre>"},{"location":"advanced/repl/#examples-and-tips","title":"Examples and Tips","text":"<p>The REPL help includes:</p> <ul> <li>Command examples</li> <li>Expression syntax</li> <li>Aggregation functions</li> <li>Common patterns</li> <li>Troubleshooting tips</li> </ul>"},{"location":"advanced/repl/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"advanced/repl/#data-quality-exploration","title":"Data Quality Exploration","text":"<pre><code>ja&gt; from messy_data.jsonl\nja&gt; project name,email,age\nja&gt; execute --lines=10\n# See what fields look like\n\nja&gt; agg count,nulls=count_if(email==null),invalids=count_if(!email.contains(\"@\"))\nja&gt; execute\n# Check data quality\n\nja&gt; select email != null and email.contains(\"@\") and age &gt; 0\nja&gt; execute --lines=5\n# Clean data step by step\n</code></pre>"},{"location":"advanced/repl/#schema-discovery","title":"Schema Discovery","text":"<pre><code>ja&gt; from unknown_data.jsonl\nja&gt; execute --lines=1\n# See the structure\n\nja&gt; project _keys=keys(.)\nja&gt; execute --lines=10\n# See all field names\n\nja&gt; groupby typeof(some_field)\nja&gt; agg count\nja&gt; execute\n# Check field types\n</code></pre>"},{"location":"advanced/repl/#performance-testing","title":"Performance Testing","text":"<pre><code>ja&gt; from huge_file.jsonl\nja&gt; select simple_condition\nja&gt; execute --lines=1000\n# Test filter performance\n\nja&gt; project field1,field2,field3\nja&gt; execute --lines=1000  \n# Test projection performance\n</code></pre>"},{"location":"advanced/repl/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Make your queries faster</li> <li>Complex Expressions - Advanced filtering</li> <li>Cookbook Examples - Real-world patterns</li> </ul>"},{"location":"concepts/all-i-kept/","title":"All I Kept Was a <code>.jsonl</code> File","text":"<p>I Spent Weeks Designing a \u201cLightweight Database.\u201d  All I Kept Was a <code>.jsonl</code> File.</p>"},{"location":"concepts/all-i-kept/#1-the-premise","title":"1. The Premise","text":"<p>I wanted a tiny, version\u2011control\u2011friendly database format\u2014simpler than SQLite, richer than CSV. \u201cEasy,\u201d I thought. \u201cCSV plus a JSON schema sidecar, maybe a CLI, relational joins, views\u2026\u201d</p> <p>Spoiler: every new layer solved a problem the previous layer created.</p>"},{"location":"concepts/all-i-kept/#2-the-detour-adding-layers","title":"2. The Detour\u202f\u2014\u202fAdding Layers","text":"Layer Why I Added It What Went Wrong CSV + per\u2011table JSON schema Types, keys, validation Duplication; header drift; quoting hell Relation objects with streaming iterators Lazy memory use Exhausted iterators, subtle bugs View files &amp; materialization logic \u201cSQL\u2011like\u201d power IO noise, unclear semantics Full CLI w/ persist commands Nice UX Just wrappers for wrappers <p>Each addition felt clever\u2014until it didn\u2019t.</p>"},{"location":"concepts/all-i-kept/#3-the-unbuilding-phase","title":"3. The Un\u2011building Phase","text":"<p>I asked, \u201cWhat irreducible need remains if I delete this layer?\u201d Piece by piece I removed:</p> <ul> <li>per\u2011table schemas</li> <li>reset logic</li> <li>auto\u2011persist tricks</li> <li>even the catalog for foreign keys</li> </ul> <p>\u2026and nothing essential broke.</p>"},{"location":"concepts/all-i-kept/#4-what-survived","title":"4. What Survived","text":"<p>A single convention:</p> <p>One JSON object per line, in a file named <code>*.jsonl</code>.</p> <p>That\u2019s it. Need really advanced filtering? Pipe through <code>jq</code> or run a five\u2011line Python script.</p>"},{"location":"concepts/all-i-kept/#5-why-simpler-won","title":"5. Why Simpler Won","text":"Perks of <code>.jsonl</code> Hidden Cost Avoided Human\u2011diffable text No binary dumps Streamable GBs No exhausted generators Nested structures free No quoting gymnastics Zero tooling lock\u2011in No API promises to maintain"},{"location":"concepts/all-i-kept/#6-the-real-lesson-social-coordination","title":"6. The Real Lesson: Social Coordination","text":"<p>Most \u201cbig wins\u201d in data tooling aren\u2019t technical genius; they\u2019re social:</p> <ul> <li>CSV won because everyone agreed to tolerate its warts.</li> <li>Markdown won because \u201cgood enough\u201d beats rich\u2011text battles.</li> <li>Git won because a single mental model (\u201ccommit\u201d) spread like a meme.</li> </ul> <p>If a convention is:</p> <ol> <li>Plaintext</li> <li>Easy to explain in one sentence</li> <li>Good\u2011enough interop</li> </ol> <p>\u2026then the coordination win dwarfs any feature checklist.</p> <p><code>.jsonl</code> clears that bar. Anything I\u2019d add would mostly be me having fun\u2014not solving a mass pain\u2011point.</p>"},{"location":"concepts/all-i-kept/#7-what-i-actually-shipped","title":"7. What I Actually Shipped","text":"<p>Nothing\u2014except this insight:</p> <p>\u201cPut each record on its own JSON line, commit it, and move on.\u201d</p> <p>I closed my repo, published this post, and kept the loader script (\\~30\u202fLOC) in a gist.</p>"},{"location":"concepts/all-i-kept/#8-when-not-to-use-jsonl","title":"8. When Not to Use <code>.jsonl</code>","text":"<ul> <li>You need ACID transactions \u2192 SQLite/DuckDB</li> <li>You need strict typing &amp; joins at scale \u2192 Postgres / BigQuery</li> <li>You need columnar analytics \u2192 Parquet + DuckDB</li> </ul> <p>But for configs, logs, small/medium datasets, and exploratory hacks: <code>.jsonl</code> + <code>jq</code> + Git is unbeatable in simplicity\u2011per\u2011byte.</p>"},{"location":"concepts/all-i-kept/#9-takeaways","title":"9. Takeaways","text":"<ol> <li>Architect by deletion. Start adding; keep deleting until pain appears.</li> <li>Ship conventions, not frameworks. Naming the pattern is often enough.</li> <li>Judge by coordination cost. The simplest format lots of people accept beats a genius format nobody standardizes on.</li> </ol> <p>I didn\u2019t deliver a shiny package. (Well, I did deliver a shiny JSONL. I'm not a monster.)</p>"},{"location":"concepts/chained-groups/","title":"Chained Grouping: Deep Dive","text":"<p>Chained groupby is one of <code>ja</code>'s most innovative features. It allows you to build multi-level aggregations while maintaining the JSONL format throughout the pipeline.</p>"},{"location":"concepts/chained-groups/#the-problem-with-traditional-grouping","title":"The Problem with Traditional Grouping","text":"<p>Most data tools force you to specify all grouping levels upfront:</p> <pre><code>-- SQL requires specifying all levels at once\nSELECT region, product, SUM(amount)\nFROM sales\nGROUP BY region, product\n</code></pre> <p>This approach has limitations:</p> <ul> <li>Can't filter between grouping levels</li> <li>Can't inspect intermediate groupings</li> <li>Hard to build complex hierarchies progressively</li> <li>Doesn't compose well with other operations</li> </ul>"},{"location":"concepts/chained-groups/#the-ja-solution-metadata-preserving-grouping","title":"The ja Solution: Metadata-Preserving Grouping","text":"<p>Instead of immediately aggregating, <code>ja groupby</code> adds metadata to preserve grouping information:</p> <pre><code># First level: group by region\nja groupby region sales.jsonl\n</code></pre> <p>This adds metadata to each row:</p> <pre><code>{\n  \"sale_id\": 1,\n  \"region\": \"North\",\n  \"product\": \"Widget\",\n  \"amount\": 100,\n  \"_groups\": [{\"field\": \"region\", \"value\": \"North\"}],\n  \"_group_size\": 3,\n  \"_group_index\": 0\n}\n</code></pre>"},{"location":"concepts/chained-groups/#key-metadata-fields","title":"Key Metadata Fields","text":"<ul> <li><code>_groups</code>: Array of grouping levels with field names and values</li> <li><code>_group_size</code>: Number of rows in this group</li> <li><code>_group_index</code>: This row's position within its group</li> </ul>"},{"location":"concepts/chained-groups/#building-multi-level-hierarchies","title":"Building Multi-Level Hierarchies","text":""},{"location":"concepts/chained-groups/#two-level-grouping","title":"Two-Level Grouping","text":"<pre><code>ja groupby region sales.jsonl | ja groupby product\n</code></pre> <p>The second groupby extends the metadata:</p> <pre><code>{\n  \"sale_id\": 1,\n  \"region\": \"North\", \n  \"product\": \"Widget\",\n  \"amount\": 100,\n  \"_groups\": [\n    {\"field\": \"region\", \"value\": \"North\"},\n    {\"field\": \"product\", \"value\": \"Widget\"}\n  ],\n  \"_group_size\": 2,\n  \"_group_index\": 0\n}\n</code></pre>"},{"location":"concepts/chained-groups/#three-level-and-beyond","title":"Three-Level and Beyond","text":"<pre><code>ja groupby year sales.jsonl \\\n  | ja groupby region \\\n  | ja groupby product \\\n  | ja groupby month\n</code></pre> <p>Each level adds to the <code>_groups</code> array, creating a complete hierarchy.</p>"},{"location":"concepts/chained-groups/#the-power-of-composition","title":"The Power of Composition","text":""},{"location":"concepts/chained-groups/#filter-between-levels","title":"Filter Between Levels","text":"<pre><code># Group by user, filter active groups, then group by product\nja groupby user_id transactions.jsonl \\\n  | ja select '_group_size &gt; 5' \\\n  | ja groupby product_id \\\n  | ja agg total=sum(amount)\n</code></pre> <p>This finds users with more than 5 transactions, then analyzes their product preferences.</p>"},{"location":"concepts/chained-groups/#transform-between-levels","title":"Transform Between Levels","text":"<pre><code># Add computed fields between groupings\nja groupby customer orders.jsonl \\\n  | ja project customer,order_id,amount,is_large=amount&gt;100 \\\n  | ja groupby is_large \\\n  | ja agg count,avg_amount=avg(amount)\n</code></pre>"},{"location":"concepts/chained-groups/#inspect-intermediate-results","title":"Inspect Intermediate Results","text":"<pre><code># See the grouping structure\nja groupby region sales.jsonl | ja groupby product | head -3\n</code></pre> <p>You can examine the data at any stage to understand the grouping.</p>"},{"location":"concepts/chained-groups/#aggregation-the-final-step","title":"Aggregation: The Final Step","text":"<p>When you're ready to aggregate, <code>ja agg</code> uses the metadata to produce clean results:</p> <pre><code>ja groupby region sales.jsonl \\\n  | ja groupby product \\\n  | ja agg total=sum(amount),count,avg=avg(amount)\n</code></pre> <p>Output:</p> <pre><code>{\"region\": \"North\", \"product\": \"Widget\", \"total\": 500, \"count\": 3, \"avg\": 166.67}\n{\"region\": \"North\", \"product\": \"Gadget\", \"total\": 300, \"count\": 2, \"avg\": 150.00}\n{\"region\": \"South\", \"product\": \"Widget\", \"total\": 400, \"count\": 2, \"avg\": 200.00}\n</code></pre> <p>Notice how all grouping fields are preserved in the output.</p>"},{"location":"concepts/chained-groups/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"concepts/chained-groups/#conditional-aggregation-within-groups","title":"Conditional Aggregation Within Groups","text":"<pre><code>ja groupby department employees.jsonl \\\n  | ja agg \\\n      total_salary=sum(salary), \\\n      senior_count=count_if(level==\"senior\"), \\\n      avg_senior_salary=avg_if(salary,level==\"senior\")\n</code></pre>"},{"location":"concepts/chained-groups/#nested-field-grouping","title":"Nested Field Grouping","text":"<pre><code>ja groupby user.region sales.jsonl \\\n  | ja groupby product.category \\\n  | ja agg revenue=sum(amount)\n</code></pre>"},{"location":"concepts/chained-groups/#time-series-grouping","title":"Time-Series Grouping","text":"<pre><code>ja groupby date sales.jsonl \\\n  | ja project date,customer,amount,month=date[0:7] \\\n  | ja groupby month \\\n  | ja agg daily_avg=avg(amount)\n</code></pre>"},{"location":"concepts/chained-groups/#implementation-details","title":"Implementation Details","text":""},{"location":"concepts/chained-groups/#metadata-structure","title":"Metadata Structure","text":"<p>The <code>_groups</code> array maintains the complete grouping hierarchy:</p> <pre><code>\"_groups\": [\n  {\"field\": \"region\", \"value\": \"North\"},\n  {\"field\": \"product\", \"value\": \"Widget\"},\n  {\"field\": \"size\", \"value\": \"Large\"}\n]\n</code></pre>"},{"location":"concepts/chained-groups/#efficient-grouping","title":"Efficient Grouping","text":"<p>Internally, <code>ja</code> uses Python's <code>defaultdict</code> and tuple keys for efficient grouping:</p> <pre><code># Conceptual implementation\ngroup_key = tuple((g[\"field\"], g[\"value\"]) for g in row[\"_groups\"])\ngroups[group_key].append(clean_row)\n</code></pre>"},{"location":"concepts/chained-groups/#memory-efficiency","title":"Memory Efficiency","text":"<p>Because <code>ja</code> processes data streaming, even complex multi-level groupings don't require loading entire datasets into memory.</p>"},{"location":"concepts/chained-groups/#comparison-with-direct-aggregation","title":"Comparison with Direct Aggregation","text":""},{"location":"concepts/chained-groups/#when-to-use-chained-grouping","title":"When to Use Chained Grouping","text":"<p>Use chained grouping when you need:</p> <ul> <li>Multi-level hierarchies: More than one grouping dimension</li> <li>Intermediate filtering: Filtering between grouping levels  </li> <li>Progressive building: Building up complex queries step by step</li> <li>Inspection: Examining intermediate grouping states</li> <li>Composition: Integrating with other operations</li> </ul> <pre><code># Complex pipeline with chained grouping\ncat transactions.jsonl \\\n  | ja select 'amount &gt; 10' \\\n  | ja join customers.jsonl --on customer_id=id \\\n  | ja groupby tier \\\n  | ja select '_group_size &gt; 100' \\\n  | ja groupby month \\\n  | ja agg revenue=sum(amount),customers=count_distinct(customer_id)\n</code></pre>"},{"location":"concepts/chained-groups/#when-to-use-direct-aggregation","title":"When to Use Direct Aggregation","text":"<p>Use the <code>--agg</code> flag for simple, single-level aggregations:</p> <pre><code># Simple aggregation - more efficient\nja groupby category --agg 'total=sum(amount),count' sales.jsonl\n</code></pre>"},{"location":"concepts/chained-groups/#real-world-examples","title":"Real-World Examples","text":""},{"location":"concepts/chained-groups/#e-commerce-analysis","title":"E-commerce Analysis","text":"<pre><code># Multi-dimensional sales analysis\ncat orders.jsonl \\\n  | ja select 'status == \"completed\"' \\\n  | ja join products.jsonl --on product_id=id \\\n  | ja groupby category \\\n  | ja groupby price_tier \\\n  | ja groupby month \\\n  | ja agg \\\n      revenue=sum(total), \\\n      orders=count, \\\n      unique_customers=count_distinct(customer_id), \\\n      avg_order=avg(total)\n</code></pre>"},{"location":"concepts/chained-groups/#log-analysis","title":"Log Analysis","text":"<pre><code># Server log analysis by endpoint and status\ncat access.log.jsonl \\\n  | ja select 'timestamp &gt; \"2024-01-01\"' \\\n  | ja groupby endpoint \\\n  | ja select '_group_size &gt; 1000' \\\n  | ja groupby status_code \\\n  | ja agg \\\n      requests=count, \\\n      avg_response_time=avg(response_time), \\\n      error_rate=count_if(status_code&gt;=400)/count\n</code></pre>"},{"location":"concepts/chained-groups/#user-behavior-analysis","title":"User Behavior Analysis","text":"<pre><code># User engagement analysis\ncat events.jsonl \\\n  | ja groupby user_id \\\n  | ja select '_group_size &gt;= 10' \\\n  | ja groupby event_type \\\n  | ja groupby date \\\n  | ja agg \\\n      daily_events=count, \\\n      unique_users=count_distinct(user_id)\n</code></pre>"},{"location":"concepts/chained-groups/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with single-level grouping, then add levels</li> <li>Filter Early: Use <code>select</code> before grouping to reduce data size  </li> <li>Inspect Intermediate Results: Use <code>head</code> or <code>--lines</code> to check grouping structure</li> <li>Use Meaningful Names: Choose descriptive names for aggregated fields</li> <li>Leverage Metadata: Use <code>_group_size</code> for filtering and analysis</li> </ol>"},{"location":"concepts/chained-groups/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/chained-groups/#common-issues","title":"Common Issues","text":"<p>Empty Results After Chaining:</p> <pre><code># Check each step\nja groupby region data.jsonl | ja agg count\nja groupby region data.jsonl | ja groupby product | ja agg count\n</code></pre> <p>Unexpected Grouping Values:</p> <pre><code># Inspect the grouping metadata\nja groupby region data.jsonl | ja project region,_groups | head -5\n</code></pre> <p>Performance Issues:</p> <pre><code># Consider filtering early\nja select 'relevant_condition' data.jsonl | ja groupby region | ja groupby product\n</code></pre>"},{"location":"concepts/chained-groups/#whats-next","title":"What's Next?","text":"<ul> <li>Expression Language - Learn advanced filtering</li> <li>Performance Guide - Optimize complex pipelines</li> <li>Cookbook Examples - See real-world usage</li> </ul>"},{"location":"concepts/jsonl-algebra/","title":"The Algebra Behind ja","text":""},{"location":"concepts/jsonl-algebra/#what-makes-it-an-algebra","title":"What Makes it an Algebra?","text":"<p>An algebra is a mathematical structure with:</p> <ol> <li>A set of elements (relations/tables)</li> <li>Operations on those elements</li> <li>Properties that those operations satisfy</li> </ol> <p>In <code>ja</code>, our algebra consists of:</p> <ul> <li>Elements: Relations (collections of JSON objects)</li> <li>Operations: select, project, join, union, etc.</li> <li>Properties: Closure, associativity, commutativity (where applicable)</li> </ul>"},{"location":"concepts/jsonl-algebra/#the-relational-model","title":"The Relational Model","text":""},{"location":"concepts/jsonl-algebra/#relations-as-sets","title":"Relations as Sets","text":"<p>In <code>ja</code>, a JSONL file represents a relation - an unordered set of tuples (JSON objects):</p> <pre><code>{\"id\": 1, \"name\": \"Alice\", \"age\": 30}\n{\"id\": 2, \"name\": \"Bob\", \"age\": 25}\n{\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n</code></pre> <p>This is a relation with schema <code>(id: number, name: string, age: number)</code>.</p>"},{"location":"concepts/jsonl-algebra/#operations-as-functions","title":"Operations as Functions","text":"<p>Each operation is a function that transforms relations:</p> <pre><code>\u03c3[age &gt; 30](R) \u2192 R'\n</code></pre> <p>This reads as: \"select (\u03c3) where age &gt; 30 from relation R produces relation R'\"</p>"},{"location":"concepts/jsonl-algebra/#core-operations","title":"Core Operations","text":""},{"location":"concepts/jsonl-algebra/#1-selection-filter","title":"1. Selection (\u03c3) - Filter","text":"<p>Selection filters rows based on a predicate:</p> <pre><code>ja select 'age &gt; 30' people.jsonl\n</code></pre> <p>Algebraic notation: \u03c3age &gt; 30</p> <p>Properties:</p> <ul> <li>Commutative: \u03c3p1 = \u03c3p2</li> <li>Can be combined: \u03c3p1 = \u03c3p1 \u2227 p2</li> </ul>"},{"location":"concepts/jsonl-algebra/#2-projection-transform","title":"2. Projection (\u03c0) - Transform","text":"<p>Projection selects and computes columns:</p> <pre><code>ja project name,income=salary*12 employees.jsonl\n</code></pre> <p>Algebraic notation: \u03c0name, income=salary\u00d712</p> <p>Properties:</p> <ul> <li>Not commutative in general</li> <li>Idempotent for simple projections: \u03c0a = \u03c0a</li> </ul>"},{"location":"concepts/jsonl-algebra/#3-join-combine","title":"3. Join (\u22c8) - Combine","text":"<p>Join combines relations based on a condition:</p> <pre><code>ja join users.jsonl orders.jsonl --on id=user_id\n</code></pre> <p>Algebraic notation: Users \u22c8[id=user_id] Orders</p> <p>Properties:</p> <ul> <li>Commutative: R \u22c8 S = S \u22c8 R</li> <li>Associative: (R \u22c8 S) \u22c8 T = R \u22c8 (S \u22c8 T)</li> </ul>"},{"location":"concepts/jsonl-algebra/#4-union-combine-all","title":"4. Union (\u222a) - Combine All","text":"<p>Union combines all rows from two relations:</p> <pre><code>ja union employees.jsonl contractors.jsonl\n</code></pre> <p>Properties:</p> <ul> <li>Commutative: R \u222a S = S \u222a R</li> <li>Associative: (R \u222a S) \u222a T = R \u222a (S \u222a T)</li> <li>Identity: R \u222a \u2205 = R</li> </ul>"},{"location":"concepts/jsonl-algebra/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"concepts/jsonl-algebra/#grouping-and-aggregation","title":"Grouping and Aggregation","text":"<p>Grouping extends relational algebra with aggregation:</p> <pre><code>ja groupby department employees.jsonl | ja agg avg_salary=avg(salary)\n</code></pre> <p>Algebraic notation: \u03b3department, avg_salary=AVG(salary)</p>"},{"location":"concepts/jsonl-algebra/#chained-grouping","title":"Chained Grouping","text":"<p>Our innovation: grouping that preserves composability:</p> <pre><code>cat sales.jsonl \\\n  | ja groupby region \\      # First level grouping\n  | ja groupby product \\     # Second level grouping  \n  | ja agg total=sum(amount) # Final aggregation\n</code></pre> <p>Each groupby adds metadata rather than aggregating, maintaining the relation structure.</p> <p>The key insight is using JSON-native metadata to preserve grouping hierarchy:</p> <pre><code>{\n  \"order_id\": 101,\n  \"user_id\": 1,\n  \"amount\": 250,\n  \"_groups\": [\n    {\"field\": \"user_id\", \"value\": 1},\n    {\"field\": \"amount\", \"value\": 250}\n  ],\n  \"_group_size\": 1,\n  \"_group_index\": 0\n}\n</code></pre>"},{"location":"concepts/jsonl-algebra/#theoretical-guarantees","title":"Theoretical Guarantees","text":""},{"location":"concepts/jsonl-algebra/#1-closure-property","title":"1. Closure Property","text":"<p>Every operation produces a valid relation, ensuring composability:</p> <pre><code>Relation \u2192 Operation \u2192 Relation\n</code></pre>"},{"location":"concepts/jsonl-algebra/#2-optimization-opportunities","title":"2. Optimization Opportunities","text":"<p>The algebraic properties enable optimizations:</p> <pre><code># These are equivalent (push selection down)\nja join huge.jsonl small.jsonl --on id=id | ja select 'active == true'\nja select 'active == true' huge.jsonl | ja join - small.jsonl --on id=id\n</code></pre>"},{"location":"concepts/jsonl-algebra/#3-declarative-nature","title":"3. Declarative Nature","text":"<p>You specify what you want, not how to compute it:</p> <pre><code># Declarative\nja groupby user_id orders.jsonl | ja agg total=sum(amount)\n\n# vs Imperative (pseudocode)\nfor order in orders:\n    totals[order.user_id] += order.amount\n</code></pre>"},{"location":"concepts/jsonl-algebra/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Predictability: Mathematical foundations mean predictable behavior</li> <li>Composability: Operations can be combined in any order (where valid)</li> <li>Optimization: Algebraic laws enable automatic optimization</li> <li>Reasoning: You can reason about transformations algebraically</li> </ol>"},{"location":"concepts/jsonl-algebra/#design-insights","title":"Design Insights","text":""},{"location":"concepts/jsonl-algebra/#json-native-metadata","title":"JSON-Native Metadata","text":"<p>Rather than encoding grouping information in compound strings like \"1.250\", we use structured JSON:</p> <pre><code>\"_groups\": [\n  {\"field\": \"user_id\", \"value\": 1},\n  {\"field\": \"amount\", \"value\": 250}\n]\n</code></pre> <p>This approach:</p> <ul> <li>Preserves data types (no string parsing needed)</li> <li>Is self-documenting</li> <li>Enables complex querying of grouping structure</li> <li>Maintains the principle that data stays sensible throughout pipelines</li> </ul>"},{"location":"concepts/jsonl-algebra/#streaming-by-design","title":"Streaming by Design","text":"<p>Each operation processes data incrementally, enabling:</p> <ul> <li>Processing of arbitrarily large datasets</li> <li>Real-time data processing</li> <li>Low memory footprint</li> <li>Natural integration with Unix pipes</li> </ul>"},{"location":"concepts/jsonl-algebra/#further-reading","title":"Further Reading","text":"<ul> <li>Composability in Practice</li> <li>Chained Grouping Deep Dive</li> <li>Expression Language</li> </ul>"},{"location":"cookbook/log-analysis/","title":"Log Analysis with ja","text":"<p>This cookbook shows how to analyze server logs using <code>ja</code>. We'll work with common log formats and build progressively more complex analyses.</p>"},{"location":"cookbook/log-analysis/#sample-data","title":"Sample Data","text":"<p>Let's start with web server access logs in JSONL format:</p> <p><code>access.jsonl</code>:</p> <pre><code>{\"timestamp\": \"2024-01-15T10:30:45Z\", \"method\": \"GET\", \"path\": \"/api/users\", \"status\": 200, \"response_time\": 45, \"ip\": \"192.168.1.100\", \"user_agent\": \"Mozilla/5.0\"}\n{\"timestamp\": \"2024-01-15T10:31:12Z\", \"method\": \"POST\", \"path\": \"/api/login\", \"status\": 401, \"response_time\": 120, \"ip\": \"192.168.1.105\", \"user_agent\": \"curl/7.68.0\"}\n{\"timestamp\": \"2024-01-15T10:31:45Z\", \"method\": \"GET\", \"path\": \"/api/users/123\", \"status\": 200, \"response_time\": 67, \"ip\": \"192.168.1.100\", \"user_agent\": \"Mozilla/5.0\"}\n{\"timestamp\": \"2024-01-15T10:32:01Z\", \"method\": \"DELETE\", \"path\": \"/api/users/456\", \"status\": 403, \"response_time\": 23, \"ip\": \"192.168.1.107\", \"user_agent\": \"PostmanRuntime/7.28.4\"}\n{\"timestamp\": \"2024-01-15T10:32:15Z\", \"method\": \"GET\", \"path\": \"/health\", \"status\": 200, \"response_time\": 5, \"ip\": \"10.0.0.1\", \"user_agent\": \"health-check\"}\n</code></pre>"},{"location":"cookbook/log-analysis/#basic-analysis","title":"Basic Analysis","text":""},{"location":"cookbook/log-analysis/#1-response-status-distribution","title":"1. Response Status Distribution","text":"<pre><code>ja groupby status access.jsonl | ja agg count\n</code></pre> <p>Output:</p> <pre><code>{\"status\": 200, \"count\": 3}\n{\"status\": 401, \"count\": 1}\n{\"status\": 403, \"count\": 1}\n</code></pre>"},{"location":"cookbook/log-analysis/#2-average-response-time-by-status","title":"2. Average Response Time by Status","text":"<pre><code>ja groupby status access.jsonl | ja agg avg_response_time=avg(response_time),count\n</code></pre> <p>Output:</p> <pre><code>{\"status\": 200, \"avg_response_time\": 39.0, \"count\": 3}\n{\"status\": 401, \"avg_response_time\": 120.0, \"count\": 1}\n{\"status\": 403, \"avg_response_time\": 23.0, \"count\": 1}\n</code></pre>"},{"location":"cookbook/log-analysis/#3-error-rate","title":"3. Error Rate","text":"<pre><code>ja agg \\\n  total_requests=count, \\\n  error_requests=count_if(status&gt;=400), \\\n  error_rate=count_if(status&gt;=400)/count \\\n  access.jsonl\n</code></pre> <p>Output:</p> <pre><code>{\"total_requests\": 5, \"error_requests\": 2, \"error_rate\": 0.4}\n</code></pre>"},{"location":"cookbook/log-analysis/#time-based-analysis","title":"Time-Based Analysis","text":""},{"location":"cookbook/log-analysis/#4-extract-time-components","title":"4. Extract Time Components","text":"<pre><code>ja project \\\n  timestamp, \\\n  method, \\\n  path, \\\n  status, \\\n  response_time, \\\n  hour=timestamp[11:13], \\\n  minute=timestamp[14:16] \\\n  access.jsonl\n</code></pre>"},{"location":"cookbook/log-analysis/#5-requests-per-hour","title":"5. Requests per Hour","text":"<pre><code>ja project timestamp,hour=timestamp[11:13],method,path,status access.jsonl \\\n  | ja groupby hour \\\n  | ja agg requests=count,avg_response_time=avg(response_time)\n</code></pre>"},{"location":"cookbook/log-analysis/#6-peak-traffic-analysis","title":"6. Peak Traffic Analysis","text":"<pre><code>ja project timestamp,minute=timestamp[11:16],status,response_time access.jsonl \\\n  | ja groupby minute \\\n  | ja agg requests=count,errors=count_if(status&gt;=400) \\\n  | ja sort requests --desc \\\n  | head -10\n</code></pre>"},{"location":"cookbook/log-analysis/#endpoint-analysis","title":"Endpoint Analysis","text":""},{"location":"cookbook/log-analysis/#7-most-popular-endpoints","title":"7. Most Popular Endpoints","text":"<pre><code>ja groupby path access.jsonl \\\n  | ja agg requests=count \\\n  | ja sort requests --desc\n</code></pre> <p>Output:</p> <pre><code>{\"path\": \"/api/users\", \"requests\": 1}\n{\"path\": \"/api/users/123\", \"requests\": 1}\n{\"path\": \"/api/login\", \"requests\": 1}\n{\"path\": \"/api/users/456\", \"requests\": 1}\n{\"path\": \"/health\", \"requests\": 1}\n</code></pre>"},{"location":"cookbook/log-analysis/#8-endpoint-performance","title":"8. Endpoint Performance","text":"<pre><code>ja groupby path access.jsonl \\\n  | ja agg \\\n    requests=count, \\\n    avg_response_time=avg(response_time), \\\n    max_response_time=max(response_time), \\\n    error_rate=count_if(status&gt;=400)/count \\\n  | ja sort avg_response_time --desc\n</code></pre>"},{"location":"cookbook/log-analysis/#9-api-vs-health-checks","title":"9. API vs Health Checks","text":"<pre><code>ja project path,status,response_time,is_api=path.startswith(\"/api\") access.jsonl \\\n  | ja groupby is_api \\\n  | ja agg \\\n    requests=count, \\\n    avg_response_time=avg(response_time), \\\n    error_rate=count_if(status&gt;=400)/count\n</code></pre>"},{"location":"cookbook/log-analysis/#multi-dimensional-analysis","title":"Multi-Dimensional Analysis","text":""},{"location":"cookbook/log-analysis/#10-method-and-status-cross-tabulation","title":"10. Method and Status Cross-Tabulation","text":"<pre><code>ja groupby method access.jsonl \\\n  | ja groupby status \\\n  | ja agg count \\\n  | ja sort method,status\n</code></pre> <p>Output:</p> <pre><code>{\"method\": \"DELETE\", \"status\": 403, \"count\": 1}\n{\"method\": \"GET\", \"status\": 200, \"count\": 3}\n{\"method\": \"POST\", \"status\": 401, \"count\": 1}\n</code></pre>"},{"location":"cookbook/log-analysis/#11-hourly-error-analysis","title":"11. Hourly Error Analysis","text":"<pre><code>ja project \\\n  timestamp, \\\n  hour=timestamp[11:13], \\\n  status, \\\n  path, \\\n  response_time \\\n  access.jsonl \\\n  | ja groupby hour \\\n  | ja groupby 'status&gt;=400' \\\n  | ja agg count,paths=list(path) \\\n  | ja select 'status&gt;=400 == true'\n</code></pre>"},{"location":"cookbook/log-analysis/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"cookbook/log-analysis/#12-slow-requests-analysis","title":"12. Slow Requests Analysis","text":"<pre><code># Define slow requests as &gt; 50ms\nja select 'response_time &gt; 50' access.jsonl \\\n  | ja groupby path \\\n  | ja agg \\\n    slow_requests=count, \\\n    avg_slow_time=avg(response_time), \\\n    max_time=max(response_time)\n</code></pre>"},{"location":"cookbook/log-analysis/#13-user-agent-analysis","title":"13. User Agent Analysis","text":"<pre><code>ja project user_agent,status,path access.jsonl \\\n  | ja groupby user_agent \\\n  | ja agg \\\n    requests=count, \\\n    unique_paths=count_distinct(path), \\\n    error_rate=count_if(status&gt;=400)/count \\\n  | ja sort requests --desc\n</code></pre>"},{"location":"cookbook/log-analysis/#14-ip-address-security-analysis","title":"14. IP Address Security Analysis","text":"<pre><code># Find IPs with high error rates\nja groupby ip access.jsonl \\\n  | ja agg \\\n    requests=count, \\\n    errors=count_if(status&gt;=400), \\\n    error_rate=count_if(status&gt;=400)/count \\\n  | ja select 'error_rate &gt; 0.5 and requests &gt; 1' \\\n  | ja sort error_rate --desc\n</code></pre>"},{"location":"cookbook/log-analysis/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"cookbook/log-analysis/#15-performance-monitoring-dashboard","title":"15. Performance Monitoring Dashboard","text":"<pre><code># Generate performance summary\nja project \\\n  timestamp, \\\n  hour=timestamp[11:13], \\\n  status, \\\n  response_time, \\\n  is_error=status&gt;=400, \\\n  is_slow=response_time&gt;100 \\\n  access.jsonl \\\n  | ja agg \\\n    total_requests=count, \\\n    avg_response_time=avg(response_time), \\\n    p95_response_time=percentile(response_time,0.95), \\\n    error_rate=sum(is_error)/count, \\\n    slow_rate=sum(is_slow)/count\n</code></pre>"},{"location":"cookbook/log-analysis/#16-security-alert-detection","title":"16. Security Alert Detection","text":"<pre><code># Find suspicious patterns\nja select 'status == 401 or status == 403' access.jsonl \\\n  | ja groupby ip \\\n  | ja agg \\\n    failed_attempts=count, \\\n    unique_paths=count_distinct(path), \\\n    time_span=max(timestamp)-min(timestamp) \\\n  | ja select 'failed_attempts &gt;= 3' \\\n  | ja sort failed_attempts --desc\n</code></pre>"},{"location":"cookbook/log-analysis/#17-api-rate-limiting-analysis","title":"17. API Rate Limiting Analysis","text":"<pre><code># Analyze request patterns per IP\nja project \\\n  ip, \\\n  timestamp, \\\n  minute=timestamp[0:16], \\\n  path \\\n  access.jsonl \\\n  | ja groupby ip \\\n  | ja groupby minute \\\n  | ja agg requests_per_minute=count \\\n  | ja select 'requests_per_minute &gt; 10' \\\n  | ja groupby ip \\\n  | ja agg \\\n    peak_minutes=count, \\\n    max_rpm=max(requests_per_minute)\n</code></pre>"},{"location":"cookbook/log-analysis/#combining-multiple-log-sources","title":"Combining Multiple Log Sources","text":""},{"location":"cookbook/log-analysis/#18-join-with-application-logs","title":"18. Join with Application Logs","text":"<p><code>app.jsonl</code>:</p> <pre><code>{\"timestamp\": \"2024-01-15T10:30:45Z\", \"level\": \"INFO\", \"message\": \"User authenticated\", \"user_id\": 123}\n{\"timestamp\": \"2024-01-15T10:31:12Z\", \"level\": \"WARN\", \"message\": \"Invalid credentials\", \"user_id\": null}\n{\"timestamp\": \"2024-01-15T10:31:45Z\", \"level\": \"INFO\", \"message\": \"User data retrieved\", \"user_id\": 123}\n</code></pre> <pre><code># Correlate access logs with application logs\nja join app.jsonl access.jsonl --on timestamp=timestamp \\\n  | ja project timestamp,method,path,status,level,message,user_id \\\n  | ja groupby level \\\n  | ja agg count,avg_response_time=avg(response_time)\n</code></pre>"},{"location":"cookbook/log-analysis/#19-error-correlation-analysis","title":"19. Error Correlation Analysis","text":"<pre><code># Find patterns between HTTP errors and application errors\nja join app.jsonl access.jsonl --on timestamp=timestamp \\\n  | ja select 'status &gt;= 400 or level == \"ERROR\"' \\\n  | ja groupby path \\\n  | ja agg \\\n    http_errors=count_if(status&gt;=400), \\\n    app_errors=count_if(level==\"ERROR\"), \\\n    total_issues=count\n</code></pre>"},{"location":"cookbook/log-analysis/#time-series-analysis","title":"Time Series Analysis","text":""},{"location":"cookbook/log-analysis/#20-request-volume-trends","title":"20. Request Volume Trends","text":"<pre><code># Analyze request patterns over time\nja project \\\n  timestamp, \\\n  minute_bucket=timestamp[0:16], \\\n  status, \\\n  response_time \\\n  access.jsonl \\\n  | ja groupby minute_bucket \\\n  | ja agg \\\n    requests=count, \\\n    errors=count_if(status&gt;=400), \\\n    avg_response_time=avg(response_time) \\\n  | ja sort minute_bucket\n</code></pre>"},{"location":"cookbook/log-analysis/#21-anomaly-detection","title":"21. Anomaly Detection","text":"<pre><code># Find time periods with unusual patterns\nja project timestamp,minute=timestamp[0:16],status,response_time access.jsonl \\\n  | ja groupby minute \\\n  | ja agg \\\n    requests=count, \\\n    avg_response_time=avg(response_time), \\\n    error_rate=count_if(status&gt;=400)/count \\\n  | ja project \\\n    minute, \\\n    requests, \\\n    avg_response_time, \\\n    error_rate, \\\n    is_anomaly='requests &gt; 100 or avg_response_time &gt; 200 or error_rate &gt; 0.1' \\\n  | ja select 'is_anomaly == true'\n</code></pre>"},{"location":"cookbook/log-analysis/#export-for-visualization","title":"Export for Visualization","text":""},{"location":"cookbook/log-analysis/#22-prepare-data-for-grafanacharts","title":"22. Prepare Data for Grafana/Charts","text":"<pre><code># Export time series data\nja project \\\n  timestamp, \\\n  hour=timestamp[11:13], \\\n  status, \\\n  response_time \\\n  access.jsonl \\\n  | ja groupby hour \\\n  | ja agg \\\n    requests=count, \\\n    avg_response_time=avg(response_time), \\\n    error_count=count_if(status&gt;=400) \\\n  | ja export csv &gt; hourly_metrics.csv\n</code></pre>"},{"location":"cookbook/log-analysis/#23-create-status-code-distribution","title":"23. Create Status Code Distribution","text":"<pre><code># Format for pie chart\nja groupby status access.jsonl \\\n  | ja agg count \\\n  | ja project label=status,value=count \\\n  | ja export json\n</code></pre>"},{"location":"cookbook/log-analysis/#tips-for-production-use","title":"Tips for Production Use","text":""},{"location":"cookbook/log-analysis/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Filter Early: Apply time range filters first</li> <li>Sample Large Datasets: Use <code>head</code> for exploratory analysis</li> <li>Index Common Fields: Consider pre-processing for frequently queried fields</li> </ol> <pre><code># Efficient large log analysis\ncat large_access.log.jsonl \\\n  | ja select 'timestamp &gt; \"2024-01-15T00:00:00Z\"' \\\n  | ja select 'status &gt;= 400' \\\n  | ja groupby path \\\n  | ja agg error_count=count\n</code></pre>"},{"location":"cookbook/log-analysis/#automation-scripts","title":"Automation Scripts","text":"<p>Create reusable analysis scripts:</p> <pre><code>#!/bin/bash\n# error_summary.sh\nja select 'status &gt;= 400' $1 \\\n  | ja groupby status \\\n  | ja groupby path \\\n  | ja agg count \\\n  | ja sort count --desc\n</code></pre>"},{"location":"cookbook/log-analysis/#integration-with-monitoring","title":"Integration with Monitoring","text":"<pre><code># Real-time monitoring pipeline\ntail -f /var/log/access.log \\\n  | ja select 'status &gt;= 500' \\\n  | ja project timestamp,path,status,ip \\\n  | while read line; do\n      echo \"CRITICAL ERROR: $line\" | send_alert\n    done\n</code></pre>"},{"location":"cookbook/log-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Handle large log files efficiently</li> <li>Format Conversion - Work with different log formats</li> <li>Real-time Processing - Build live monitoring systems</li> </ul>"},{"location":"operations/overview/","title":"Operations Overview","text":"<p><code>ja</code> provides a complete set of relational algebra operations designed for JSON data. Each operation follows the principle of taking relations (JSONL data) and producing relations, enabling infinite composability.</p>"},{"location":"operations/overview/#core-operations","title":"Core Operations","text":""},{"location":"operations/overview/#selection-and-filtering","title":"Selection and Filtering","text":"Operation Symbol Purpose Example select \u03c3 Filter rows based on conditions <code>ja select 'age &gt; 30'</code> distinct \u03b4 Remove duplicate rows <code>ja distinct</code>"},{"location":"operations/overview/#projection-and-transformation","title":"Projection and Transformation","text":"Operation Symbol Purpose Example project \u03c0 Select/compute columns <code>ja project name,total=price*qty</code> rename \u03c1 Rename fields <code>ja rename old_name=new_name</code>"},{"location":"operations/overview/#set-operations","title":"Set Operations","text":"Operation Symbol Purpose Example union \u222a Combine all rows from relations <code>ja union file1.jsonl file2.jsonl</code> intersection \u2229 Keep only rows in both relations <code>ja intersection file1.jsonl file2.jsonl</code> difference - Keep rows in left but not right <code>ja difference file1.jsonl file2.jsonl</code>"},{"location":"operations/overview/#join-operations","title":"Join Operations","text":"Operation Symbol Purpose Example join \u22c8 Inner join on condition <code>ja join users.jsonl orders.jsonl --on id=user_id</code> product \u00d7 Cartesian product <code>ja product features.jsonl options.jsonl</code>"},{"location":"operations/overview/#grouping-and-aggregation","title":"Grouping and Aggregation","text":"Operation Symbol Purpose Example groupby \u03b3 Group rows (chainable) <code>ja groupby department</code> agg \u03b3 Aggregate data <code>ja agg count,total=sum(amount)</code>"},{"location":"operations/overview/#ordering","title":"Ordering","text":"Operation Symbol Purpose Example sort \u03c4 Sort rows <code>ja sort name,age --desc</code>"},{"location":"operations/overview/#quick-reference","title":"Quick Reference","text":""},{"location":"operations/overview/#selection-filter-rows","title":"Selection (Filter Rows)","text":"<pre><code># Basic filtering\nja select 'age &gt; 30' people.jsonl\n\n# Complex conditions\nja select 'status == \"active\" and score &gt;= 80' users.jsonl\n\n# Nested field filtering\nja select 'user.location.country == \"US\"' data.jsonl\n\n# Using group metadata\nja groupby department data.jsonl | ja select '_group_size &gt; 10'\n</code></pre>"},{"location":"operations/overview/#projection-selecttransform-columns","title":"Projection (Select/Transform Columns)","text":"<pre><code># Select specific fields\nja project name,email people.jsonl\n\n# Compute new fields\nja project name,total=price*quantity orders.jsonl\n\n# Nested field selection\nja project user.name,user.email,order.total data.jsonl\n\n# Flatten nested structures\nja project user.name,user.email --flatten data.jsonl\n</code></pre>"},{"location":"operations/overview/#aggregation","title":"Aggregation","text":"<pre><code># Simple aggregation\nja agg count,total=sum(amount) orders.jsonl\n\n# After grouping\nja groupby department employees.jsonl | ja agg avg_salary=avg(salary)\n\n# Chained grouping\nja groupby region sales.jsonl | ja groupby product | ja agg revenue=sum(amount)\n\n# Conditional aggregation\nja agg shipped=count_if(status==\"shipped\"),total=sum(amount) orders.jsonl\n</code></pre>"},{"location":"operations/overview/#joins","title":"Joins","text":"<pre><code># Inner join\nja join users.jsonl orders.jsonl --on id=user_id\n\n# Join with nested fields\nja join customers.jsonl orders.jsonl --on customer.id=customer_id\n\n# Multiple files in pipeline\ncat orders.jsonl | ja join customers.jsonl --on customer_id=id\n</code></pre>"},{"location":"operations/overview/#set-operations_1","title":"Set Operations","text":"<pre><code># Combine files\nja union current.jsonl historical.jsonl\n\n# Find common records\nja intersection whitelist.jsonl data.jsonl\n\n# Find differences\nja difference all_users.jsonl inactive_users.jsonl\n</code></pre>"},{"location":"operations/overview/#advanced-features","title":"Advanced Features","text":""},{"location":"operations/overview/#nested-data-support","title":"Nested Data Support","text":"<p>All operations support dot notation for nested fields:</p> <pre><code># Works with any level of nesting\nja select 'user.profile.preferences.notifications == true'\nja project user.profile.name,user.profile.email\nja groupby user.location.country\nja join file1.jsonl file2.jsonl --on user.id=customer.id\n</code></pre>"},{"location":"operations/overview/#expression-language","title":"Expression Language","text":"<p>Rich expression support in <code>select</code> and <code>project</code>:</p> <pre><code># Arithmetic\nja project name,annual_salary=monthly_salary*12\n\n# String operations  \nja select 'name.startswith(\"A\")'\n\n# Comparisons\nja select 'created_date &gt; \"2024-01-01\"'\n\n# Logical operations\nja select 'age &gt;= 18 and status == \"active\"'\n</code></pre>"},{"location":"operations/overview/#aggregation-functions","title":"Aggregation Functions","text":"Function Purpose Example <code>count</code> Count rows <code>count</code> <code>sum(field)</code> Sum numeric values <code>sum(amount)</code> <code>avg(field)</code> Average of values <code>avg(score)</code> <code>min(field)</code> Minimum value <code>min(date)</code> <code>max(field)</code> Maximum value <code>max(price)</code> <code>list(field)</code> Collect values in list <code>list(tag)</code> <code>first(field)</code> First value <code>first(name)</code> <code>last(field)</code> Last value <code>last(status)</code>"},{"location":"operations/overview/#conditional-aggregations","title":"Conditional Aggregations","text":"Function Purpose Example <code>count_if(condition)</code> Count matching rows <code>count_if(status==\"paid\")</code> <code>sum_if(field, condition)</code> Sum matching values <code>sum_if(amount, region==\"North\")</code> <code>avg_if(field, condition)</code> Average matching values <code>avg_if(score, active==true)</code>"},{"location":"operations/overview/#composition-examples","title":"Composition Examples","text":""},{"location":"operations/overview/#building-complex-pipelines","title":"Building Complex Pipelines","text":"<pre><code># E-commerce analysis pipeline\ncat orders.jsonl \\\n  | ja select 'status == \"completed\"' \\\n  | ja join products.jsonl --on product_id=id \\\n  | ja join customers.jsonl --on customer_id=id \\\n  | ja project \\\n      customer.name, \\\n      product.category, \\\n      total=quantity*price, \\\n      month=date[0:7] \\\n  | ja groupby customer.tier \\\n  | ja groupby product.category \\\n  | ja agg \\\n      revenue=sum(total), \\\n      orders=count, \\\n      avg_order=avg(total)\n</code></pre>"},{"location":"operations/overview/#data-quality-pipeline","title":"Data Quality Pipeline","text":"<pre><code># Clean and validate data\ncat raw_data.jsonl \\\n  | ja select 'email != null and age &gt; 0' \\\n  | ja project \\\n      name, \\\n      email=email.lower(), \\\n      age, \\\n      valid=email.contains(\"@\") \\\n  | ja select 'valid == true' \\\n  | ja distinct \\\n  | ja sort name\n</code></pre>"},{"location":"operations/overview/#time-series-analysis","title":"Time Series Analysis","text":"<pre><code># Analyze user engagement over time\ncat events.jsonl \\\n  | ja project \\\n      user_id, \\\n      event_type, \\\n      date=timestamp[0:10] \\\n  | ja groupby date \\\n  | ja groupby event_type \\\n  | ja agg \\\n      events=count, \\\n      unique_users=count_distinct(user_id) \\\n  | ja sort date\n</code></pre>"},{"location":"operations/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"operations/overview/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Filter Early: Use <code>select</code> before expensive operations</li> <li>Project Only Needed Fields: Reduce data size with <code>project</code></li> <li>Use Direct Aggregation: Use <code>--agg</code> for simple grouping</li> <li>Order Operations: Put most selective filters first</li> </ol> <pre><code># Optimized pipeline\ncat large_file.jsonl \\\n  | ja select 'timestamp &gt; \"2024-01-01\"' \\\n  | ja project user_id,amount,category \\\n  | ja groupby category \\\n  | ja agg total=sum(amount)\n</code></pre>"},{"location":"operations/overview/#memory-efficiency","title":"Memory Efficiency","text":"<p>All operations are streaming by design:</p> <ul> <li>Process arbitrarily large files</li> <li>Constant memory usage (except for operations requiring sorting/grouping)</li> <li>Natural integration with Unix pipes</li> </ul>"},{"location":"operations/overview/#error-handling","title":"Error Handling","text":""},{"location":"operations/overview/#common-issues","title":"Common Issues","text":"<p>Invalid Expressions:</p> <pre><code># Check expression syntax\nja select 'age &gt; thirty'  # Error: 'thirty' not defined\nja select 'age &gt; 30'      # Correct\n</code></pre> <p>Missing Fields:</p> <pre><code># Handle missing fields gracefully\nja select 'field != null'  # Only rows where field exists\n</code></pre> <p>Type Mismatches:</p> <pre><code># Ensure consistent types\nja select 'amount &gt; 0'     # Assumes amount is numeric\n</code></pre>"},{"location":"operations/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Deep Dive: Selection - Advanced filtering techniques</li> <li>Deep Dive: Grouping - Master chained groupby</li> <li>Deep Dive: Joins - Complex join patterns</li> <li>Cookbook - Real-world examples</li> </ul>"},{"location":"tools/dataset-generator/","title":"Dataset Generator","text":"<p>The <code>ja-generate-dataset</code> command creates synthetic datasets specifically designed to showcase jsonl-algebra's capabilities. It generates two related JSONL files with realistic nested structures and relationships.</p>"},{"location":"tools/dataset-generator/#quick-start","title":"Quick Start","text":"<pre><code># Install with dataset support\npip install \"jsonl-algebra[dataset]\"\n\n# Generate default dataset (20 companies, 100 people)\nja-generate-dataset\n\n# Generate larger dataset\nja-generate-dataset --num-companies 50 --num-people 1000\n\n# Generate to specific directory\nja-generate-dataset --output-dir examples/\n</code></pre>"},{"location":"tools/dataset-generator/#generated-data-structure","title":"Generated Data Structure","text":""},{"location":"tools/dataset-generator/#companies-companiesjsonl","title":"Companies (<code>companies.jsonl</code>)","text":"<p>Each company record contains:</p> <pre><code>{\n  \"id\": \"uuid-string\",\n  \"name\": \"Company Name Inc\",\n  \"industry\": \"Technology\",\n  \"headquarters\": {\n    \"city\": \"San Francisco\",\n    \"state\": \"CA\", \n    \"country\": \"USA\"\n  },\n  \"size\": 1500,\n  \"founded\": 2010\n}\n</code></pre> <p>Industries: Technology, Healthcare, Finance, Education, Retail, Manufacturing, Consulting, Entertainment, Transportation, Real Estate, Construction, Agriculture, Energy, Media</p>"},{"location":"tools/dataset-generator/#people-peoplejsonl","title":"People (<code>people.jsonl</code>)","text":"<p>Each person record contains:</p> <pre><code>{\n  \"id\": \"uuid-string\",\n  \"created_at\": \"2023-05-15T10:30:00Z\",\n  \"status\": \"active\",\n  \"household_id\": \"shared-uuid-for-family\",\n  \"person\": {\n    \"name\": {\n      \"first\": \"Sarah\",\n      \"last\": \"Johnson\"\n    },\n    \"age\": 32,\n    \"gender\": \"female\",\n    \"email\": \"sarah.johnson@gmail.com\",\n    \"phone\": \"555-123-4567\",\n    \"location\": {\n      \"city\": \"San Francisco\",\n      \"state\": \"CA\",\n      \"country\": \"USA\"\n    },\n    \"interests\": [\"hiking\", \"photography\", \"cooking\"],\n    \"job\": {\n      \"title\": \"Software Engineer\", \n      \"company_name\": \"Tech Solutions Inc\",\n      \"salary\": 95000.0\n    }\n  }\n}\n</code></pre> <p>Key Features:</p> <ul> <li>Household relationships: People sharing <code>household_id</code> have the same last name and location</li> <li>Employment relationships: <code>person.job.company_name</code> links to company <code>name</code> field</li> <li>Nested structures: Rich nested JSON perfect for testing ja's navigation capabilities</li> <li>Realistic distributions: Age, gender, salary, and location follow realistic patterns</li> </ul>"},{"location":"tools/dataset-generator/#command-line-options","title":"Command Line Options","text":"<pre><code>ja-generate-dataset [OPTIONS]\n\nData Generation:\n  --num-companies N        Number of companies (default: 20)\n  --num-people N          Number of people (default: 100)  \n  --max-household-size N  Max people per household (default: 5)\n\nRandomness Control:\n  --deterministic         Use fixed seed for reproducible data (default)\n  --random               Use random seed for varied output\n  --seed N               Custom seed for deterministic mode (default: 42)\n\nOutput Control:\n  --output-dir PATH       Output directory (default: current directory)\n  --companies-file PATH   Custom companies file path\n  --people-file PATH      Custom people file path\n</code></pre>"},{"location":"tools/dataset-generator/#example-workflows","title":"Example Workflows","text":""},{"location":"tools/dataset-generator/#basic-exploration","title":"Basic Exploration","text":"<pre><code># Generate sample data\nja-generate-dataset --num-companies 10 --num-people 50\n\n# Explore the data\nja people.jsonl --head 3 --pretty\nja companies.jsonl --select name,industry,size --head 5\n\n# Count people by company\nja people.jsonl --group-by person.job.company_name --count\n</code></pre>"},{"location":"tools/dataset-generator/#nested-data-operations","title":"Nested Data Operations","text":"<pre><code># Extract names and ages\nja people.jsonl --select person.name,person.age\n\n# Filter by age and location  \nja people.jsonl --where 'person.age &gt; 30' --select person.name,person.location.state\n\n# Group by nested fields\nja people.jsonl --group-by person.location.state,person.gender --count\n</code></pre>"},{"location":"tools/dataset-generator/#relational-operations","title":"Relational Operations","text":"<pre><code># Join people with their companies\nja people.jsonl --join companies.jsonl --on 'person.job.company_name = name' \\\\\n  --select person.name,name,industry,person.job.salary\n\n# Find high earners in tech companies\nja people.jsonl --join companies.jsonl --on 'person.job.company_name = name' \\\\\n  --where 'industry = \"Technology\" and person.job.salary &gt; 100000' \\\\\n  --select person.name,name,person.job.salary\n</code></pre>"},{"location":"tools/dataset-generator/#aggregation-examples","title":"Aggregation Examples","text":"<pre><code># Average salary by industry\nja people.jsonl --join companies.jsonl --on 'person.job.company_name = name' \\\\\n  --group-by industry --agg 'avg(person.job.salary)'\n\n# Company size distribution\nja companies.jsonl --group-by industry --agg 'avg(size),count(*)'\n\n# Household statistics\nja people.jsonl --group-by household_id --agg 'count(*),avg(person.age)' \\\\\n  --select '*,_count as household_size,_avg_person_age as avg_age'\n</code></pre>"},{"location":"tools/dataset-generator/#testing-and-development","title":"Testing and Development","text":"<p>The dataset generator is designed for both documentation examples and unit testing:</p>"},{"location":"tools/dataset-generator/#deterministic-mode-default","title":"Deterministic Mode (Default)","text":"<ul> <li>Always produces the same output for given parameters</li> <li>Perfect for examples, documentation, and reproducible tests</li> <li>Uses seed 42 by default</li> </ul>"},{"location":"tools/dataset-generator/#random-mode","title":"Random Mode","text":"<ul> <li>Generates varied data for stress testing</li> <li>Useful for property-based testing</li> <li>Each run produces different realistic data</li> </ul>"},{"location":"tools/dataset-generator/#usage-in-tests","title":"Usage in Tests","text":"<pre><code># In test files\nimport subprocess\nimport tempfile\nimport os\n\ndef setup_test_data():\n    \\\"\\\"\\\"Generate deterministic test data.\\\"\\\"\\\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        subprocess.run([\n            \"python\", \"scripts/generate_dataset.py\",\n            \"--deterministic\", \"--seed\", \"123\",\n            \"--num-companies\", \"5\", \"--num-people\", \"25\", \n            \"--output-dir\", tmpdir\n        ])\n        return os.path.join(tmpdir, \"people.jsonl\"), os.path.join(tmpdir, \"companies.jsonl\")\n</code></pre>"},{"location":"tools/dataset-generator/#data-relationships","title":"Data Relationships","text":"<p>The generated datasets are designed to demonstrate common data patterns:</p> <pre><code>Companies (1) \u2192 (N) People\n  company.name = person.job.company_name\n\nHouseholds (1) \u2192 (N) People  \n  household_id groups people with shared:\n  - person.name.last (family name)\n  - person.location (shared address)\n\nGeographic Hierarchy:\n  Country \u2192 State \u2192 City\n  All records use USA with realistic state/city combinations\n</code></pre>"},{"location":"tools/dataset-generator/#dependencies","title":"Dependencies","text":"<ul> <li>Core: Works with Python standard library only</li> <li>Enhanced: Install <code>faker</code> for richer company names and data variety</li> </ul> <p><code>bash   pip install \"jsonl-algebra[dataset]\"  # Includes faker</code></p> <p>Without faker, the generator uses built-in data pools that still produce realistic, varied output.</p>"},{"location":"tools/dataset-generator/#tips","title":"Tips","text":"<ol> <li>Start small: Use <code>--num-people 20 --num-companies 5</code> for initial exploration</li> <li>Use deterministic mode: For examples and tests where consistency matters  </li> <li>Scale gradually: Large datasets (10K+ records) are great for performance testing</li> <li>Combine with ja: The suggested commands in the output are a great starting point</li> <li>Explore relationships: Use joins to see how nested structures connect across files</li> </ol> <p>The generated data is specifically crafted to showcase ja's strengths in handling real-world nested JSON data patterns.</p>"}]}